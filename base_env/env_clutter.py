import os
from typing import Any, Dict, List, Union, Tuple, Optional
import numpy as np
import sapien
import torch
import cv2
import random
import sys

# å¯¼å…¥é…ç½®
try:
    from .config import Config, get_config
except ImportError:
    # å¤„ç†ç›´æŽ¥è¿è¡Œæ—¶çš„ç›¸å¯¹å¯¼å…¥é—®é¢˜
    from config import Config, get_config

# å¯¼å…¥AnyGraspç›¸å…³æ¨¡å—
try:
    # æ·»åŠ AnyGraspè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
    anygrasp_path = "/home/linux/jzh/RL_Robot/anygrasp_sdk/grasp_detection"
    if anygrasp_path not in sys.path:
        sys.path.insert(0, anygrasp_path)
    from gsnet import AnyGrasp
    from graspnetAPI import GraspGroup
    ANYGRASP_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: AnyGraspæœªèƒ½å¯¼å…¥ - {e}")
    print("å°†è·³è¿‡æŠ“å–ç‚¹æ£€æµ‹åŠŸèƒ½")
    ANYGRASP_AVAILABLE = False

import mani_skill.envs.utils.randomization as randomization
from mani_skill import ASSET_DIR
from mani_skill.agents.robots import Fetch, Panda
from mani_skill.envs.sapien_env import BaseEnv
from mani_skill.sensors.camera import CameraConfig
from mani_skill.utils import sapien_utils
from mani_skill.utils.building import actors
from mani_skill.utils.building.actor_builder import ActorBuilder
from mani_skill.utils.io_utils import load_json
from mani_skill.utils.registration import register_env
from mani_skill.utils.scene_builder.table import TableSceneBuilder
from mani_skill.utils.structs import Actor, Pose
from mani_skill.utils.structs.types import GPUMemoryConfig, SimConfig

# æ–°å¢žï¼šIKå’ŒæŽ§åˆ¶å™¨ç›¸å…³å¯¼å…¥
# from mani_skill.agents.controllers.pd_ee_pose import PDEEPoseController

# æ–°å¢žï¼šå¯¼å…¥SAPIENçº¦æŸç›¸å…³æ¨¡å—
import sapien.physx as physx


@register_env(
    "EnvClutter-v1",
    asset_download_ids=["ycb"],
    max_episode_steps=200,
)
class EnvClutterEnv(BaseEnv):
    """
    **ä»»åŠ¡æè¿°:**
    å¤æ‚å †å æŠ“å–çŽ¯å¢ƒï¼ŒåŒ…å«å„ç§å½¢çŠ¶çš„YCBç‰©ä½“å †ç§¯åœ¨æ‰˜ç›˜ä¸­ã€‚
    æœºæ¢°è‡‚éœ€è¦æŒ‘é€‰æœ€é€‚åˆæŠ“å–çš„ç‰©ä½“ï¼Œå¹¶å°†å…¶æ”¾åˆ°æŒ‡å®šä½ç½®ã€‚
    
    **éšæœºåŒ–:**
    - ç‰©ä½“åœ¨æ‰˜ç›˜å†…éšæœºç”Ÿæˆ
    - ç‰©ä½“åˆå§‹å§¿æ€éšæœºåŒ–
    - ç›®æ ‡ä½ç½®å›ºå®šåœ¨æ‰˜ç›˜å³ä¾§
    
    **æˆåŠŸæ¡ä»¶:**
    - ç›®æ ‡ç‰©ä½“è¢«æˆåŠŸæŠ“å–å¹¶æ”¾ç½®åˆ°ç›®æ ‡ä½ç½®
    - æœºå™¨äººé™æ­¢
    """
    
    SUPPORTED_REWARD_MODES = ["dense", "sparse"]
    SUPPORTED_ROBOTS = ["panda", "fetch"]
    agent: Union[Panda, Fetch]
    
    # æ‰˜ç›˜å‚æ•° (åŸºäºŽtraybox.urdfçš„å°ºå¯¸)
    tray_size = [0.6, 0.6, 0.15]  # æ‰˜ç›˜å†…éƒ¨å°ºå¯¸ (é•¿xå®½xé«˜)
    tray_spawn_area = [0.23, 0.23]  # æ‰˜ç›˜å†…ç‰©ä½“ç”ŸæˆåŒºåŸŸ (è€ƒè™‘è¾¹ç•Œå¢™å’Œå®‰å…¨è¾¹è·)
    
    # æ³¨æ„ï¼šç‰©ä½“ç›¸å…³å‚æ•°çŽ°åœ¨ä»Žconfigä¸­åŠ¨æ€èŽ·å–
    # BOX_OBJECTS, num_objects_per_type, MAX_N, MAX_EPISODE_STEPS ç­‰
    # éƒ½åœ¨ __init__ æ–¹æ³•ä¸­ä»Žé…ç½®æ–‡ä»¶åˆå§‹åŒ–
    
    
    # AnyGraspç›¸å…³é…ç½® - ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹demoçš„é«˜è´¨é‡å‚æ•°
    ANYGRASP_CHECKPOINT = "/home/linux/jzh/RL_Robot/anygrasp_sdk/grasp_detection/log/checkpoint_detection.tar"  # æ¨¡åž‹æƒé‡è·¯å¾„
    ANYGRASP_MAX_GRIPPER_WIDTH = 0.1   # ðŸ”§ ä¿®å¤ï¼šå¢žåŠ åˆ°10cmï¼Œä¸Žå®˜æ–¹demoä¸€è‡´
    ANYGRASP_GRIPPER_HEIGHT = 0.025     
    ANYGRASP_TOP_DOWN_GRASP = False     # æ˜¯å¦ä¼˜å…ˆé¡¶éƒ¨æŠ“å–
    
    def __init__(
        self,
        *args,
        robot_uids="panda",
        robot_init_qpos_noise=0.02,
        num_envs=1,
        parallel_in_single_scene=False,
        **kwargs,
    ):
        self.robot_init_qpos_noise = robot_init_qpos_noise
        
        # åŸºæœ¬ç‰©ä½“é…ç½® - ç®€åŒ–ä¸ºå›ºå®šé…ç½®
        self.BOX_OBJECTS = ["004_sugar_box", "009_gelatin_box", "008_pudding_box"]
        self.num_objects_per_type = 18  # æ¯ç§ç±»åž‹4ä¸ªç‰©ä½“
        self.total_objects_per_env = 54  # æ€»å…±12ä¸ªç‰©ä½“
        
        # ä»»åŠ¡ç›¸å…³å‚æ•°
        self.goal_thresh = 0.05  # æˆåŠŸé˜ˆå€¼
        self.MAX_EPISODE_STEPS = 500  # æœ€å¤§æ­¥æ•°
        
        # è·Ÿè¸ªå˜é‡
        self.grasped_objects_count = 0  # å·²æˆåŠŸæŠ“å–çš„ç‰©ä½“æ•°é‡
        self.current_target_idx = 0     # å½“å‰ç›®æ ‡ç‰©ä½“ç´¢å¼•
        
        # åˆå§‹ä½ç½®è®°å½•ï¼ˆç”¨äºŽè®¡ç®—ä½ç§»ï¼‰
        self.initial_positions = {}
        
        # ç®€åŒ–çŽ¯å¢ƒä½¿ç”¨è¿žç»­åŠ¨ä½œæ¨¡å¼
        self.use_discrete_action = False
        
        # YCBæ•°æ®é›†ç¼“å­˜
        self._ycb_dataset = None
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°æ­£ç¡®ä¼ é€’ç»™çˆ¶ç±»
        super().__init__(
            *args,
            robot_uids=robot_uids,
            num_envs=num_envs,
            parallel_in_single_scene=parallel_in_single_scene,
            **kwargs,
        )
    
    def _load_ycb_dataset(self):
        """åŠ è½½YCBæ•°æ®é›†ä¿¡æ¯ï¼ŒåŸºäºŽå®˜æ–¹å®žçŽ°"""
        if self._ycb_dataset is None:
            self._ycb_dataset = {
                "model_data": load_json(ASSET_DIR / "assets/mani_skill2_ycb/info_pick_v0.json"),
            }
        return self._ycb_dataset
    
    def _create_scaled_ycb_builder(self, obj_id: str, scale: float = 1.0, add_collision: bool = True, add_visual: bool = True):
        """
        åˆ›å»ºç¼©æ”¾çš„YCBå¯¹è±¡æž„å»ºå™¨ï¼ŒåŸºäºŽå®˜æ–¹get_ycb_builderå‡½æ•°å®žçŽ°
        
        Args:
            obj_id: YCBå¯¹è±¡IDï¼Œå¦‚"004_sugar_box"
            scale: ç¼©æ”¾æ¯”ä¾‹ï¼Œå¦‚0.7è¡¨ç¤ºç¼©å°åˆ°åŽŸæ¥çš„70%
            add_collision: æ˜¯å¦æ·»åŠ ç¢°æ’žå½¢çŠ¶
            add_visual: æ˜¯å¦æ·»åŠ è§†è§‰å½¢çŠ¶
        """
        # ç¡®ä¿æ•°æ®é›†å·²åŠ è½½
        dataset = self._load_ycb_dataset()
        model_db = dataset["model_data"]
        
        # åˆ›å»ºactor builder
        builder = self.scene.create_actor_builder()
        
        # èŽ·å–æ¨¡åž‹å…ƒæ•°æ®
        metadata = model_db[obj_id]
        density = metadata.get("density", 1000)
        
        # ä½¿ç”¨è‡ªå®šä¹‰ç¼©æ”¾è€Œä¸æ˜¯metadataä¸­çš„scales
        custom_scale = scale
        physical_material = None
        
        # æž„å»ºæ¨¡åž‹è·¯å¾„
        model_dir = ASSET_DIR / "assets/mani_skill2_ycb/models" / obj_id
        
        # æ·»åŠ ç¢°æ’žå½¢çŠ¶ï¼ˆå¦‚æžœéœ€è¦ï¼‰
        if add_collision:
            collision_file = str(model_dir / "collision.ply")
            builder.add_multiple_convex_collisions_from_file(
                filename=collision_file,
                scale=[custom_scale] * 3,  # åº”ç”¨è‡ªå®šä¹‰ç¼©æ”¾
                material=physical_material,
                density=density,
            )
        
        # æ·»åŠ è§†è§‰å½¢çŠ¶ï¼ˆå¦‚æžœéœ€è¦ï¼‰
        if add_visual:
            visual_file = str(model_dir / "textured.obj")
            builder.add_visual_from_file(
                filename=visual_file, 
                scale=[custom_scale] * 3  # åº”ç”¨è‡ªå®šä¹‰ç¼©æ”¾
            )
        
        return builder
        

    @property
    def _default_sim_config(self):
        return SimConfig(
            gpu_memory_config=GPUMemoryConfig(
                max_rigid_contact_count=2**21,
                max_rigid_patch_count=2**19
            )
        )

    @property
    def _default_sensor_configs(self):
        # ðŸ”§ å¹³è¡¡ç›¸æœºä½ç½®ï¼šæ—¢è¦èŽ·å¾—åˆç†çš„æ·±åº¦å€¼ï¼Œåˆè¦ä¿è¯AnyGraspèƒ½æ£€æµ‹åˆ°æŠ“å–
        # æ‰˜ç›˜ä½ç½®åœ¨ [-0.2, 0.0, 0.006]ï¼Œç‰©ä½“åœ¨z=0.04-0.1å·¦å³
        # ä½¿ç”¨é€‚ä¸­çš„ç›¸æœºé«˜åº¦å’Œè·ç¦»ï¼Œæ–œå‘ä¿¯è§†è§’åº¦æœ‰åˆ©äºŽæŠ“å–æ£€æµ‹
        pose = sapien_utils.look_at(eye=[0.0, 0, 0.35], target=[-0.2, 0.0, 0.05])
        return [
            CameraConfig(
                "base_camera",
                pose=pose,
                width=640,   # ðŸ”§ ä¿®å¤ï¼šæé«˜åˆ†è¾¨çŽ‡åˆ°640x480ï¼ŒæŽ¥è¿‘å®˜æ–¹demo
                height=480,
                fov=np.pi / 2,
                near=0.01,
                far=100,
            )
        ]

    @property
    def _default_human_render_camera_configs(self):
        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.2, 0.35])
        return CameraConfig(
            "render_camera", 
            pose=pose, 
            width=512, 
            height=512, 
            fov=1, 
            near=0.01, 
            far=100
        )

    def _load_agent(self, options: dict):
        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))

    def _load_scene(self, options: dict):
        # æž„å»ºæ¡Œé¢åœºæ™¯
        self.scene_builder = TableSceneBuilder(
            self, robot_init_qpos_noise=self.robot_init_qpos_noise
        )
        self.scene_builder.build()
        
        # åŠ è½½æ‰˜ç›˜
        self._load_tray()
        
        # åˆ›å»ºç‰©ä½“åˆ—è¡¨
        self.all_objects = []
        self.selectable_objects = []
        self.object_info = []  # å­˜å‚¨ç‰©ä½“ä¿¡æ¯
        
        # ä¸ºæ¯ä¸ªçŽ¯å¢ƒåˆ›å»ºç‰©ä½“
        for env_idx in range(self.num_envs):
            env_objects = []
            env_selectable = []
            env_info = []
            
            # åˆ›å»ºæ¯ç§ç±»åž‹çš„ç‰©ä½“
            for obj_type in self.BOX_OBJECTS:
                for i in range(self.num_objects_per_type):
                    # åˆ›å»ºç‰©ä½“ - é’ˆå¯¹004_sugar_boxåº”ç”¨ç‰¹æ®Šç¼©æ”¾
                    if obj_type == "004_sugar_box":
                        builder = self._create_scaled_ycb_builder(obj_type, scale=0.7)
                    else:
                        builder = actors.get_actor_builder(self.scene, id=f"ycb:{obj_type}")

                    # åœ¨æ‰˜ç›˜å†…éšæœºç”Ÿæˆä½ç½®
                    x, y, z = self._generate_object_position_in_tray(i)
                    
                    # éšæœºå§¿æ€
                    quat = randomization.random_quaternions(1)[0]
                    initial_pose = sapien.Pose(p=[x, y, z], q=quat.cpu().numpy())
                    
                    builder.initial_pose = initial_pose
                    builder.set_scene_idxs([env_idx])
                    
                    obj_name = f"env_{env_idx}_{obj_type}_{i}"
                    obj = builder.build(name=obj_name)
                    
                    env_objects.append(obj)
                    env_selectable.append(obj)
                    
                    # å­˜å‚¨ç‰©ä½“ä¿¡æ¯
                    obj_info = {
                        'type': obj_type,
                        'size': self._get_object_size(obj_type),
                        'initial_pose': initial_pose,
                        'center': [x, y, z],
                        'exposed_area': 1.0,  # åˆå§‹æš´éœ²é¢ç§¯ï¼ŒåŽç»­ä¼šè®¡ç®—
                    }
                    env_info.append(obj_info)
            
            self.all_objects.extend(env_objects)
            self.selectable_objects.append(env_selectable)
            self.object_info.append(env_info)
        
        # åˆå¹¶æ‰€æœ‰ç‰©ä½“
        if self.all_objects:
            self.merged_objects = Actor.merge(self.all_objects, name="all_objects")
        
        # åˆ›å»ºç›®æ ‡ä½ç½®æ ‡è®°
        self.goal_site = actors.build_sphere(
            self.scene,
            radius=self.goal_thresh,
            color=[0, 1, 0, 0],
            name="goal_site",
            body_type="kinematic",
            add_collision=False,
            initial_pose=sapien.Pose(),
        )
        self._hidden_objects.append(self.goal_site)
        
        # åˆå§‹åŒ–ç›®æ ‡ç‰©ä½“ç›¸å…³å˜é‡
        self.target_object = None
        self.target_object_indices = []

    def _load_tray(self):
        """åŠ è½½æ‰˜ç›˜URDFæ–‡ä»¶"""
        # èŽ·å–æ‰˜ç›˜URDFæ–‡ä»¶è·¯å¾„
        tray_urdf_path = "/home/linux/jzh/RL_Robot/assets/tray/traybox.urdf"
        
        if not os.path.exists(tray_urdf_path):
            raise FileNotFoundError(f"æ‰˜ç›˜URDFæ–‡ä»¶æœªæ‰¾åˆ°: {tray_urdf_path}")
        
        # åˆ›å»ºURDFåŠ è½½å™¨
        loader = self.scene.create_urdf_loader()
        
        # è®¾ç½®æ‰˜ç›˜çš„ç‰©ç†å±žæ€§
        loader.set_material(static_friction=0.8, dynamic_friction=0.6, restitution=0.05)
        loader.fix_root_link = True  # å›ºå®šæ‰˜ç›˜ä¸åŠ¨
        loader.scale = 1.0  # ä¿æŒåŽŸå§‹å°ºå¯¸
        
        # è§£æžURDFæ–‡ä»¶
        parsed_result = loader.parse(tray_urdf_path)
        
        # åªä½¿ç”¨ actor_builders æ–¹å¼
        actor_builders = parsed_result.get("actor_builders", [])
        
        if not actor_builders:
            raise ValueError("æ‰˜ç›˜URDFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°actor_builders")
        
        self.trays = []
        
        # ä½¿ç”¨ actor_builders åŠ è½½æ‰˜ç›˜
        for env_idx in range(self.num_envs):
            builder = actor_builders[0]
            # è®¾ç½®æ‰˜ç›˜ä½ç½® (æ”¾åœ¨æ¡Œé¢ä¸Šï¼Œæœºå™¨äººå‰æ–¹)
            tray_position = [-0.2, 0.0, 0.006]  # æ¡Œé¢é«˜åº¦åŠ ä¸Šæ‰˜ç›˜åº•éƒ¨åŽšåº¦
            builder.initial_pose = sapien.Pose(p=tray_position)
            builder.set_scene_idxs([env_idx])
            
            # ä½¿ç”¨ build_static åˆ›å»ºé™æ€æ‰˜ç›˜ï¼Œç¡®ä¿ä¸ä¼šç§»åŠ¨
            tray = builder.build_static(name=f"tray_{env_idx}")
            self.trays.append(tray)
        
        # åˆå¹¶æ‰€æœ‰æ‰˜ç›˜
        if self.trays:
            self.merged_trays = Actor.merge(self.trays, name="all_trays")
        
        #print(f"æˆåŠŸåŠ è½½æ‰˜ç›˜ï¼Œå…± {len(self.trays)} ä¸ª")

    def _generate_object_position_in_tray(self, stack_level=0):
        """åœ¨æ‰˜ç›˜å†…ç”Ÿæˆç‰©ä½“ä½ç½®"""
        # æ‰˜ç›˜ä¸­å¿ƒä½ç½®
        tray_center_x = -0.2
        tray_center_y = 0.0
        tray_bottom_z = 0.02 + 0.02  # æ‰˜ç›˜åº•éƒ¨ + å°åç§»
        
        # æ‰˜ç›˜è¾¹ç•Œè®¡ç®—ï¼ˆåŸºäºŽURDFæ–‡ä»¶ä¸­çš„è¾¹ç•Œå¢™ä½ç½®ï¼‰
        # è¾¹ç•Œå¢™åœ¨æ‰˜ç›˜ä¸­å¿ƒçš„Â±0.2ç±³å¤„
        # å®žé™…å¯ç”¨ç©ºé—´ï¼šä»Žä¸­å¿ƒå‘ä¸¤è¾¹å„0.18ç±³ï¼ˆç•™å‡ºå®‰å…¨è¾¹è·ï¼‰
        safe_spawn_area_x = 0.18
        safe_spawn_area_y = 0.18
        
        # åœ¨æ‰˜ç›˜å†…éšæœºç”Ÿæˆxyä½ç½®
        x = tray_center_x + random.uniform(-safe_spawn_area_x, safe_spawn_area_x)
        y = tray_center_y + random.uniform(-safe_spawn_area_y, safe_spawn_area_y)
        
        # å †å é«˜åº¦
        z = tray_bottom_z + stack_level * 0.04  # æ¯å±‚é«˜åº¦
        
        return x, y, z

    def _get_object_size(self, obj_type):
        """èŽ·å–ç‰©ä½“çš„å¤§å°ä¿¡æ¯ï¼Œè€ƒè™‘ç¼©æ”¾æ•ˆæžœ"""
        # åŸºäºŽYCBæ•°æ®é›†çš„å®žé™…ç‰©ä½“å°ºå¯¸ï¼ˆå•ä½ï¼šç±³ï¼‰
        base_sizes = {
            #"003_cracker_box": [0.16, 0.21, 0.07],         # é¥¼å¹²ç›’: 16cm x 21cm x 7cm
            "004_sugar_box": [0.09, 0.175, 0.044],         # ç³–ç›’: 9cm x 17.5cm x 4.4cm
            "006_mustard_bottle": [0.095, 0.095, 0.177],   # èŠ¥æœ«ç“¶: 9.5cm x 9.5cm x 17.7cm
            "008_pudding_box": [0.078, 0.109, 0.032],      # å¸ƒä¸ç›’: 7.8cm x 10.9cm x 3.2cm
            "009_gelatin_box": [0.028, 0.085, 0.114],      # æ˜Žèƒ¶ç›’: 2.8cm x 8.5cm x 11.4cm  
            #"010_potted_meat_can": [0.101, 0.051, 0.051],  # ç½è£…è‚‰ç½å¤´: 10.1cm x 5.1cm x 5.1cm
        }
        
        base_size = base_sizes.get(obj_type, [0.05, 0.05, 0.05])
        
        # å¯¹äºŽ004_sugar_boxåº”ç”¨0.7å€ç¼©æ”¾
        if obj_type == "004_sugar_box":
            return [dim * 0.7 for dim in base_size]  # ç¼©æ”¾åˆ°åŽŸæ¥çš„70%
        else:
            return base_size

    def _sample_target_objects(self):
        """éšæœºé€‰æ‹©ç›®æ ‡ç‰©ä½“"""
        target_objects = []
        self.target_object_indices = []
        
        for env_idx in range(self.num_envs):
            if len(self.selectable_objects[env_idx]) > 0:
                # éšæœºé€‰æ‹©ä¸€ä¸ªå°šæœªæŠ“å–çš„ç‰©ä½“
                available_objects = [obj for i, obj in enumerate(self.selectable_objects[env_idx]) 
                                   if i not in getattr(self, 'completed_objects', set())]
                
                if available_objects:
                    target_obj = random.choice(available_objects)
                    target_idx = self.selectable_objects[env_idx].index(target_obj)
                    target_objects.append(target_obj)
                    self.target_object_indices.append(target_idx)
                else:
                    # å¦‚æžœæ²¡æœ‰å¯é€‰æ‹©çš„ç‰©ä½“ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç‰©ä½“ä½œä¸ºå ä½ç¬¦
                    target_objects.append(self.selectable_objects[env_idx][0])
                    self.target_object_indices.append(0)
        
        if target_objects:
            self.target_object = Actor.merge(target_objects, name="target_object")
            
        # åˆå§‹åŒ–å®Œæˆçš„ç‰©ä½“é›†åˆ
        if not hasattr(self, 'completed_objects'):
            self.completed_objects = set()

    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):
        """åˆå§‹åŒ–æ¯ä¸ªepisode"""
        with torch.device(self.device):
            b = len(env_idx)
            self.scene_builder.initialize(env_idx)
            
            # é‡ç½®æ‰˜ç›˜ä½ç½®
            if hasattr(self, 'merged_trays'):
                # åœ¨GPUä»¿çœŸä¸­ï¼Œé™æ€å¯¹è±¡ä¸èƒ½æ”¹å˜ä½å§¿ï¼Œæ‰€ä»¥è·³è¿‡
                if not self.scene.gpu_sim_enabled:
                    if b == self.num_envs:
                        self.merged_trays.pose = self.merged_trays.initial_pose
                    else:
                        mask = torch.isin(self.merged_trays._scene_idxs, env_idx)
                        self.merged_trays.pose = self.merged_trays.initial_pose[mask]
                else:
                    #print("GPUä»¿çœŸæ¨¡å¼ä¸‹è·³è¿‡é™æ€æ‰˜ç›˜ä½å§¿é‡ç½®")
                    pass
            
            # é‡ç½®ç‰©ä½“åˆ°åˆå§‹ä½ç½®
            if hasattr(self, 'merged_objects'):
                if b == self.num_envs:
                    self.merged_objects.pose = self.merged_objects.initial_pose
                else:
                    mask = torch.isin(self.merged_objects._scene_idxs, env_idx)
                    self.merged_objects.pose = self.merged_objects.initial_pose[mask]
            
            # è®¾ç½®ç›®æ ‡ä½ç½® - å›ºå®šåœ¨æ‰˜ç›˜å³ä¾§
            goal_pos = torch.zeros((b, 3), device=self.device)
            
            # æ‰˜ç›˜ä¸­å¿ƒä½ç½®ï¼š[-0.2, 0.0, 0.02]
            # æ‰˜ç›˜å°ºå¯¸ï¼šé•¿0.6mï¼Œå®½0.6m
            # ç›®æ ‡ä½ç½®è®¾å®šåœ¨æ‰˜ç›˜å³ä¾§å¤–10cmå¤„ï¼Œé¿å…ä¸Žæ‰˜ç›˜è¾¹ç•Œå†²çª
            goal_pos[:, 0] = -0.4  # æ‰˜ç›˜å³ä¾§çš„å›ºå®šä½ç½®
            goal_pos[:, 1] = 0.4  
            goal_pos[:, 2] = 0.05  # æ¡Œé¢é«˜åº¦5cmï¼Œç¡®ä¿ç‰©ä½“ç¨³å®šæ”¾ç½®
            
            self.goal_pos = goal_pos
            self.goal_site.set_pose(Pose.create_from_pq(self.goal_pos))
            
            # è®°å½•åˆå§‹ç‰©ä½“ä½ç½®ï¼ˆç”¨äºŽè®¡ç®—ä½ç§»ï¼‰
            self.initial_object_positions = []
            for i in range(b):
                env_positions = []
                for obj in self.all_objects:
                    if hasattr(obj, '_scene_idxs') and len(obj._scene_idxs) > 0:
                        if obj._scene_idxs[0] == env_idx[i]:
                            env_positions.append(obj.pose.p.clone())
                self.initial_object_positions.append(env_positions)
            
            
            # é‡æ–°é€‰æ‹©ç›®æ ‡ç‰©ä½“ - åªåœ¨è¿žç»­åŠ¨ä½œæ¨¡å¼ä¸‹ä½¿ç”¨
            if not self.use_discrete_action:
                self._sample_target_objects()
        
            
            
            
            # é‡ç½®ä»»åŠ¡ç›¸å…³å˜é‡
            self.grasped_objects_count = 0
            self.completed_objects = set()
            
            # è®°å½•æ‰€æœ‰ç‰©ä½“çš„åˆå§‹ä½ç½®ï¼ˆç”¨äºŽè®¡ç®—ä½ç§»ï¼‰
            self.initial_positions = {}
            for i, obj in enumerate(self.all_objects):
                if hasattr(obj, 'pose'):
                    self.initial_positions[i] = obj.pose.p.clone()
            
            # é€‰æ‹©æ–°çš„ç›®æ ‡ç‰©ä½“
            self._sample_target_objects()
            
            # åˆå§‹å§¿æ€é‡ç½®
            target_qpos = np.array([-1.6137, 1.3258, 1.9346, -0.8884, -1.6172, 1.0867, -3.0494, 0.04, 0.04])
            self.agent.reset(target_qpos)
            #self.agent.reset()

    def _get_obs_extra(self, info: Dict):
        """èŽ·å–é¢å¤–è§‚æµ‹ä¿¡æ¯ - ç®€åŒ–ç‰ˆæœ¬"""
        obs = dict(
            is_grasped=info["is_grasped"],
            tcp_pose=self.agent.tcp.pose.raw_pose,
            grasped_count=info["grasped_count"],
        )
        
        if "state" in self.obs_mode:
            if hasattr(self, 'target_object') and self.target_object is not None:
                obs.update(
                    target_obj_pose=self.target_object.pose.raw_pose,
                    tcp_to_obj_pos=self.target_object.pose.p - self.agent.tcp.pose.p,
                )
            else:
                # æä¾›é›¶å€¼ä½œä¸ºå ä½ç¬¦
                batch_size = self.num_envs
                zero_pose = torch.zeros((batch_size, 7), device=self.device)
                zero_pos = torch.zeros((batch_size, 3), device=self.device)
                obs.update(
                    target_obj_pose=zero_pose,
                    tcp_to_obj_pos=zero_pos,
                )
            
            # æ·»åŠ ä»»åŠ¡è¿›åº¦ä¿¡æ¯
            obs.update(
                progress_ratio=torch.tensor([
                    self.grasped_objects_count / self.total_objects_per_env
                ] * self.num_envs, device=self.device),
            )
        
        return obs


    def evaluate(self):
        """è¯„ä¼°ä»»åŠ¡å®Œæˆæƒ…å†µ"""
        success = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        is_grasped = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        is_robot_static = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        
        if hasattr(self, 'target_object') and self.target_object is not None:
            # æ£€æŸ¥å½“å‰ç›®æ ‡ç‰©ä½“æ˜¯å¦è¢«æŠ“å–
            is_grasped = self.agent.is_grasping(self.target_object)
            
            # æ£€æŸ¥æœºå™¨äººæ˜¯å¦é™æ­¢
            is_robot_static = self.agent.is_static(0.2)
        
        # æˆåŠŸæ¡ä»¶ï¼šæ‰€æœ‰ç‰©ä½“éƒ½è¢«æˆåŠŸæŠ“å–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥å·²æŠ“å–ç‰©ä½“æ•°é‡æ˜¯å¦ç­‰äºŽæ€»æ•°
        success = torch.tensor([
            self.grasped_objects_count >= self.total_objects_per_env
        ] * self.num_envs, device=self.device, dtype=bool)
        
        return {
            "success": success,
            "is_grasped": is_grasped,
            "is_robot_static": is_robot_static,
            "grasped_count": torch.tensor([self.grasped_objects_count] * self.num_envs, device=self.device),
        }

    def _calculate_other_objects_displacement(self):
        """è®¡ç®—å…¶ä»–ç‰©ä½“çš„ä½ç§»è·ç¦»"""
        total_displacement = torch.zeros(self.num_envs, device=self.device)
        
        for env_idx in range(self.num_envs):
            displacement = 0.0
            obj_count = 0
            
            for i, obj in enumerate(self.all_objects):
                if hasattr(obj, '_scene_idxs') and len(obj._scene_idxs) > 0:
                    if obj._scene_idxs[0] == env_idx:
                        # è·³è¿‡ç›®æ ‡ç‰©ä½“
                        if hasattr(self, 'target_object_indices') and env_idx < len(self.target_object_indices):
                            if i == self.target_object_indices[env_idx]:
                                continue
                        
                        # è®¡ç®—ä½ç§»
                        if hasattr(self, 'initial_object_positions') and env_idx < len(self.initial_object_positions):
                            if obj_count < len(self.initial_object_positions[env_idx]):
                                initial_pos = self.initial_object_positions[env_idx][obj_count]
                                current_pos = obj.pose.p
                                displacement += torch.linalg.norm(current_pos - initial_pos)
                                obj_count += 1
            
            total_displacement[env_idx] = displacement
        
        return total_displacement

    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è®¡ç®—å¥–åŠ± - åªåœ¨æŠ“å–ç»“æŸæ—¶ç»™å¥–åŠ±"""
        reward = torch.zeros(self.num_envs, device=self.device)
        
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæŠ“å–å¥–åŠ±ï¼ˆç¨€ç–å¥–åŠ±ï¼Œåªåœ¨æˆåŠŸæ—¶ç»™äºˆï¼‰
        grasp_reward = self._compute_grasp_reward(info)
        
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šä½ç§»æƒ©ç½šï¼ˆå‡è½»æƒ©ç½šå¼ºåº¦ï¼‰
        displacement_penalty = self._compute_displacement_penalty()
        
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šè½»å¾®æ—¶é—´æƒ©ç½šï¼ˆé¿å…è¿‡åº¦è´Ÿå¥–åŠ±ï¼‰
        time_penalty = 0.001  # éžå¸¸å°çš„æ—¶é—´æƒ©ç½š
        
        # ç»„åˆå¥–åŠ±
        reward = grasp_reward - displacement_penalty * 0.1 - time_penalty  # å‡è½»æƒ©ç½šæƒé‡
        
        # å®Œæˆæ‰€æœ‰ç‰©ä½“çš„å¤§å¥–åŠ±
        if info.get("success", False).any():
            completion_bonus = 20.0  # å¢žåŠ å®Œæˆå¥–åŠ±
            reward[info["success"]] += completion_bonus
        
        return reward
    
    def _compute_grasp_reward(self, info: Dict):
        """è®¡ç®—æŠ“å–å¥–åŠ± - ç¬¬ä¸€ä¼˜å…ˆçº§ï¼Œåªåœ¨æˆåŠŸæŠ“å–æ—¶ç»™å¥–åŠ±"""
        reward = torch.zeros(self.num_envs, device=self.device)
        
        if hasattr(self, 'target_object') and self.target_object is not None:
            # æ£€æŸ¥æŠ“å–çŠ¶æ€
            is_grasped = info.get("is_grasped", torch.zeros_like(reward, dtype=bool))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æŠ“å–ï¼ˆä¸Žä¸Šä¸€æ­¥æ¯”è¾ƒï¼‰
            if not hasattr(self, '_prev_grasped'):
                self._prev_grasped = torch.zeros_like(is_grasped)
            
            # åªæœ‰æ–°æŠ“å–æˆåŠŸæ—¶æ‰ç»™å¥–åŠ±
            new_grasp = is_grasped & (~self._prev_grasped)
            if new_grasp.any():
                self.grasped_objects_count += new_grasp.sum().item()
                reward += new_grasp.float() * 10.0  # æˆåŠŸæŠ“å–ç»™10åˆ†å¤§å¥–åŠ±
                print(f"ðŸŽ‰ æˆåŠŸæŠ“å–! å½“å‰å·²æŠ“å–æ•°é‡: {self.grasped_objects_count}")
                
                # é€‰æ‹©ä¸‹ä¸€ä¸ªç›®æ ‡ç‰©ä½“
                self._update_target_object()
            
            self._prev_grasped = is_grasped
            
        return reward
    
    def _update_target_object(self):
        """æ›´æ–°ç›®æ ‡ç‰©ä½“ - é€‰æ‹©ä¸‹ä¸€ä¸ªæœªæŠ“å–çš„ç‰©ä½“"""
        if not hasattr(self, 'completed_objects'):
            self.completed_objects = set()
        
        # å°†å½“å‰ç›®æ ‡ç‰©ä½“æ ‡è®°ä¸ºå·²å®Œæˆ
        if hasattr(self, 'target_object_indices') and self.target_object_indices:
            for env_idx, target_idx in enumerate(self.target_object_indices):
                if env_idx < len(self.target_object_indices):
                    self.completed_objects.add(target_idx)
        
        # é‡æ–°é€‰æ‹©ç›®æ ‡ç‰©ä½“
        self._sample_target_objects()
    
    def _compute_displacement_penalty(self):
        """è®¡ç®—ä½ç§»æƒ©ç½š - ç¬¬äºŒä¼˜å…ˆçº§"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè®¡ç®—æ‰€æœ‰éžç›®æ ‡ç‰©ä½“çš„ä½ç§»
        total_displacement = self._calculate_other_objects_displacement()
        # é™åˆ¶æƒ©ç½šåœ¨åˆç†èŒƒå›´å†…
        displacement_penalty = torch.clamp(total_displacement * 0.5, 0, 2.0)
        return displacement_penalty
    
    
   

    def compute_sparse_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è®¡ç®—ç¨€ç–å¥–åŠ±"""
        reward = torch.zeros(self.num_envs, device=self.device)
        
        # åªæœ‰æˆåŠŸæ—¶æ‰ç»™å¥–åŠ±
        reward[info["success"]] = 1.0
        
        # å…¶ä»–ç‰©ä½“ä½ç§»æƒ©ç½š
        other_displacement = self._calculate_other_objects_displacement()
        displacement_penalty = other_displacement * 0.1
        reward -= displacement_penalty
        
        return reward

    def compute_normalized_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è®¡ç®—å½’ä¸€åŒ–å¯†é›†å¥–åŠ±"""
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œnormalized_dense_reward åº”è¯¥æ˜¯å¯¹ dense_reward çš„å½’ä¸€åŒ–
        # ä¸åº”è¯¥æ ¹æ® reward_mode é€‰æ‹©ä¸åŒçš„å¥–åŠ±å‡½æ•°
        dense_reward = self.compute_dense_reward(obs=obs, action=action, info=info)
        return dense_reward / 10.0 


    # AnyGraspåŠŸèƒ½å·²ç§»é™¤ - ç®€åŒ–ä¸ºåŸºæœ¬æŠ“å–çŽ¯å¢ƒ
    
    # ç›¸æœºè§‚æµ‹åŠŸèƒ½ç®€åŒ– - ä¸“æ³¨äºŽåŸºæœ¬æŠ“å–ä»»åŠ¡
    
    # ä»¥ä¸‹æ‰€æœ‰AnyGraspç›¸å…³åŠŸèƒ½å·²ç§»é™¤ï¼Œä¸“æ³¨äºŽåŸºæœ¬æŠ“å–ä»»åŠ¡
    