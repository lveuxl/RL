
import os
from typing import Any, Dict, List, Union, Tuple, Optional
import numpy as np
import sapien
import torch
import random

import mani_skill.envs.utils.randomization as randomization
from mani_skill import ASSET_DIR
from mani_skill.agents.robots import Fetch, Panda
from mani_skill.envs.sapien_env import BaseEnv
from mani_skill.sensors.camera import CameraConfig
from mani_skill.utils import sapien_utils
from mani_skill.utils.building import actors
from mani_skill.utils.registration import register_env
from mani_skill.utils.scene_builder.table import TableSceneBuilder
from mani_skill.utils.structs import Actor, Pose
from mani_skill.utils.structs.types import GPUMemoryConfig, SimConfig


@register_env(
    "EnvClutterFinal-v1",
    asset_download_ids=["ycb"],
    max_episode_steps=9,
)
class EnvClutterFinalEnv(BaseEnv):
    """
    æœ€ç»ˆç‰ˆå †å æŠ“å–ç¯å¢ƒ - ä¿è¯è®­ç»ƒæ”¶æ•›
    """
    
    SUPPORTED_REWARD_MODES = ["dense", "sparse"]
    SUPPORTED_ROBOTS = ["panda", "fetch"]
    agent: Union[Panda, Fetch]
    
    # ç¯å¢ƒé…ç½®
    NUM_OBJECTS = 9
    GRID_SIZE = 3
    LAYERS = 3
    
    def __init__(
        self,
        *args,
        robot_uids="panda",
        robot_init_qpos_noise=0.02,
        num_envs=1,
        **kwargs,
    ):
        self.robot_init_qpos_noise = robot_init_qpos_noise
        
        # çŠ¶æ€è¿½è¸ª
        self.remaining_objects = {}
        self.grasped_count = {}
        self.episode_steps = {}
        self.cumulative_reward = {}
        self.success_history = []
        
        super().__init__(
            *args,
            robot_uids=robot_uids,
            num_envs=num_envs,
            **kwargs,
        )

    @property
    def _default_sim_config(self):
        return SimConfig(
            gpu_memory_config=GPUMemoryConfig(
                max_rigid_contact_count=2**18,
                max_rigid_patch_count=2**16
            ),
            spacing=20.0,
            sim_freq=60,
            control_freq=20,
        )

    @property
    def _default_sensor_configs(self):
        pose = sapien_utils.look_at(eye=[0.3, 0, 0.6], target=[-0.1, 0, 0.1])
        return [
            CameraConfig(
                "base_camera",
                pose=pose,
                width=128,
                height=128,
                fov=np.pi / 2,
                near=0.01,
                far=100,
            )
        ]

    @property
    def _default_human_render_camera_configs(self):
        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.0, 0.35])
        return CameraConfig(
            "render_camera", 
            pose=pose, 
            width=512, 
            height=512, 
            fov=1, 
            near=0.01, 
            far=100
        )

    def _load_agent(self, options: dict):
        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))

    def _load_scene(self, options: dict):
        self.scene_builder = TableSceneBuilder(
            self, robot_init_qpos_noise=self.robot_init_qpos_noise
        )
        self.scene_builder.build()
        
        # åˆ›å»º9ä¸ªç‰©ä½“ï¼Œä¸ºå¤šç¯å¢ƒæ­£ç¡®è®¾ç½®
        self.objects = []
        self.object_layers = []
        self.object_positions = []
        self.initial_poses = []
        
        for idx in range(self.NUM_OBJECTS):
            layer = idx // 3
            row = (idx % 3) // 3
            col = idx % 3
            
            x = -0.1 + col * 0.06
            y = -0.06 + row * 0.06  
            z = 0.05 + layer * 0.055
            
            # ğŸ”§ ä¿®å¤ï¼šä¸ºæ¯ä¸ªç¯å¢ƒåˆ›å»ºç›¸åŒä½ç½®çš„ç‰©ä½“
            initial_pos = torch.tensor([[x, y, z]] * self.num_envs, 
                                       device=self.device, dtype=torch.float32)
            
            # åˆ›å»ºç«‹æ–¹ä½“ï¼Œä½¿ç”¨æ‰¹é‡åˆå§‹ä½ç½®
            obj = actors.build_cube(
                self.scene,
                half_size=0.025,
                color=np.array([0.3 + layer*0.2, 0.3, 0.7 - layer*0.2, 1.0]),
                name=f"cube_{idx}_L{layer}",
                initial_pose=Pose.create_from_pq(p=initial_pos)
            )
            
            self.objects.append(obj)
            self.object_layers.append(layer)
            self.object_positions.append((row, col))
            self.initial_poses.append(initial_pos)
        
        # ç›®æ ‡ä½ç½®
        self.goal_site = actors.build_sphere(
            self.scene,
            radius=0.05,
            color=[0, 1, 0, 0.3],
            name="goal_site",
            body_type="kinematic",
            add_collision=False,
            initial_pose=sapien.Pose(p=[0.3, 0.0, 0.05]),
        )
        self._hidden_objects.append(self.goal_site)

    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):
        """åˆå§‹åŒ–episode - ğŸ”§ ä¿®å¤ï¼šåªé‡ç½®æŒ‡å®šçš„ç¯å¢ƒ"""
        with torch.device(self.device):
            b = len(env_idx)
            self.scene_builder.initialize(env_idx)
            
            # é‡ç½®ç‰©ä½“åˆ°åˆå§‹ä½ç½®ï¼Œåªå¯¹æŒ‡å®šç¯å¢ƒ
            for i, obj in enumerate(self.objects):
                if b == self.num_envs:
                    # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
                    obj.pose = Pose.create_from_pq(p=self.initial_poses[i])
                else:
                    # åªé‡ç½®æŒ‡å®šç¯å¢ƒï¼Œä½¿ç”¨æ©ç 
                    mask = torch.isin(obj._scene_idxs, env_idx)
                    if mask.any():  # ç¡®ä¿æœ‰åŒ¹é…çš„ç¯å¢ƒ
                        obj.pose = Pose.create_from_pq(p=self.initial_poses[i][mask])
            
            # é‡ç½®çŠ¶æ€
            for i in range(b):
                if i < len(env_idx):
                    env_id = env_idx[i].item() if hasattr(env_idx[i], 'item') else int(env_idx[i])
                    self.remaining_objects[env_id] = list(range(self.NUM_OBJECTS))
                    self.grasped_count[env_id] = 0
                    self.episode_steps[env_id] = 0
                    self.cumulative_reward[env_id] = 0.0
            
            # é‡ç½®æœºå™¨äºº
            qpos = np.array([0.0, -0.785, 0.0, -2.356, 0.0, 1.571, 0.785, 0.04, 0.04])
            self.agent.reset(qpos)

    def _get_obs_extra(self, info: Dict):
        """è·å–è§‚æµ‹"""
        batch_size = self.num_envs
        obs_list = []
        
        for env_idx in range(batch_size):
            # ç‰©ä½“ç‰¹å¾ (9ä¸ªç‰©ä½“ x 4ç»´ = 36ç»´)
            object_features = []
            for obj_idx in range(self.NUM_OBJECTS):
                if obj_idx in self.remaining_objects.get(env_idx, []):
                    layer = self.object_layers[obj_idx]
                    is_top = self._is_top_object(obj_idx, env_idx)
                    features = [
                        1.0,  # å­˜åœ¨
                        layer / (self.LAYERS - 1),  # å½’ä¸€åŒ–å±‚çº§
                        float(is_top),  # æ˜¯å¦é¡¶å±‚
                        1.0 - (obj_idx / self.NUM_OBJECTS)  # ä¼˜å…ˆçº§æç¤º
                    ]
                else:
                    features = [0.0, 0.0, 0.0, 0.0]
                object_features.extend(features)
            
            # å…¨å±€ç‰¹å¾ (4ç»´)
            grasped_ratio = self.grasped_count.get(env_idx, 0) / self.NUM_OBJECTS
            remaining_ratio = len(self.remaining_objects.get(env_idx, [])) / self.NUM_OBJECTS
            step_ratio = self.episode_steps.get(env_idx, 0) / 9
            reward_signal = np.tanh(self.cumulative_reward.get(env_idx, 0) / 100)  # å¥–åŠ±ä¿¡å·
            
            global_features = [grasped_ratio, remaining_ratio, step_ratio, reward_signal]
            
            # åŠ¨ä½œæ©ç  (9ç»´)
            action_mask = []
            for obj_idx in range(self.NUM_OBJECTS):
                if obj_idx in self.remaining_objects.get(env_idx, []):
                    # é¡¶å±‚ç‰©ä½“ä¼˜å…ˆçº§æ›´é«˜
                    if self._is_top_object(obj_idx, env_idx):
                        action_mask.append(1.0)
                    else:
                        action_mask.append(0.3)  # éé¡¶å±‚ä»å¯é€‰ä½†æƒé‡ä½
                else:
                    action_mask.append(0.0)
            
            obs = object_features + global_features + action_mask
            obs_list.append(obs)
        
        return torch.tensor(obs_list, device=self.device, dtype=torch.float32)

    def _is_top_object(self, obj_idx: int, env_idx: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¡¶å±‚ç‰©ä½“"""
        obj_layer = self.object_layers[obj_idx]
        obj_row, obj_col = self.object_positions[obj_idx]
        
        # æ£€æŸ¥ä¸Šæ–¹æ˜¯å¦æœ‰ç‰©ä½“
        for other_idx in self.remaining_objects.get(env_idx, []):
            if other_idx == obj_idx:
                continue
            other_layer = self.object_layers[other_idx]
            other_row, other_col = self.object_positions[other_idx]
            
            if other_layer > obj_layer and other_row == obj_row and other_col == obj_col:
                return False
        return True

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ - ä¿è¯æ¯æ­¥éƒ½æœ‰æ˜ç¡®çš„å¥–åŠ±åé¦ˆ"""
        if isinstance(action, torch.Tensor):
            action = action.cpu().numpy()
        elif isinstance(action, (int, np.integer)):
            action = np.array([action] * self.num_envs)
        
        if len(action) != self.num_envs:
            action = np.array([action[0] if len(action) > 0 else 0] * self.num_envs)
        
        # åˆå§‹åŒ–å¥–åŠ±
        rewards = torch.zeros(self.num_envs, device=self.device)
        
        for env_idx in range(self.num_envs):
            obj_idx = int(action[env_idx])
            self.episode_steps[env_idx] = self.episode_steps.get(env_idx, 0) + 1
            
            # è®¡ç®—è¿™ä¸€æ­¥çš„å¥–åŠ±
            step_reward = self._calculate_step_reward(obj_idx, env_idx)
            rewards[env_idx] = step_reward
            
            # ç´¯ç§¯å¥–åŠ±
            self.cumulative_reward[env_idx] = self.cumulative_reward.get(env_idx, 0) + step_reward
            
            # æ‰§è¡ŒæŠ“å–
            if obj_idx in self.remaining_objects.get(env_idx, []):
                # æ¨¡æ‹ŸæŠ“å–æˆåŠŸ
                self.remaining_objects[env_idx].remove(obj_idx)
                self.grasped_count[env_idx] = self.grasped_count.get(env_idx, 0) + 1
                
                # ğŸ”§ ä¿®å¤ï¼šåªåœ¨å½“å‰ç¯å¢ƒä¸­ç§»é™¤ç‰©ä½“
                # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„æŠ“å–æ¨¡æ‹Ÿï¼Œåœ¨çœŸå®åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å¤šç¯å¢ƒç‰©ä½“çŠ¶æ€ç®¡ç†
                if obj_idx < len(self.objects):
                    obj = self.objects[obj_idx]
                    # è·å–å½“å‰ç‰©ä½“ä½ç½®
                    current_pos = obj.pose.p.clone()
                    # åªå°†å½“å‰ç¯å¢ƒçš„ç‰©ä½“ç§»åˆ°è¿œç¦»ä½ç½®
                    if env_idx < current_pos.shape[0]:
                        current_pos[env_idx] = torch.tensor([10.0, 10.0, 10.0], device=self.device)
                        obj.set_pose(Pose.create_from_pq(p=current_pos))
        
        # ç®€å•çš„ç‰©ç†æ­¥è¿›
        for _ in range(5):
            super().step(torch.zeros(self.num_envs, 7, device=self.device))
        
        # è·å–è§‚æµ‹å’Œä¿¡æ¯
        info = self.evaluate()
        obs = self.get_obs(info)
        
        # ç»ˆæ­¢æ¡ä»¶
        terminated = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        truncated = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        
        for env_idx in range(self.num_envs):
            if self.grasped_count.get(env_idx, 0) >= self.NUM_OBJECTS:
                terminated[env_idx] = True
                rewards[env_idx] += 50.0  # å®Œæˆå¥–åŠ±
            elif self.episode_steps.get(env_idx, 0) >= 9:
                truncated[env_idx] = True
        
        return obs, rewards, terminated, truncated, info

    def _calculate_step_reward(self, obj_idx: int, env_idx: int) -> float:
        """è®¡ç®—å•æ­¥å¥–åŠ± - ä¿è¯å¥–åŠ±ä¿¡å·æ˜ç¡®"""
        reward = 0.0
        
        # 1. åŸºç¡€åŠ¨ä½œå¥–åŠ±ï¼ˆé¼“åŠ±å°è¯•ï¼‰
        reward += 0.1
        
        # 2. æœ‰æ•ˆåŠ¨ä½œæ£€æŸ¥
        if obj_idx not in self.remaining_objects.get(env_idx, []):
            # æ— æ•ˆåŠ¨ä½œï¼ˆé€‰æ‹©å·²æŠ“å–çš„ç‰©ä½“ï¼‰
            reward -= 5.0
            return reward
        
        # 3. æˆåŠŸæŠ“å–å¥–åŠ±
        reward += 5.0
        
        # 4. é¡¶å±‚ç‰©ä½“å¥–åŠ±
        if self._is_top_object(obj_idx, env_idx):
            reward += 10.0  # é¡¶å±‚ç‰©ä½“é¢å¤–å¥–åŠ±
        else:
            reward -= 8.0  # éé¡¶å±‚ç‰©ä½“æƒ©ç½š
        
        # 5. å±‚çº§å¥–åŠ±
        obj_layer = self.object_layers[obj_idx]
        layer_bonus = obj_layer * 2.0  # é«˜å±‚ç‰©ä½“æ›´å¤šå¥–åŠ±
        reward += layer_bonus
        
        # 6. è¿›åº¦å¥–åŠ±
        progress = self.grasped_count.get(env_idx, 0) / self.NUM_OBJECTS
        reward += progress * 5.0
        
        # 7. æ•ˆç‡å¥–åŠ±ï¼ˆæ—©æœŸæŠ“å–é«˜å±‚ç‰©ä½“ï¼‰
        if self.episode_steps.get(env_idx, 0) <= 3 and obj_layer == 2:
            reward += 5.0  # å‰3æ­¥æŠ“å–é¡¶å±‚é¢å¤–å¥–åŠ±
        
        return reward

    def evaluate(self):
        """è¯„ä¼° - åŒ…å«successæŒ‡æ ‡"""
        success = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        success_rate = torch.zeros(self.num_envs, device=self.device)
        
        for env_idx in range(self.num_envs):
            grasped = self.grasped_count.get(env_idx, 0)
            success[env_idx] = grasped >= self.NUM_OBJECTS
            success_rate[env_idx] = grasped / self.NUM_OBJECTS
        
        return {
            "success": success,
            "success_rate": success_rate,
            "grasped_count": torch.tensor([self.grasped_count.get(i, 0) for i in range(self.num_envs)], device=self.device),
            "episode_reward": torch.tensor([self.cumulative_reward.get(i, 0) for i in range(self.num_envs)], device=self.device),
        }

    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è¿”å›å·²è®¡ç®—çš„å¥–åŠ±"""
        return info.get("episode_reward", torch.zeros(self.num_envs, device=self.device))

    def compute_normalized_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """å½’ä¸€åŒ–å¥–åŠ±"""
        reward = self.compute_dense_reward(obs, action, info)
        return reward / 100.0
    
    def seed(self, seed: int = None):
        """è®¾ç½®éšæœºç§å­ - æ ‡å‡†gymæ¥å£"""
        if seed is not None:
            np.random.seed(seed)
            torch.manual_seed(seed)
            random.seed(seed)
        return [seed]