import os
import time
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical  # æ”¹ä¸ºCategoricalåˆ†å¸ƒ
from torch.utils.tensorboard import SummaryWriter
import gymnasium as gym
from collections import deque
import mani_skill.envs
from env_clutter import EnvClutterEnv
from utils import CsvLogger  # å¯¼å…¥CsvLogger
# æ–°å¢ï¼šå¯¼å…¥è§†é¢‘å½•åˆ¶ç›¸å…³æ¨¡å—
from mani_skill.utils.wrappers.record import RecordEpisode
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
import warnings
warnings.filterwarnings("ignore")

class PPOActor(nn.Module):
    """PPO Actorç½‘ç»œ - æ”¯æŒç¦»æ•£åŠ¨ä½œ"""
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(PPOActor, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.logits_layer = nn.Linear(hidden_dim, action_dim)  # è¾“å‡ºlogits
        
    def forward(self, state, mask=None):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        logits = self.logits_layer(x)
        
        # ä¸åœ¨forwardä¸­åº”ç”¨æ©ç ï¼Œåªè¿”å›åŸå§‹logits
        return logits
    
    def get_action(self, state, mask=None):
        logits = self.forward(state, mask)
        
        # å±è”½éæ³•åŠ¨ä½œ
        if mask is not None:
            # ç¡®ä¿æ©ç æ˜¯0/1å€¼
            if not torch.all((mask == 0) | (mask == 1)):
                # å¦‚æœæ©ç ä¸æ˜¯0/1å€¼ï¼Œå°†å…¶è½¬æ¢ä¸º0/1å€¼
                # å‡è®¾éé›¶å€¼è¡¨ç¤ºæœ‰æ•ˆåŠ¨ä½œ
                mask = (mask != 0).float()
            
            logits = torch.where(mask.bool(), logits, torch.tensor(-1e8, device=logits.device))
        
        dist = Categorical(logits=logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action, log_prob
    
    def evaluate_action(self, state, action, mask=None):
        logits = self.forward(state, mask)
        
        # å±è”½éæ³•åŠ¨ä½œ
        if mask is not None:
            # ç¡®ä¿æ©ç æ˜¯0/1å€¼
            if not torch.all((mask == 0) | (mask == 1)):
                # å¦‚æœæ©ç ä¸æ˜¯0/1å€¼ï¼Œå°†å…¶è½¬æ¢ä¸º0/1å€¼
                # å‡è®¾éé›¶å€¼è¡¨ç¤ºæœ‰æ•ˆåŠ¨ä½œ
                mask = (mask != 0).float()
            
            logits = torch.where(mask.bool(), logits, torch.tensor(-1e8, device=logits.device))
        
        dist = Categorical(logits=logits)
        log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        return log_prob, entropy

class PPOCritic(nn.Module):
    """PPO Criticç½‘ç»œ"""
    def __init__(self, state_dim, hidden_dim=256):
        super(PPOCritic, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.value_layer = nn.Linear(hidden_dim, 1)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        value = self.value_layer(x)
        return value

class PPOAgent:
    """PPOæ™ºèƒ½ä½“"""
    def __init__(self, state_dim, action_dim, lr_actor=3e-4, lr_critic=3e-4, gamma=0.99, 
                 gae_lambda=0.95, clip_epsilon=0.2, entropy_coef=0.01, value_coef=0.5):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç½‘ç»œ
        self.actor = PPOActor(state_dim, action_dim).to(self.device)
        self.critic = PPOCritic(state_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        
    def get_action(self, state, mask=None):
        state = torch.FloatTensor(state).to(self.device)
        if mask is not None:
            mask = torch.FloatTensor(mask).to(self.device)
        with torch.no_grad():
            action, log_prob = self.actor.get_action(state, mask)
        return action.cpu().numpy(), log_prob.item()
    
    def get_value(self, state):
        state = torch.FloatTensor(state).to(self.device)
        with torch.no_grad():
            value = self.critic(state)
        return value.item()
    
    def compute_gae(self, rewards, values, next_values, dones):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡"""
        advantages = []
        gae = 0
        
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = next_values
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)
        
        return advantages
    
    def update(self, states, actions, old_log_probs, rewards, values, dones, masks=None, epochs=10):
        """æ›´æ–°ç½‘ç»œ - é€‚é…ç¦»æ•£åŠ¨ä½œ"""
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(np.array(actions)).to(self.device)  # ç¦»æ•£åŠ¨ä½œç”¨LongTensor
        
        # å¤„ç†masks
        if masks is not None:
            masks = torch.FloatTensor(np.array(masks)).to(self.device)
        
        # å¤„ç†old_log_probs
        old_log_probs_array = []
        for log_prob in old_log_probs:
            if isinstance(log_prob, torch.Tensor):
                old_log_probs_array.append(log_prob.item() if log_prob.numel() == 1 else log_prob.cpu().numpy())
            elif isinstance(log_prob, np.ndarray):
                old_log_probs_array.append(log_prob.item() if log_prob.size == 1 else log_prob)
            else:
                old_log_probs_array.append(float(log_prob))
        old_log_probs = torch.FloatTensor(old_log_probs_array).to(self.device)
        
        rewards = torch.FloatTensor(rewards).to(self.device)
        
        # å¤„ç†values
        values_array = []
        for value in values:
            if isinstance(value, torch.Tensor):
                values_array.append(value.item() if value.numel() == 1 else value.cpu().numpy())
            elif isinstance(value, np.ndarray):
                values_array.append(value.item() if value.size == 1 else value)
            else:
                values_array.append(float(value))
        values = torch.FloatTensor(values_array).to(self.device)
        
        dones = torch.FloatTensor(dones).to(self.device)
        
        # è®¡ç®—ä¼˜åŠ¿
        next_values = self.critic(states[-1:]).squeeze()
        advantages = self.compute_gae(rewards, values, next_values, dones)
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = advantages + values
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # æ›´æ–°ç½‘ç»œ
        for _ in range(epochs):
            # ActoræŸå¤±
            current_mask = masks if masks is not None else None
            new_log_probs, entropy = self.actor.evaluate_action(states, actions, current_mask)
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy.mean()
            
            # CriticæŸå¤±
            new_values = self.critic(states).squeeze()
            critic_loss = F.mse_loss(new_values, returns)
            
            # æ›´æ–°
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
            self.actor_optimizer.step()
            
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
            self.critic_optimizer.step()
        
        return actor_loss.item(), critic_loss.item()
    
    def save(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
        }, filepath)
    
    def load(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

def flatten_obs(obs):
    """å±•å¹³è§‚æµ‹ - æ”¯æŒç¦»æ•£åŠ¨ä½œè§‚æµ‹"""
    if isinstance(obs, dict):
        flattened = []
        for key in sorted(obs.keys()):
            if key in ['sensor_data']:
                continue  # è·³è¿‡å›¾åƒæ•°æ®
            value = obs[key]
            if isinstance(value, torch.Tensor):
                flattened.append(value.flatten())
            elif isinstance(value, np.ndarray):
                flattened.append(torch.from_numpy(value).flatten())
            elif isinstance(value, (list, tuple)):
                flattened.append(torch.tensor(value).flatten())
            else:
                flattened.append(torch.tensor([value]).flatten())
        return torch.cat(flattened)
    else:
        # å¤„ç†éå­—å…¸ç±»å‹çš„è§‚æµ‹
        if isinstance(obs, torch.Tensor):
            return obs.flatten()
        elif isinstance(obs, np.ndarray):
            return torch.from_numpy(obs).flatten()
        else:
            return torch.tensor(obs).flatten()

def extract_mask(obs):
    """ä»è§‚æµ‹ä¸­æå–åŠ¨ä½œæ©ç """
    MAX_N = 15  # ä¸env_clutter.pyä¸­çš„MAX_Nä¿æŒä¸€è‡´
    
    if isinstance(obs, dict) and 'discrete_action_obs' in obs:
        discrete_obs = obs['discrete_action_obs']
        if isinstance(discrete_obs, torch.Tensor):
            # discrete_action_obsçš„ç»“æ„ï¼š[action_mask(MAX_N), object_features(MAX_N*7), step_count(1)]
            # æˆ‘ä»¬éœ€è¦æå–å‰MAX_Nä¸ªå…ƒç´ ä½œä¸ºaction_mask
            mask = discrete_obs[:MAX_N] if discrete_obs.dim() == 1 else discrete_obs[:, :MAX_N]
            mask = mask.cpu().numpy()
            # ç¡®ä¿è¿”å›1Dæ•°ç»„
            if mask.ndim > 1:
                mask = mask.flatten()
            return mask
        elif isinstance(discrete_obs, np.ndarray):
            mask = discrete_obs[:MAX_N] if discrete_obs.ndim == 1 else discrete_obs[:, :MAX_N]
            # ç¡®ä¿è¿”å›1Dæ•°ç»„
            if mask.ndim > 1:
                mask = mask.flatten()
            return mask
    elif isinstance(obs, torch.Tensor):
        # å¦‚æœobsç›´æ¥æ˜¯å¼ é‡ï¼Œè¯´æ˜æ˜¯ä¿®æ”¹åçš„ç¯å¢ƒè¿”å›çš„å±•å¹³è§‚æµ‹
        # æ–°çš„è§‚æµ‹ç»“æ„ï¼šæŒ‰å­—æ¯é¡ºåºæ’åˆ—çš„é”®
        # discrete_action_obsæ˜¯ç¬¬ä¸€ä¸ªé”®ï¼ŒåŒ…å«121ä¸ªå…ƒç´ 
        # å…¶ä¸­å‰15ä¸ªæ˜¯æ©ç 
        
        if obs.dim() == 1:
            # 1Då¼ é‡ï¼Œç›´æ¥æå–å‰15ä¸ªå…ƒç´ ä½œä¸ºæ©ç 
            mask = obs[:MAX_N]
        else:
            # 2Då¼ é‡ï¼Œå–ç¬¬ä¸€ä¸ªbatchçš„å‰15ä¸ªå…ƒç´ 
            mask = obs[0, :MAX_N]
        
        mask = mask.cpu().numpy()
        # ç¡®ä¿è¿”å›1Dæ•°ç»„
        if mask.ndim > 1:
            mask = mask.flatten()
        return mask
    return None

def train_ppo(args):
    """è®­ç»ƒPPOæ™ºèƒ½ä½“"""
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(
        "EnvClutter-v1",
        num_envs=args.num_envs,
        obs_mode="rgb" if args.record_video else "state",  # å½•åˆ¶è§†é¢‘æ—¶ä½¿ç”¨rgbæ¨¡å¼
        control_mode="pd_ee_delta_pose",
        reward_mode="dense",
        render_mode="rgb_array" if args.record_video else ("human" if args.render else None),
        use_discrete_action=True,  # å¯ç”¨ç¦»æ•£åŠ¨ä½œ
        # å½•åˆ¶è§†é¢‘æ—¶å¢åŠ ä¼ æ„Ÿå™¨é…ç½®
        **(dict(sensor_configs=dict(width=args.video_width, height=args.video_height)) if args.record_video else {})
    )
    
    # æ–°å¢ï¼šè§†é¢‘å½•åˆ¶åŒ…è£…å™¨
    if args.record_video:
        video_output_dir = os.path.join(args.log_dir, "training_videos")
        os.makedirs(video_output_dir, exist_ok=True)
        print(f"è®­ç»ƒè§†é¢‘å°†ä¿å­˜åˆ°: {video_output_dir}")
        
        # è®¾ç½®è§†é¢‘å½•åˆ¶è§¦å‘å™¨ï¼šæ¯éš”æŒ‡å®šé—´éš”å½•åˆ¶ä¸€æ¬¡
        def video_trigger(episode_count):
            return episode_count % args.video_record_interval == 0
        
        env = RecordEpisode(
            env,
            output_dir=video_output_dir,
            save_trajectory=args.save_trajectory,
            save_video=True,
            trajectory_name="training_trajectory",
            max_steps_per_video=args.max_video_steps,
            video_fps=args.video_fps,
            render_substeps=True,  # å¯ç”¨å­æ­¥æ¸²æŸ“ä»¥è·å¾—æ›´æµç•…çš„è§†é¢‘
            info_on_video=True,  # åœ¨è§†é¢‘ä¸Šæ˜¾ç¤ºä¿¡æ¯
            save_video_trigger=video_trigger,  # ä½¿ç”¨è§¦å‘å™¨æ§åˆ¶å½•åˆ¶æ—¶æœº
            avoid_overwriting_video=True,  # é¿å…è¦†ç›–å·²æœ‰è§†é¢‘
        )
        print("âœ“ è§†é¢‘å½•åˆ¶åŒ…è£…å™¨æ·»åŠ æˆåŠŸ")
    
    # æ–°å¢ï¼šå‘é‡åŒ–åŒ…è£…å™¨ï¼ˆå¦‚æœå¯ç”¨äº†è§†é¢‘å½•åˆ¶ï¼‰
    if args.record_video:
        env = ManiSkillVectorEnv(env, args.num_envs, ignore_terminations=False, record_metrics=True)
        print("âœ“ å‘é‡åŒ–åŒ…è£…å™¨æ·»åŠ æˆåŠŸ")
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
    obs, _ = env.reset()
    flattened_obs = flatten_obs(obs)
    state_dim = flattened_obs.shape[0]
    
    # è·å–åŠ¨ä½œç»´åº¦
    if hasattr(env, 'discrete_action_space') and env.discrete_action_space is not None:
        action_dim = env.discrete_action_space.n
        print(f"ä½¿ç”¨ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ŒåŠ¨ä½œç»´åº¦: {action_dim}")
    elif hasattr(env.unwrapped, 'discrete_action_space') and env.unwrapped.discrete_action_space is not None:
        action_dim = env.unwrapped.discrete_action_space.n
        print(f"ä½¿ç”¨ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ŒåŠ¨ä½œç»´åº¦: {action_dim}")
    else:
        action_dim = env.action_space.shape[0]
        print(f"ä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´ï¼ŒåŠ¨ä½œç»´åº¦: {action_dim}")
    
    print(f"çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"æ¸²æŸ“æ¨¡å¼: {'è§†é¢‘å½•åˆ¶' if args.record_video else ('äººç±»è§‚å¯Ÿ' if args.render else 'å…³é—­')}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = PPOAgent(state_dim, action_dim)
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    writer = SummaryWriter(log_dir=args.log_dir)
    csv_logger = CsvLogger(os.path.join(args.log_dir, "training_log.csv"))
    
    # è®­ç»ƒå¾ªç¯
    episode_rewards = deque(maxlen=100)
    episode_success_rates = deque(maxlen=100)
    
    total_steps = 0
    episode_count = 0
    
    for epoch in range(args.epochs):
        # æ”¶é›†æ•°æ®
        states, actions, log_probs, rewards, values, dones, masks = [], [], [], [], [], [], []
        
        obs, _ = env.reset()
        episode_reward = 0
        episode_success = 0
        total_displacement = 0
        
        print(f"\n--- Epoch {epoch + 1}/{args.epochs} ---")
        
        for step in range(args.steps_per_epoch):
            # å±•å¹³è§‚æµ‹
            flattened_obs = flatten_obs(obs)
            state = flattened_obs.cpu().numpy() if isinstance(flattened_obs, torch.Tensor) else flattened_obs
            
            # æå–æ©ç 
            mask = extract_mask(obs)
            
            # è·å–åŠ¨ä½œ
            action, log_prob = agent.get_action(state, mask)
            value = agent.get_value(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # æ¸²æŸ“ç¯å¢ƒ
            if args.render and not args.record_video:  # é¿å…é‡å¤æ¸²æŸ“
                env.render()
                time.sleep(0.01)
            
            # å¤„ç†å¥–åŠ±å’Œä¿¡æ¯
            if isinstance(reward, torch.Tensor):
                reward = reward.item() if reward.numel() == 1 else reward.mean().item()
            elif isinstance(reward, np.ndarray):
                reward = reward.item() if reward.size == 1 else reward.mean()
            
            # å¤„ç†doneæ ‡å¿—
            if isinstance(done, torch.Tensor):
                done = done.item() if done.numel() == 1 else done.any().item()
            elif isinstance(done, np.ndarray):
                done = done.item() if done.size == 1 else done.any()
            
            # å¤„ç†æˆåŠŸä¿¡æ¯
            success = False
            displacement = 0.0
            if isinstance(info, dict):
                success = info.get('success', False)
                displacement = info.get('displacement', 0.0)
                if isinstance(success, torch.Tensor):
                    success = success.item() if success.numel() == 1 else success.any().item()
                elif isinstance(success, np.ndarray):
                    success = success.item() if success.size == 1 else success.any()
            
            # å­˜å‚¨æ•°æ®
            states.append(state)
            actions.append(action)
            log_probs.append(log_prob)
            rewards.append(reward)
            values.append(value)
            dones.append(done)
            if mask is not None:
                masks.append(mask)
            
            episode_reward += reward
            total_displacement += displacement
            if success:
                episode_success = 1
            
            obs = next_obs
            total_steps += 1
            
            # æ‰“å°æ­¥éª¤ä¿¡æ¯ï¼ˆå½•åˆ¶è§†é¢‘æ—¶æ›´è¯¦ç»†ï¼‰
            if (args.render or args.record_video) and step % 10 == 0:
                print(f"æ­¥éª¤ {step}: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.3f}, æˆåŠŸ={success}, å®Œæˆ={done}")
            
            # å¦‚æœepisodeç»“æŸ
            if done:
                episode_rewards.append(episode_reward)
                episode_success_rates.append(episode_success)
                episode_count += 1
                
                print(f"Episode {episode_count} ç»“æŸ: å¥–åŠ±={episode_reward:.3f}, æˆåŠŸ={episode_success}")
                
                # è§†é¢‘å½•åˆ¶å®Œæˆæç¤º
                if args.record_video and (episode_count - 1) % args.video_record_interval == 0:
                    print(f"ğŸ“¹ Episode {episode_count} çš„è®­ç»ƒè§†é¢‘å·²å½•åˆ¶å®Œæˆ")
                
                # é‡ç½®ç¯å¢ƒ
                obs, _ = env.reset()
                episode_reward = 0
                episode_success = 0
                total_displacement = 0
        
        # æ›´æ–°æ™ºèƒ½ä½“
        if len(states) > 0:
            print(f"æ›´æ–°æ™ºèƒ½ä½“ï¼Œæ•°æ®é‡: {len(states)}")
            mask_data = masks if len(masks) > 0 else None
            actor_loss, critic_loss = agent.update(
                states, actions, log_probs, rewards, values, dones, mask_data
            )
            
            # è®°å½•æ—¥å¿—
            avg_reward = np.mean(episode_rewards) if episode_rewards else 0
            avg_success_rate = np.mean(episode_success_rates) if episode_success_rates else 0
            
            writer.add_scalar('Training/Episode_Reward', avg_reward, epoch)
            writer.add_scalar('Training/Success_Rate', avg_success_rate, epoch)
            writer.add_scalar('Training/Actor_Loss', actor_loss, epoch)
            writer.add_scalar('Training/Critic_Loss', critic_loss, epoch)
            
            # æ–°å¢ï¼šè§†é¢‘å½•åˆ¶ç›¸å…³æ—¥å¿—
            if args.record_video:
                writer.add_scalar('Training/Episodes_Recorded', episode_count // args.video_record_interval, epoch)
            
            # CSVæ—¥å¿—
            csv_logger.log({
                'epoch': epoch,
                'episode': episode_count,
                'avg_reward': avg_reward,
                'success_rate': avg_success_rate,
                'total_displacement': total_displacement,
                'steps': total_steps,
                'actor_loss': actor_loss,
                'critic_loss': critic_loss,
                'video_recorded': args.record_video and (episode_count - 1) % args.video_record_interval == 0
            })
            
            if epoch % args.log_interval == 0:
                log_msg = (f"Epoch {epoch}, "
                          f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}, "
                          f"æˆåŠŸç‡: {avg_success_rate:.2f}, "
                          f"ActoræŸå¤±: {actor_loss:.4f}, "
                          f"CriticæŸå¤±: {critic_loss:.4f}")
                
                if args.record_video:
                    recorded_episodes = episode_count // args.video_record_interval
                    log_msg += f", å·²å½•åˆ¶è§†é¢‘: {recorded_episodes} ä¸ªepisode"
                
                print(log_msg)
        
        # ä¿å­˜æ¨¡å‹
        if epoch % args.save_interval == 0:
            save_path = os.path.join(args.model_dir, f"ppo_model_epoch_{epoch}.pth")
            agent.save(save_path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_save_path = os.path.join(args.model_dir, "ppo_model_final.pth")
    agent.save(final_save_path)
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_save_path}")
    
    env.close()
    writer.close()

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒEnvClutterç¯å¢ƒçš„PPOæ™ºèƒ½ä½“')
    parser.add_argument('--epochs', type=int, default=1000, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--steps_per_epoch', type=int, default=2048, help='æ¯è½®æ­¥æ•°')
    parser.add_argument('--num_envs', type=int, default=1, help='å¹¶è¡Œç¯å¢ƒæ•°é‡')
    parser.add_argument('--log_dir', type=str, default='./logs', help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--model_dir', type=str, default='./models/env_clutter', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_interval', type=int, default=10, help='æ—¥å¿—è®°å½•é—´éš”')
    parser.add_argument('--save_interval', type=int, default=100, help='æ¨¡å‹ä¿å­˜é—´éš”')
    parser.add_argument('--render', action='store_true', help='æ˜¯å¦æ¸²æŸ“')
    
    # æ–°å¢ï¼šè§†é¢‘å½•åˆ¶ç›¸å…³å‚æ•°
    parser.add_argument('--record_video', action='store_true', help='æ˜¯å¦å½•åˆ¶è®­ç»ƒè§†é¢‘')
    parser.add_argument('--save_trajectory', action='store_true', help='æ˜¯å¦ä¿å­˜è½¨è¿¹æ•°æ®')
    parser.add_argument('--video_record_interval', type=int, default=50, help='è§†é¢‘å½•åˆ¶é—´éš”ï¼ˆæ¯å¤šå°‘ä¸ªepisodeå½•åˆ¶ä¸€æ¬¡ï¼‰')
    parser.add_argument('--max_video_steps', type=int, default=1000, help='æ¯ä¸ªè§†é¢‘çš„æœ€å¤§æ­¥æ•°')
    parser.add_argument('--video_fps', type=int, default=30, help='è§†é¢‘å¸§ç‡')
    parser.add_argument('--video_width', type=int, default=256, help='è§†é¢‘å®½åº¦')
    parser.add_argument('--video_height', type=int, default=256, help='è§†é¢‘é«˜åº¦')
    parser.add_argument('--settle_steps', type=int, default=30, help='ç‰©ä½“ç¨³å®šç­‰å¾…æ­¥æ•°ï¼ˆå½•åˆ¶è§†é¢‘æ—¶ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç›®å½•
    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    train_ppo(args)

if __name__ == "__main__":
    main() 