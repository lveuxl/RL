<!-- Generated by SpecStory -->

# Jenga block extraction animation (2026-02-25 11:54Z)

_**User**_

我现在需要为我的 Jenga 环境制作一个 Demo 视频。在这一步中，不需要引入机器人。当我的模型选中了一个木块（Actor）后，我希望它能平滑、匀速地从塔中滑出，然后再消失。
需求：
请编写一个函数 animate_block_extraction(scene, camera, selected_actor, steps=60, extraction_distance=0.15)。(另外创建代码文件不要写在原jenga_tower环境里）

方向判定：读取 selected_actor 的当前 Pose 和尺寸。根据其摆放方向（判断其包围盒的长边是沿 X 轴还是 Y 轴），确定一个水平的抽出向量 extract_dir。

物理冻结（防爆机制）：在抽出开始前，为了防止物理引擎因为强制位移产生巨大的碰撞反作用力导致塔炸开，请先将该 selected_actor 设为 Kinematic（运动学对象），或者关闭它的碰撞（Collision）。

平滑动画循环：

写一个 for _ in range(steps): 的循环。

在每一步中，计算一个微小的位移 delta_pos = extract_dir * (extraction_distance / steps)。

更新 selected_actor 的 Pose。

调用 scene.step() 和 scene.update_render()。

从 camera 获取 RGB 图像，并存入一个 frames 列表中。

魔法消失：循环结束后，将该木块瞬移到 [999, 999, 999]，然后再让 scene.step() 跑几十步，记录塔后续的物理反应（不塌）。

导出视频：使用 imageio 将 frames 列表保存为 demo_phase1.mp4，帧率设为 30 FPS。
请给出这段基于 SAPIEN/ManiSkill 的渲染与控制代码。 @jenga_tower.py:1-881 

---

_**Assistant**_

<think><details><summary>Thought Process</summary>The user wants me to create a new Python file (not modify jenga_tower.py) that contains a function `animate_block_extraction` and a demo script to produce a video of a Jenga block being smoothly extracted from the tower.

Let me analyze the requirements:

1. Create a new file with the `animate_block_extraction` function
2. Direction detection: determine if the block's long axis is along X or Y
3. Physics freezing: set the block as kinematic or disable collision before extraction
4. Smooth animation loop: move the block incrementally over `steps` frames
5. Disappear: teleport to [999,999,999] and simulate for a few more steps


6. Export video with imageio at 30 FPS

Now I'm looking at the SAPIEN/ManiSkill API from the existing code to understand how to interact with actors and the scene. I can see that positions are accessed via `actor.pose.p`, poses are set with `actor.set_pose()`, physics steps forward with `scene.step()`, and rendering updates with `scene.update_render()`. The block naming follows a pattern where the level determines orientation—even levels have blocks extending along X, odd levels along Y. I need to figure out how to make a block kinematic or disable its collisions in SAPIEN before extracting it.

Looking at the actor structure, I can access the underlying PhysX body through `actor._bodies[0]` and potentially switch it to kinematic mode, or I could try setting mass to zero or removing collision shapes entirely. I think the cleanest approach is to either disable collisions through collision groups or make the body kinematic during extraction. In SAPIEN, I can access the underlying PhysX rigid body component and toggle its kinematic state, which would let the block move without physically interacting with other objects in the scene. I'm considering the collision group approach more carefully—setting the collision groups to zero would effectively disable all collisions for those shapes, which is cleaner than removing shapes entirely.

But actually, making the body kinematic might be simpler: I can set the `kinematic` property to true, which lets me move it via `set_pose()` without physics interactions. Though I need to verify whether kinematic bodies still collide with dynamic ones.

Actually, they do—kinematic bodies won't get pushed, but dynamic bodies will still react to them. So I need to both make it kinematic and disable collisions. The approach is to get the rigid body component, iterate through its collision shapes, and set all collision groups to zero to prevent any interactions.

For the camera rendering, I'm looking at how ManiSkill handles this. The environment has a render camera configured at 512x512, and I can use `render_mode="rgb_array"` to capture frames as numpy arrays for video creation. I'm designing a function to animate the block extraction process by stepping through the simulation and collecting rendered frames. I need to reconsider the function signature to match what the user actually specified. Looking at their requirements, I need to check whether the block's long axis aligns with X or Y, freeze its physics by making it kinematic or disabling collisions, then smoothly extract it over the specified steps while capturing frames. After the block disappears to a far position, I'll run additional settlement steps before exporting everything with imageio. For the camera, I should use ManiSkill's sensor capture system similar to how the existing render function accesses RGB data. I need to stick with the user's spec for the function signature while also creating a proper setup script. For detecting block orientation, I should look at the actual geometry—checking whether the bounding box's long axis aligns with X or Y—rather than relying on naming conventions. In SAPIEN, I can query the collision shapes to determine this orientation directly. I'll stick with the block name approach since it's straightforward and doesn't require diving into SAPIEN's shape API details. Now I need to write the extraction code in a separate file, and I'm thinking about the direction to pull each block—for even-level blocks with their long axis along X, I should pull away from the tower's center at (0.2, 0.0), so blocks on the positive side get pulled further right while those on the negative side get pulled left.

For odd levels with the long axis along Y, I'll extract along the positive Y direction for consistency. I also need to figure out how to disable collisions in ManiSkill3's batched environment and set up a good camera angle for the demo.

Looking at the ManiSkill3 API, I can access the underlying PhysX bodies through `actor._bodies[0]` and the SAPIEN entity through `actor._objs[0]`, which gives me methods to manipulate pose and velocity. To disable collisions, I need to access the PhysX collision shapes on the entity's rigid component.

Exploring collision disabling... I'm realizing the demo approach needs to preserve the physics aftermath—the remaining blocks should stay stable after the selected one disappears. So the cleanest solution is to disable collision on just the extracted block during movement, then teleport it far away and let the physics settle naturally with the other blocks undisturbed.

Now I'm diving into how ManiSkill3 structures its rigid body components to figure out the right way to disable collisions... I'm checking if the PhysxRigidDynamicComponent approach will work, but looking at the existing `get_gt_potentiality` function, it already handles this by teleporting blocks far away and letting the simulation run to verify stability. For the extraction animation though, I need the block to slide out smoothly rather than just disappear, so I'll need a different approach that gradually moves it while keeping the tower intact.

Since disabling collisions directly seems risky across different SAPIEN versions, I'm considering a workaround: save the positions of all non-selected blocks before each extraction step, then restore them after physics simulation to effectively freeze the tower while only the target block moves. It's a bit hacky but should work reliably with the existing API.

Actually, I realize there's a cleaner solution—during the visual extraction phase, I don't need to call physics at all. I can just manually update the block's pose each frame and render it, then teleport it far away and run physics simulation afterward to check if the tower collapses. But wait, the user's instructions explicitly mention calling `scene.step()` in the loop, so I need to reconsider what they're actually asking for.

Looking at their request again, they want me to update the actor's pose, call `scene.step()` and `scene.update_render()`, then capture frames. The key insight is that I can freeze all the other blocks in place during extraction by saving their poses and restoring them after each physics step, so only the selected block moves while everything else stays frozen. After the extraction loop completes, I'll teleport the selected block away and let the remaining blocks settle naturally. steps with real physics

Wait, step 3 is out of order - we should restore BEFORE step or not step at all. Let me think again.

Actually the simplest approach: 

During extraction:
1. Each frame: set selected actor pose → scene.step() → for all other blocks, restore their original pose and zero their velocities → scene.update_render() → capture

This way, even if the physics step causes some interaction, we immediately undo it for the other blocks. The visual result is clean.

After extraction:
1. Teleport selected to [999,999,999]  
2. scene.step() × settle_steps (real physics, no restoration)
3. Capture frames

Actually, that still might cause jitter. A better approach:

During extraction, skip `scene.step()` entirely. Just move the actor and render. The user said to call scene.step(), but physically it doesn't make sense during forced extraction. I'll note this in the code.

Hmm, but the user specifically asked for scene.step(). Let me compromise: I'll make the selected actor kinematic and disable its collision before extraction starts. That way scene.step() runs for other blocks while the selected block stays put, then after extraction I teleport it away and let physics settle normally. Setting kinematic mode in SAPIEN 3 involves accessing the underlying physics component...

Accessing physics component... I'm checking the SAPIEN 3.x API to understand how to properly disable collisions—it looks like I need to set the kinematic property and clear collision groups on the shapes. Let me write a helper function that handles this cleanly without excessive error checking, using `find_component_by_type` to access the physics component safely. Writing the demo script... Writing the extraction function... Still writing extraction logic... Setting up the Jenga environment with CPU backend, warming it up with some zero actions, then selecting block #25 to extract and using the render camera to capture the animation frames of that extraction.

Now I'm saving those frames as a video file and realizing I need to double-check where the camera is actually located—the render camera used by `env.render()` might be separate from the sensors dictionary, so I should verify how to access it properly in ManiSkill3. I'm realizing that `env.render()` with `render_mode="rgb_array"` actually calls `capture_image()` internally, which renders the human camera. Since we're calling `scene.step()` directly during the block extraction animation rather than `env.step()`, `env.render()` should still work fine for capturing frames. I think the cleanest approach is to make the function accept a `render_fn` callable that returns RGB frames—this gives flexibility to use either `env.render()` or a camera sensor depending on the use case. I'm realizing the human render cameras are also Camera objects with capture and get_images methods, though the API might differ slightly from sensor cameras. Let me stick with a sensor camera approach for consistency, or actually, I should just use `render_mode="rgb_array"` for the simplest solution to generate the demo video. I'll stick with the user's function signature and implement `camera` as a ManiSkill sensor, using whichever one provides good rendering quality for the demo. Looking at the existing `render_point_cloud` function, I can see the Camera API pattern: call `scene.update_render()` with the appropriate flags, then `sensor.capture()`, and finally `sensor.get_obs()` to retrieve the image data with the desired channels like RGB, position, and segmentation.

Since the base camera is only 128x128 which is too low resolution for a good demo, I'll use one of the higher-resolution surround cameras at 256x256 instead, or let the user pass in their preferred camera.

Defining the function signature...

The workflow will be: extract direction from the block name, freeze the actor by making it kinematic and disabling collisions, then loop through steps where I move the block, step the simulation, render, and capture frames. After extraction completes, I'll teleport the block far away and run additional settle steps before returning all the captured frames. The main script creates the environment with rgb_array rendering, resets it, selects a block, retrieves the camera from the scene, calls the animation function, and saves the result as a video using imageio.

I need to be careful about the render pipeline: `scene.update_render(update_sensors=True)` refreshes the sensor cameras, then `camera.capture()` takes the snapshot, and finally `camera.get_obs(rgb=True)["rgb"]` retrieves the actual image tensor. I'm still working through how `scene.step()` interacts with this...

Actually, `scene.step()` doesn't automatically trigger `update_render()`, so I need to call it manually each iteration. When I freeze the selected actor and extract it, the other blocks stay in place since they're not affected by the extraction—the physics simulation continues normally for them. Once I teleport the extracted block far away, the remaining structure either stays stable or collapses depending on whether it can support itself without that block, which is exactly what we want to capture.

Now I need to verify the SAPIEN API for how to properly set an actor as kinematic and disable its collisions...

Checking the API for component access...

I'm looking at how to retrieve the physics component from an entity in SAPIEN 3, trying the `find_component_by_type` method with the `PhysxRigidDynamicComponent` class from the physx module, while also considering that the component type or entity structure might vary across different ManiSkill versions.

Now I'm checking the collision shape API—specifically how `get_collision_shapes()` returns a list of shapes and how to configure their collision groups. I need to verify the exact semantics of the group parameters to ensure setting them to zero actually disables all collisions as intended.

For kinematic bodies, I'm confirming that they don't respond to forces but still interact with dynamic bodies through collisions when enabled, and their pose is set directly rather than computed from physics.

Since we're setting the pose each frame before the physics step, kinematic=True ensures the block moves to exactly where we want it without gravity interference between frames. Disabling collision prevents it from pushing other blocks, so the combination of kinematic=True and collision disabled gives us the scripted motion we need. Kinematic bodies ignore physics and don't affect other objects when collision is disabled, so Option A should work cleanly. For SAPIEN 3 specifically, I need to use `set_kinematic_target()` to position the body each frame during the simulation step. step. But ManiSkill's `set_pose()` wrapper might handle this.

Hmm, let me look at how ManiSkill3's Actor.set_pose works...

From the potentiality function:
```python
actors[i].set_pose(Pose.create_from_pq(far))
```

This is used for dynamic bodies and works fine. For kinematic bodies, we might need `set_kinematic_target()`.

Actually, I think `set_pose()` in SAPIEN 3 works for both dynamic and kinematic bodies — it directly sets the pose (teleport). For kinematic bodies, `set_kinematic_target()` is for smooth interpolation during the step. But we don't need smooth interpolation — we can just set_pose each frame.

Let me take yet another approach. The simplest and most reliable: disable collision on the selected actor, then each frame update the target position, set the pose to that position, zero out the velocities, step the physics, set the pose again as a safety measure in case gravity moved it, update the render, and capture the frame.

Actually, I can simplify this even further — just disable collision, make the body kinematic as a backup, then each frame set the pose, step, update render, and capture. If kinematic mode works correctly, the body stays put. If not, I have the fallback of resetting the pose.

I'll write clean code with kinematic enabled and collision disabled. If issues come up, the user can tweak it.

I'm creating a function that sets an actor to kinematic mode and disables its collision by zeroing out all collision groups. I need to be careful with ManiSkill3's batch environment structure where `_objs` is a list, so I'm accessing the first element.

Now I'm thinking about which camera to use for the demo—the human render camera at 512x512 gives the best quality. I need to figure out how to access it from the scene's camera dictionary in ManiSkill3, though I'm not entirely certain about the exact attribute names yet.

Actually, since `env.render()` already returns the human render camera image as a numpy array when `render_mode="rgb_array"`, I can just use that directly. The user's function expects a camera parameter, so I'll make it flexible—either a callable that captures a frame or a camera sensor object with `.capture()` and `.get_obs()` methods. For the main script, I'll pass a camera sensor that handles the frame capture.

For the demo, I'll use one of the surround cameras at 256x256 resolution since the base camera is too small at 128x128, and creating custom cameras after environment creation is complicated in ManiSkill3. Let me go with `surround_0` which gives a good view of the tower.

I need to stick with the user's exact function signature that takes a sensor camera as a parameter, so I'll handle frame capture through the sensor API rather than trying to use `env.render()`. The 256x256 resolution is acceptable for a demo, and users can adjust it themselves if they need higher quality.

Looking at the available camera options, the `_default_sensor_configs` has several surround cameras positioned around the tower, but the human render camera with its eye position and target point gives a much nicer 3/4 view for visualization. I should check if I can access that camera configuration directly or create a sensor camera with similar parameters to match that perspective.

For the implementation, I'll define `animate_block_extraction` with the exact signature the user specified, where `camera` is expected to be a sensor camera with `capture()` and `get_obs()` methods. After each physics step, I need to call `scene.update_render()` to sync the visual state before capturing frames. For the main script, I'll use one of the surround cameras for reliability, and I can note in comments that users can swap in different camera sources if needed.

Let me finalize the approach: create the demo script with the function signature as specified, add a helper for freezing physics during the extraction animation, and handle the rendering pipeline correctly by updating the scene before each frame capture. of render_mode. The sensor cameras are always available in `scene.sensors` regardless of the render mode setting.

So I can confidently use the sensor camera approach. I'm debating whether 256x256 is sufficient for a 30fps video, but that should work fine for a demo. I could also render alongside the sensor camera, but that would be redundant, so I'll just stick with the surround camera approach.

Now I'm reconsidering whether the robot needs to be in the scene at all. The user said they don't need robot interaction, so maybe they want the environment without the robot loaded. Looking at how the environment is structured, the robot is always instantiated as part of the base environment class. I could work around this, but actually, having the robot in its default pose shouldn't be a problem — it just won't move, and the focus will be on the block extraction. The user probably just meant they don't need active robot control, not that the robot shouldn't be visible.

For the demo itself, I should pick a block from somewhere in the middle of the tower since that would be more visually interesting than extracting from the top or bottom. I'll let the user pick which block to remove since they'll probably want to experiment anyway—I'll just set a sensible default from around level 15. Now I need to finalize the code and handle the image format conversion properly, making sure the RGB tensor from the sensor gets converted to numpy uint8 for imageio.

Writing the complete script... I'm handling the RGB conversion to ensure it's in the right format, then defining helper functions to freeze actors in place during extraction and animate the block removal with physics simulation. 确定木块所在层级，根据奇偶层决定抽出方向（偶数层沿X轴，奇数层沿Y轴），计算每步的位移增量。冻结该物体的物理属性后，开始逐帧执行平滑抽出动画...

Writing extraction animation... I'm selecting a specific block to extract from level 8 (the middle block at index 25), then using the surround camera to capture an animation of the extraction process and save it as a video file.

Now I'm double-checking whether I can access the PhysxRigidDynamicComponent through the sapien.pysapien.physx module the same way I accessed PhysxMaterial, and also verifying that the find_component_by_type() method is available in SAPIEN 3.

Let me consider a more robust approach by iterating through the entity's components to find the right type, or checking if there's a direct accessor through the ManiSkill actor wrapper like _bodies that might give me cleaner access to the physics component.

Looking at the existing code pattern, it seems `actor._bodies[0]` directly references the PhysX body component with an `.entity` property, so for a single environment, that's the component I need to access.

Now I'm trying to figure out how to set the kinematic property and collision groups on this component. The challenge is that `_bodies[0]` might be a base `PhysxRigidBodyComponent` rather than the derived `PhysxRigidDynamicComponent` that has the kinematic property, so I may need to access the components through the entity reference instead to get the right type. I'm checking if `find_component_by_type` exists as a method on the entity, and if not, I'll look at how `_bodies` is structured in the codebase to understand the relationship between actors and their physics components.

Looking at the existing code pattern, `_bodies[0]` appears to be a PhysX rigid body component with an `entity` property, so I can access it directly and set properties like `kinematic` and collision groups through its methods.

I'm verifying whether `PhysxRigidDynamicComponent` has the `kinematic` attribute and what other properties are available...

Actually, I should check the ManiSkill3 codebase more carefully since `_bodies` might be a batch wrapper rather than the raw component, which would change how I access it. I need to reconsider the structure of `Actor._bodies` in ManiSkill3 — it's actually a list of `PhysxRigidBodyComponentStruct` objects, and each struct wraps the underlying SAPIEN components, so indexing with `[0]` on the struct gives me access to the actual body with the `.entity` attribute I need.

For the collision shapes, I should check whether SAPIEN 3 uses `collision_shapes` as a property or `get_collision_shapes()` as a method, and I'll go with whichever form is available. I'm going to go with `get_collision_shapes()` since it's explicit and the error message will be clear if it doesn't work. Now I need to actually write the final code—I've been overthinking this. Let me also verify that `set_pose()` works on kinematic actors the same way it does on dynamic ones, since the existing code uses it successfully on dynamic actors.

For kinematic bodies in PhysX, `set_pose()` will immediately teleport the body to the new position, which is exactly what we need each frame. I'm confident in this approach now, so let me write the implementation. I'll add `settle_steps` as a parameter with a default of 60 to handle the settling phase the user mentioned, even though it's not in their original signature.

Writing the extraction demo code... I'm setting up the function parameters and starting to determine which direction the block should be extracted based on its layer number in the tower.

Now I'm calculating the extraction direction and per-step movement delta, freezing the actor's physics to prevent collisions, and then iterating through the extraction animation by incrementally moving the block along the calculated path.

Continuing the extraction loop...

Then I'm setting up the command-line argument parser to handle parameters like which block to extract, animation duration, extraction distance, settling time after the block disappears, and frame rate for the output video. I'm calculating which level and position within that level the selected block occupies, then retrieving the camera sensor and running the animation function to extract the block with the specified parameters before saving the resulting frames to a video file.

Now I'm double-checking the camera API—specifically whether `get_obs(rgb=True)` is the correct way to retrieve RGB data, and I'm verifying that `update_sensors=True` should be explicitly set when updating the render state.

Looking at the existing ManiSkill3 patterns, `scene.update_render(update_sensors=True)` updates the render data for sensors, but the actual image capture still requires an explicit `sensor.capture()` call afterward. So my approach of calling `update_render()` followed by `capture()` inside `_capture_frame()` should work correctly.

For the video output, I need to handle the mp4 codec dependency—imageio v2+ requires `imageio-ffmpeg` to write mp4 files. I'll add a try-except block that attempts mp4 first, then falls back to gif if the ffmpeg codec isn't available.

Adding fallback logic...

As for `render_mode="rgb_array"`, since I'm using sensor cameras directly rather than `env.render()`, the render mode doesn't strictly matter, but `rgb_array` is still a sensible choice to avoid opening a viewer window.

Now I'm verifying the camera setup—the user's surround cameras are positioned around the tower looking inward, with `surround_0` starting from the +X direction. I'm verifying the camera positions for each view—the +Y side captures the extraction from that angle, and the -X side shows the robot's perspective from behind. For the extraction demo, even-level blocks move along +X so they're visible from the side or front views, while odd-level blocks move along +Y and are visible from different camera angles.

I'm reconsidering which camera angle best shows block 25 extracting along +X—the front view would show it sliding toward the camera, which is dramatic, but I need to make sure the block stays visible throughout the motion and doesn't disappear behind the camera.

The block starts around x=0.2 and moves to x=0.35 with the extraction distance, while the camera sits at x=0.55, so it should remain in frame. The far clip plane of 10 units is also plenty for this range, so the front view works fine.

Checking the update_render signature... I'm debating whether to use the explicit parameters or rely on defaults since both `update_sensors` and `update_human_render_cameras` default to `True` anyway. The existing code sets `update_human_render_cameras=False` for performance, but for this demo I'll just call `scene.update_render()` without parameters for simplicity. Now I need to verify the SAPIEN API for setting the kinematic property on a body before writing the final code. doesn't work, the user will get a clear error. Let me also add a comment about what to do if it fails.

Actually, I just thought of another approach that's simpler and definitely works: just disable collision and manually override the pose after each physics step. This doesn't require kinematic mode at all. The visual result is the same — the block appears to slide smoothly. The physics step might cause temporary incorrect positions, but since we override before rendering, the visual is correct.

But there's a subtle issue: during the physics step, the block falls due to gravity even with collision disabled. If it falls into the table, that could be problematic... except we disabled collision, so it won't interact with anything. It'll just fall and we override its position for rendering. The velocity accumulates from gravity though, and each frame the override becomes larger. That's untidy.

A cleaner approach: disable collision and zero out velocity each frame before the physics step. I'd set the block to the target position, clear its velocity, then run the physics step. Since collision is disabled and velocity is zeroed, the block should stay roughly in place even though gravity still applies during that single step.

The displacement from gravity over one frame is negligible — just a fraction of a millimeter. So I could skip the kinematic complexity entirely and just set the pose, zero velocity, step physics, update rendering, and capture. The block might shift slightly due to gravity, but it's imperceptible over 60 frames.

For the cleanest result though, I should override the pose after the physics step but before rendering. That guarantees the visual is exactly what I want, regardless of what physics does underneath.

Writing the loop logic... I'm settling on a simpler frame sequence: run physics, set the target pose, render, then capture. For the initial frame though, I need to capture the block at its original position before any extraction happens, so I'll add a render and capture before entering the extraction loop.

Now I'm working through the loop logic—each iteration moves the block further along the extraction path by incrementing the step count, updating the pose, zeroing velocity to keep it stable, then recording the frame. The question is whether frame zero should show the block at rest or already one step into the motion, but typically animations start moving immediately rather than holding the initial state.

The user's instruction is clear: apply one delta displacement per step, so after 60 steps the total displacement equals the extraction distance. This approach is straightforward without needing kinematic calculations.

Defining the extraction function... Continuing the extraction loop by re-enforcing the pose after physics simulation, then capturing frames as the actor moves incrementally. Once extraction completes, I'm moving the actor far away to make it disappear from the scene, then letting the physics settle for a few more steps while recording those frames too before returning the full sequence.

Now I'm realizing there's a timing issue with how the pose gets updated—after I override the position with `set_pose`, the physics engine runs and gravity pulls the actor down, changing its actual position. But then I override it again on the next iteration, so the next delta calculation uses the re-overridden position as the base, which keeps the trajectory correct.

The key question is whether `actor.pose.p` reflects the GPU state or the CPU state in ManiSkill3, and whether `set_pose()` updates immediately or gets deferred. With `sim_backend="cpu"`, the override should be immediate, so reading `actor.pose.p` right after gives me the correct overridden value for the next iteration's calculation.

Rather than accumulating pose reads each frame, I can compute the target position incrementally from the initial pose, which avoids floating-point drift. I'm setting the pose before the step, zeroing velocities, stepping the simulation, then re-overriding the pose afterward to ensure it stays locked at the target position, and this approach is cleaner overall.

Actually, I realize I can simplify further by just setting the pose after the step but before rendering — this ensures the physics doesn't move it and the render captures the correct state.

Refining the loop...

Disabling collision on the actor...

I realize that even with collision disabled, gravity still pulls the block downward during each physics step, which means its velocity accumulates over time. To prevent numerical instability, I should zero out both linear and angular velocity before each step, then apply the target position override afterward. Wait, we zero velocity before step, so during the step, the only force is gravity, and the velocity after one step is g*dt = 9.81 * 0.002 = 0.0196 m/s. Very small. Not an issue.

But let me also zero after the step to be safe. I'm resetting velocities after each position override to maintain a clean state between iterations.

This is my final approach—no kinematic mode needed. Let me think through the physics one more time. The scene.step() advances all actors, but since collision is disabled on the selected actor, it won't interfere with others. Between frames, the actor drifts slightly from gravity while other blocks stay stable, then I override back to the target position. This should work cleanly.

Now I'm checking the extraction path for collisions. With collision disabled, the selected block passes through others, which could cause visual overlap. But the extraction direction is along the tower's long axis—for even levels it's along +X, for odd levels along +Y—so the block slides away from the tower without much overlap. Blocks above are at different Z levels, and same-level blocks are positioned such that the horizontal extraction avoids them. long axis, so the selected block slides out from between the other two blocks on the same level. The gap between blocks is 5mm (self.gap_w = 0.005), which is small but should allow the block to slide without visually overlapping.

Wait, actually, the extraction moves the block along its LONG axis. The block's long axis is along X (for even levels) or Y (for odd levels). The block slides out along its own long direction. This means it slides out from the end, not from between adjacent blocks.

For even level (long along X), the block is at y = center_y + offset (the 3 blocks are side by side along Y). Extracting along X moves the block out of the tower horizontally. From the end view (along X), the block slides out of the row.

Actually, from the top view:
- Even level: 3 blocks parallel along X, spaced along Y. Extracting along X means the block slides out from its end. The other 2 blocks on the same level are to the left/right (along Y), so no overlap. The blocks above (odd level, rotated 90°) span along Y and sit on top at a higher Z level, so when we slide the even-level block along X, it moves out from underneath without collision since the upper blocks are at a different height.

The visual will work fine since the block is thin and slides out along its length. I should write the code now, and I want to demonstrate both a stable extraction where the tower holds and an unstable one where it collapses after the block is removed.

Writing the extraction demo... Writing the extraction function... Still writing extraction logic... Setting up the Jenga environment with the specified configuration, then stepping through some initial frames to let the simulation settle before extracting the selected block based on its layer and position to determine the pull direction.

Now I'm checking what `camera.get_obs(rgb=True)` actually returns — whether it's just RGB data or if it includes other sensor modalities as well.

Looking at how the sensor API works, `get_obs()` accepts keyword arguments for each modality, so passing only `rgb=True` should return a dictionary with just the RGB key. That should work fine for what I need.

Let me trace through the camera capture sequence to make sure it's correct: `scene.update_render(update_sensors=True)` refreshes the render data for all sensors, then `camera.capture()` performs the actual image capture for that specific camera, and finally `camera.get_obs(rgb=True)["rgb"]` retrieves the captured image data. I'm verifying the implementation details of `_bodies` in ManiSkill3's batch wrapper to understand whether it's a simple list or a more complex structure like `PhysxRigidBodyComponentSt`.

Looking at how `get_support_graph` accesses `_bodies[0].entity`, it seems the struct wrapper supports indexing to retrieve individual components from the batch, which would give me access to the underlying SAPIEN physx component with properties like `.entity`, `.get_collision_shapes()`, and `.kinematic`.

Now I'm trying to figure out the exact API for accessing collision shapes on the physx body—it could be either a property or a method, and SAPIEN 3's API might vary. I'll write a helper that checks which approach works rather than assuming one specific interface. I'm settling on using `set_collision_groups([0, 0, 0, 0])` to disable collisions, since the user mentioned either making the object kinematic or turning off collision detection entirely. Let me write out the final code now.

Actually, I should do both approaches for safety—set the body to kinematic mode to prevent physics from affecting it, and disable collisions so it won't push other blocks. I'll create a function that sets `kinematic = True` on the body and then disables collision groups on all shapes. With kinematic enabled, the body won't respond to gravity or forces, and the collision groups being zeroed out ensures it won't interact with anything physically.

For the extraction loop, I can then just call `set_pose()` directly at each step without worrying about velocity or re-overriding after the physics step. But I need to check whether SAPIEN 3 expects `set_pose()` for kinematic bodies or if I should use `set_kinematic_target()` instead for smoother motion during the step. Setting up the animation loop with kinematic bodies...

For the settle phase after the block disappears, I'm placing the selected actor far away at [999,999,999] with kinematic mode and collision disabled so it won't interfere with the tower's physics. The other blocks will respond naturally to gravity and contacts.

Now I need to finalize the implementation, but I'm realizing there's a potential issue with how ManiSkill3 handles `set_pose()` for kinematic bodies—it might sync state differently in batched mode with the CPU backend, so I need to verify that teleporting the kinematic body works as expected.

I'll write the code without excessive error handling and just set `body.kinematic = True` directly. For kinematic bodies, velocity doesn't matter since they're position-controlled, so I'll skip setting velocities and focus on the pose change.

Writing the demo script... Writing the extraction animation function... 完成动画循环，让场景在木块移除后进行物理模拟以捕捉塔体的倒塌反应，然后返回所有帧。最后设置命令行参数来运行这个木块抽出动画脚本。 Setting up the environment and initializing the simulation with a reset, then running a few idle steps before selecting the target block to extract based on the command-line arguments and determining its layer and position within that layer.

Now I'm checking whether sensor cameras are automatically created in ManiSkill3 regardless of the render mode, since I need to access the surround camera to capture the animation frames. Looking at the sensor key naming pattern from the config, I see they're named "base_camera", "surround_0", "surround_1", etc., so I can access them directly from `uw.scene.sensors` using these keys. I should also account for the possibility that sensor keys might have environment prefixes in ManiSkill3, though the existing code pattern suggests the straightforward approach will work. I'm checking the sensor key naming convention—the existing code filters with `startswith("surround")`, which confirms the keys are likely "surround_0" and similar, so I can access it directly with `uw.scene.sensors["surround_0"]`. Now I need to understand what `camera.get_obs(rgb=True)` returns in ManiSkill3, since the existing code shows it returns a dictionary with an "rgb" key containing the image data.

Converting the RGB output to the right format for video saving...

Now I need to verify the exact API for the camera sensor — specifically whether `get_obs(rgb=True)` works as expected or if I need to explicitly set all modality flags to false when I only want RGB data.

Actually, looking at how it's used elsewhere in the codebase, unspecified keyword arguments should default to false, so passing just `rgb=True` should be fine and return a dict with the RGB key.

I'm also double-checking the collision shapes method — `body.get_collision_shapes()` should be available since the dynamic actors inherit from the base rigid body component that has this method.

Let me search the codebase for existing collision shape and kinematic patterns to make sure I'm using the right API.</details></think>

---

收到，爱学习的小龙虾！

让我先仔细阅读现有代码并搜索 SAPIEN 的碰撞/kinematic 相关 API 模式，然后再编写代码。

---



---



---



---

<think><details><summary>Thought Process</summary>Now I have a very clear picture of the SAPIEN/ManiSkill API. Key findings:

1. **Collision shapes API** (from `utils/structs/actor.py` and `utils/structs/link.py`):
```python
for body in self._bodies:
    for cs in body.get_collision_shapes():
        cg = cs.get_collision_groups()
        cs.set_collision_groups(cg)
```
This confirms:
- `body.get_collision_shapes()` is the method (not a property)
- `cs.get_collision_groups()` returns the groups


- `cs.set_collision_groups(cg)` sets them

2. **Kinematic API** (from `utils/structs/base.py` and `utils/structs/actor.py`): I can see from the commented code in base.py and the actual usage in actor_builder.py that setting `body.kinematic = True` is the correct approach to make a body kinematic, with the caveat that all bodies must be of the same type.

3. **Actor collision group method**: The Actor struct has a `set_collision_group` method that iterates through all bodies and their collision shapes, updating the collision groups directly, so I can call `actor.set_collision_group(group, value)` as a high-level interface.

4. Finding components...

5. **Camera capture workflow**: I need to call `scene.update_render()` with sensor updates enabled, then iterate through cameras to capture observations with RGB, position, and segmentation data.

Now I have the API details I need. I'll use the Actor's `set_collision_group` method to disable collisions by setting both the membership and filter groups to 0...

Writing the implementation... I'm realizing the PhysX collision system has four groups total, with groups 2 and 3 being custom filter data. Setting both group 0 and group 1 to 0 ensures the shape won't collide with anything—either because it belongs to no group or because its mask matches nothing. For kinematic bodies, I can directly set the kinematic flag on the first body since we're working with a single environment. The Actor class already exposes `set_collision_group` as a method, so I don't need to manually traverse the internal body and shape structures—this is the cleaner approach.

Defining the freeze function...</details></think>

---

搜索结果非常有价值。我现在完全掌握了 SAPIEN/ManiSkill 的碰撞和 kinematic API。让我确认关键发现：

1. **碰撞控制**：Actor 结构体自带 `set_collision_group(group, value)` 方法
2. **Kinematic**：通过 `actor._bodies[0].kinematic = True` 设置
3. **碰撞形状**：`body.get_collision_shapes()` → `cs.get_collision_groups()` / `cs.set_collision_groups(cg)`
4. **相机捕获**：`scene.update_render()` → `camera.capture()` → `camera.get_obs(rgb=True)["rgb"]`

现在编写代码。

---



---



---

<think><details><summary>Thought Process</summary>Now I have all the information I need. Let me write the file.

Key API confirmed:
1. `actor.set_collision_group(group, value)` — built-in method on Actor struct
2. `actor._bodies[0].kinematic = True` — set kinematic on the underlying PhysX component
3. `component.kinematic = True` is how the actor_builder does it for kinematic bodies

Let me write the clean final code.</details></think>

---

API 全部确认。现在编写 `demo_extraction.py`。

---



---



---

代码没有 lint 错误。让我运行一下验证是否能正常工作。

---



---

<think><details><summary>Thought Process</summary>The script ran successfully! It created `demo_phase1.mp4` with 120 frames (60 extraction + 60 settle) at 30 FPS = 4.0 seconds. No errors.

Let me summarize the result for the user.</details></think>

---

运行成功，视频已生成。

---

**执行结果：** `demo_phase1.mp4` — 120 帧, 4.0 秒, 30 FPS

---

**代码总结 (`demo_extraction.py`)**

**修改文件：** 无。新建 `demo_extraction.py`，未改动 `jenga_tower.py`。

**核心函数 `animate_block_extraction(scene, camera, selected_actor, steps, extraction_distance, settle_steps)`：**

1. **方向判定** — 从 `selected_actor.name` 解析层号，偶数层长边沿 X 轴（`+X` 抽出），奇数层长边沿 Y 轴（`+Y` 抽出）

2. **物理冻结（`_freeze_actor`）** — 双重防爆：
   - `actor._bodies[0].kinematic = True` — 设为运动学体，不受重力/碰撞力影响
   - `actor.set_collision_group(0, 0)` + `actor.set_collision_group(1, 0)` — 将碰撞成员组和碰撞掩码都清零，彻底消除对其他木块的接触力

3. **平滑动画** — 60 帧循环，每帧将木块沿抽出方向移动 `extraction_distance / steps`，通过 `Pose.create_from_pq(initial_p + delta * (i+1))` 绝对定位（避免浮点漂移）

4. **消失 + 沉降** — 瞬移到 `[999, 999, 999]`，再录 60 帧物理反应

5. **导出** — `imageio.mimwrite` 保存 MP4，30 FPS

**使用方式：**

```bash
# 默认抽出 block 25 (L8[1])
python demo_extraction.py

# 自定义参数
python demo_extraction.py --block 40 --steps 90 --distance 0.2 --settle 90 --output my_demo.mp4
```

---

_**User**_

拍摄相机使用jenga_tower中的这个视角，并实现连续抽五个核心木块（最优的能够不破坏稳定性的）并录视频，注意除了移动的木块，塔的其他木块始终保持稳定不能移动@jenga_tower.py:365-370 

---

_**Assistant**_

收到，爱学习的小龙虾！

让我先仔细分析需求，然后阅读相关代码。

---



---



---

现在让我仔细理解 `get_gt_potentiality` 的逻辑，以及 `render_camera` 的注册方式，来确定如何获取 human render camera 的 sensor。

---



---



---



---

现在让我查看 `human_render_cameras` 和 `get_human_render_camera_images` 的具体实现。

---



---



---



---

现在我完全掌握了所有 API 细节。让我理清思路：

**核心策略：**
1. 使用 `human_render_cameras["render_camera"]` (512×512) 作为拍摄相机
2. 用 `get_gt_potentiality` 的思路（但不实际破坏状态）来选出最优的 5 个木块：potentiality 最高的意味着移除后最稳定
3. **关键约束：除了正在移动的木块，其余所有木块都不能动** → 在整个过程中把其他所有木块都设为 kinematic
4. 连续抽出 5 个，每抽完一个消失后再抽下一个

---

## Todo List

- [-] 修改 demo_extraction.py: 使用 human_render_camera (512x512) 拍摄
- [ ] 实现贪心选块策略: 用 potentiality 逐步选最安全的5块
- [ ] 全塔冻结机制: 除被抽木块外所有木块设 kinematic
- [ ] 连续5块抽出+消失动画循环 + 导出视频
- [ ] 运行测试

---



---

## Todo List

- [x] 修改 demo_extraction.py: 使用 human_render_camera (512x512) 拍摄
- [x] 实现贪心选块策略: 用 potentiality 逐步选最安全的5块
- [x] 全塔冻结机制: 除被抽木块外所有木块设 kinematic
- [x] 连续5块抽出+消失动画循环 + 导出视频
- [-] 运行测试

---



---

无 lint 错误。运行测试：

---



---

## Todo List

- [x] 修改 demo_extraction.py: 使用 human_render_camera (512x512) 拍摄
- [x] 实现贪心选块策略: 用 potentiality 逐步选最安全的5块
- [x] 全塔冻结机制: 除被抽木块外所有木块设 kinematic
- [x] 连续5块抽出+消失动画循环 + 导出视频
- [x] 运行测试

---

运行完美成功。

---

**执行结果：** `demo_phase1.mp4` — 525 帧, 17.5 秒, 30 FPS, 512×512 分辨率

**贪心选块结果（全部 potentiality = 1.0，移除后塔完全稳定）：**

| 顺序 | 木块 | 层.位 | 方向 |
|------|------|-------|------|
| 1 | #3 | L1[0] | +Y |
| 2 | #4 | L1[1] | +Y |
| 3 | #6 | L2[0] | +X |
| 4 | #9 | L3[0] | +Y |
| 5 | #7 | L2[1] | +X |

---

**设计要点：**

1. **拍摄视角** — 使用你指定的 `_default_human_render_camera_configs` 的 `render_camera`（512×512，`eye=[0.45, 0.35, tower_h*0.8]`，俯瞰塔体）

2. **塔体稳定性保证** — **全塔冻结策略**：调用 `_freeze_all()` 将所有 54 块木块设为 `kinematic = True`，这样除了被主动 `set_pose` 移动的那一块，其余所有木块物理上完全静止，不受重力/碰撞影响，保证录制过程中塔丝毫不动

3. **贪心选块算法** — 逐轮反事实模拟：每轮遍历所有候选块（排除 L0 底层 + L17 顶层），逐个模拟移除并物理前推 80 步，选 potentiality 最高的。选完后真正移除该块，进入下一轮（保证后续选择考虑了前面已移除的影响）

4. **动画结构** — 开头 30 帧静帧展示完整塔 → 每块：15 帧静帧 + 60 帧平滑抽出 + 消失 + 15 帧静帧 → 结尾 45 帧静帧

**修改文件：** 仅 `demo_extraction.py`（完全重写），未动 `jenga_tower.py`

---

_**User**_

最稳定的不应该先抽上面的吗（除了顶层）

---

_**Assistant**_

收到，爱学习的小龙虾！

你说得完全对。当前贪心选块的结果集中在 L1-L3 底部区域，这违反直觉——**上层的木块承载的负载更少，移除它们对塔稳定性的影响更小，应该优先被选中**。

问题出在我的 `_greedy_select` 中：由于 18 层标准 Jenga 塔非常稳定，底层和上层的 potentiality 都是 1.0（满分），贪心算法在同分时按遍历顺序选了索引最小的（即最底层的），没有 tiebreaking 策略。

**修复方案：** 当 potentiality 相同时（都是 1.0），应优先选**层号更高**（更靠塔顶）的木块。这只需在比较逻辑中加入层高作为 tiebreaker。

---



---

关键就在第 70-72 行：`if pot > best_pot` —— 当 potentiality 相同时不更新，导致低索引（底层）被选中。修复：同分时优先选层号更高的。

---



---



---



---

现在结果完全符合预期了。

**修复后的选块结果（从上往下选，全部 potentiality = 1.0）：**

| 顺序 | 木块 | 层.位 | 方向 |
|------|------|-------|------|
| 1 | #21 | **L7**[0] | +Y |
| 2 | #22 | **L7**[1] | +Y |
| 3 | #18 | **L6**[0] | +X |
| 4 | #19 | **L6**[1] | +X |
| 5 | #15 | **L5**[0] | +Y |

**修改内容：** 在 `_greedy_select` 的比较逻辑中增加了 tiebreaker——当 potentiality 相同时，优先选层号更高（更靠近塔顶）的木块。具体改了两处：

1. **候选列表按层号降序排列**：高层先遍历
2. **比较条件**：`pot > best_pot or (pot == best_pot and cand//3 > best_idx//3)` — 同分时层高优先

这符合 Jenga 的物理直觉：上层木块承载负载少，移除对整体稳定性影响最小，应该最先被抽取。

---

_**User**_

第一步的平滑抽出效果很好！现在我们要加入 Franka 机器人，制作以假乱真的‘机械臂推木块’ Demo。
我们不需要训练底层的 RL 控制策略，而是使用伪物理（Pseudo-physics）和末端位姿控制（IK / OSC）来实现视觉效果。
需求：
请编写一个函数 animate_franka_pushing(env, selected_actor, steps=60)。（假设 env 是包含 Franka 的 ManiSkill BaseEnv）。

预接触（Pre-contact）姿态：

获取 selected_actor 的中心坐标和抽出轴（例如 X 轴）。

计算出木块侧面的一个点作为接触点。

控制 panda 的末端执行器（EE）移动到该接触点外侧 2cm 处（即 Pre-contact Pose）。可以使用 ManiSkill 自带的 pd_ee_delta_pose 动作接口或简单的路径插值让机械臂先移动过去，期间记录渲染帧。

同步推动动画（绑定逻辑）：

当机械爪接触到木块侧面时，进入推动循环 for _ in range(steps):。

将 selected_actor 设为 Kinematic（防止由于机械臂微小误差导致木块乱飞）。

在每一步中，计算木块沿抽出轴的微小位移。更新木块的 Pose。

同步控制：命令 Franka 的末端执行器目标位姿（Target Pose）紧紧跟随该木块接触点的当前绝对坐标。

调用 env.step(action) 和环境的 render，将图像加入 frames。

抽出与撤回：

木块被完全推出塔身（移动了 extraction_distance）后，将木块瞬移至 [999, 999, 999]。

控制 Franka 机械臂收回原位。

记录随后塔身由于重力重新分布而产生的微微晃动（证明它是真实的物理塔，而不是静态模型）。

同样使用 imageio 保存为 demo_phase2.mp4。
请给出这个结合了机器人末端控制和物体 Kinematic 运动同步的代码实现。
机器人末端执行器一直保持闭合，并且末端执行器要正对着推木块的方向，选择推的接触面为机器人面向塔的那一面或者面向塔的左面，不要选对面（接触不到）

---

_**Assistant**_

<think><details><summary>Thought Process</summary>The user wants to create a demo where a Franka robot arm pushes Jenga blocks out of the tower. This is a "pseudo-physics" approach - we're scripting the motion rather than training an RL policy.

Let me understand the requirements:

1. **Pre-contact**: Move the Franka EE to a pre-contact pose near the block's side face
2. **Synchronized pushing**: Kinematic block + EE follows the block contact point
3. **Extraction & retraction**: Block disappears, arm returns home
4. **Camera**: Use the human render camera (512x512)
5. **Gripper**: Always closed


6. **EE orientation**: Facing the push direction
7. **Contact face**: The side facing the robot or the left side (not the far side)

Now I need to figure out the robot's position and orientation relative to the tower. The Franka is positioned at `[-0.615, 0, 0]` and the tower center is at `[0.2, 0.0]`, so the robot is to the left facing toward the tower in the +X direction. I should check what action controllers are available for the Panda to determine how to command the end-effector motion.

Looking at the action space, I'm trying to understand whether to use `pd_ee_delta_pose` for incremental end-effector commands or `pd_joint_pos` for direct joint control, and how to keep the gripper closed while pushing. For even-level blocks with their long axis along X, I need to figure out the geometry of which side to contact based on the block's position relative to the robot.

For odd-level blocks oriented along Y, the extraction direction changes, so I need to determine whether the robot should push from the -Y end face or the -X side face to successfully extract the block. 的那一面或者面向塔的左面"

The robot is at -X, facing +X. So:
- "面向塔的那一面" = the -X face of the block (the face closest to the robot)
- "面向塔的左面" = depends on perspective... from the robot's view looking at +X, left would be +Y

For even-level blocks (extract along +X):
- Push the -X end face → push direction is +X ✓ (same as extraction)

For odd-level blocks (extract along +Y):
- The -X side face is available → but pushing from -X side gives +X direction, not +Y
- We need to push along +Y, so we should push from -Y end
- But -Y end face might be accessible from the robot's "left" side

Now I'm working through the geometry more carefully. The robot is positioned at (-0.615, 0, 0) while the tower sits at (0.2, 0, z). For odd-level blocks with their long axis along Y, I need to access the -Y end face at y = -0.075, which the robot can reach by moving to the appropriate x-coordinate. For even-level blocks oriented along X, I'm calculating where the -X end face is positioned relative to the tower's center.

I'm also thinking about the end-effector orientation—it needs to face the push direction so the fingers can make contact and apply force effectively. Then I need to figure out which controller the Panda robot uses in ManiSkill, since the user wants end-effector control rather than joint-level commands. I'm weighing the tradeoffs between using ManiSkill's built-in EE delta pose controller versus computing IK myself, and I think the simplest approach is to leverage the existing `pd_ee_delta_pose` controller that handles the inverse kinematics internally. -contact: Interpolate EE position from current to pre-contact point
3. Push: Move EE along push direction while synchronizing block position
4. Retract: Move EE back

For the EE orientation, I need to think carefully about the Panda's TCP frame. In ManiSkill, the TCP sits between the finger tips, and its Z-axis points along the approach direction away from the arm. For pushing, I want the TCP to face the push direction, so the approach axis should align with where I'm pushing.

Let me check the panda.py file to understand the exact TCP frame convention, available controllers, and contact geometry, then implement the function. at the panda.py that the user has in their workspace.

Let me also think about controller modes. The user has `robot_uids="panda"` in the env. The default controller is likely `pd_joint_pos`. But we can specify control_mode when creating the env:

```python
env = gym.make("JengaTower-v1", control_mode="pd_ee_delta_pose", ...)
```

For `pd_ee_delta_pose`, the action is typically:
- [dx, dy, dz, dax, day, daz, gripper_width]
- Where d* are deltas in the world frame or EE frame

For the gripper to stay closed, we send gripper=-1 (or the minimum value).

Let me read the Panda code to confirm.

Now I'm thinking through the end-effector orientation needed for pushing blocks. For even-level blocks that need pushing along +X, the EE should be oriented to approach from that direction with its flat surface facing +X. Similarly, odd-level blocks need the EE facing +Y. Since the Panda's default resting pose has the end-effector pointing downward, I need to rotate it 90° around the Y-axis to align the approach direction with the push direction.

I should check the actual TCP frame definition in the panda.py file to get the quaternion right, but for now I'll focus on implementation. My strategy is to use the `pd_ee_delta_pose` controller, compute the desired end-effector pose at each step, and send delta commands clamped for smooth motion. Let me start by examining the panda.py file and then write the code.

I need to figure out the exact controller naming convention in ManiSkill3—whether it's `arm_pd_ee_delta_pose` combined with `gripper_pd_joint_pos` or something else. Once I understand the available controllers, I can structure the `animate_franka_pushing` function that takes the environment, selected actor, and number of steps, then integrate it with the greedy selection logic from before. I should check what utilities ManiSkill provides for trajectory generation and IK solving. For a smooth demo, I'm leaning toward using `pd_ee_delta_pose` with carefully tuned small deltas and PD gains rather than pre-computing joint trajectories, since that's simpler to implement and should give predictable motion if the gains are set right.</details></think>

---

收到，爱学习的小龙虾！

这是一个复杂的多步骤任务，让我先深入理解 Panda 机器人的控制接口和动作空间。

---



---



---

<think><details><summary>Thought Process</summary>Now I have a very clear picture. Let me analyze the key details:

**Panda robot:**
- Located at `[-0.615, 0, 0]`, facing +X direction
- TCP link: `panda_hand_tcp`
- Rest qpos: `[0, π/8, 0, -5π/8, 0, 3π/4, π/4, 0.04, 0.04]` (7 arm joints + 2 finger joints)
- Gripper closed: finger joints = 0 (or -0.01 for force), open = 0.04


- The PD end-effector pose controller uses a 7D action space with delta position and rotation commands, where rotation is expressed as axis-angle, plus a separate gripper command that ranges from closed to open
- The `build_grasp_pose` method is constructing the transformation matrix...

Writing pose transformation... I need to think through the gripper's coordinate frame more carefully. The Panda gripper closes along its Y-axis, so for a horizontal push along +X, the approaching direction should be [1, 0, 0], and the closing direction needs to be perpendicular to that—likely [0, 1, 0] or [0, 0, 1] depending on the gripper's orientation. Let me reconsider what "approaching" means in the context of the TCP frame and how the gripper's finger closure aligns with the desired push direction. object (Z-axis of TCP)
- `closing`: the direction the fingers close (Y-axis of TCP)
- `ortho`: the remaining axis (X-axis of TCP)

For a top-down grasp (gripper pointing down):
- approaching = [0, 0, -1] (approaching from above)
- closing = [0, 1, 0] (fingers close along Y)

For pushing along +X (horizontal push):
- approaching = [1, 0, 0] (gripper faces +X)
- closing = [0, 0, 1] or [0, 0, -1] (fingers vertical)

For pushing along +Y (horizontal push):
- approaching = [0, 1, 0] (gripper faces +Y)
- closing = [0, 0, 1] or [0, 0, -1] (fingers vertical)

For the control interface, I'm using `pd_ee_delta_pose` mode with a 7D action space: three dimensions for position deltas, three for rotation in axis-angle format (clamped to [-0.1, 0.1]), and one for gripper width. The gripper command is -0.01 when closed and 0.04 when open.

Now I'm planning the geometry for the block extraction task. The tower is centered at (0.2, 0.0) and the robot base is at (-0.615, 0, 0). For even-level blocks with their long axis along X, I'll contact the -X face at x = 0.125, approach from the -X direction, and push along +X. For odd-level blocks with their long axis along Y, the geometry will be similar but rotated accordingly.

For the odd-level blocks, I need to contact the -Y face at y = -0.075 and push along +Y, which aligns with the "left" direction from the robot's perspective facing the tower. But I'm realizing there's a geometric constraint I need to work through. The robot at (-0.615, 0, 0) is looking toward the tower along the +X axis, so from its perspective the -Y face of the odd-level blocks (which sits at y ≈ -0.075) is on the right side—that's where I need to apply the pushing force to extract along +Y. I'm reconsidering the push mechanics for odd-level blocks—the robot could approach from the -X face and push along +Y, which would look like a sideways gripper press even though it's not a traditional end push. Since this is a kinematic demo with synchronized motion rather than real physics, the visual appearance matters more than physical accuracy, so a side-face push that slides the block out would work fine.

Actually, end-face pushes look more natural: even levels get pushed from the -X end along +X, and odd levels get pushed from the -Y end along +Y. Let me go with that approach instead. The robot needs to position itself near the appropriate end face and push along the extraction direction, which means for odd levels it has to approach from -Y. Given the Panda arm's roughly 0.855m reach and the robot base positioned about 0.815m from the tower center, reaching that -Y face should be feasible, though I need to verify the exact geometry.

Now I'm checking whether the robot can actually reach the blocks at higher levels. The robot base sits at the table level (z=0), and blocks at levels 5-7 are around 0.15-0.21m above the table. Reaching forward about 0.8m horizontally while maintaining that height should be within the Panda's capabilities, but I need to confirm the exact positioning constraints.

Rather than getting bogged down in reachability calculations, I'll implement the approach and let the inverse kinematics solver tell me if something's infeasible. If it fails, I can adjust the strategy.

For the actual implementation, I'm starting with a pre-contact approach phase where I'll get the current TCP position and compute a pose that's about 2cm behind the contact face before moving in for the push.

During the push itself, I'll move the block incrementally and have the end effector follow along to maintain contact. For the approach motion, I need to be mindful of the control frequency—the environment steps at 20 Hz while the simulator runs at 500 Hz, so I should use delta commands with smaller increments like 0.005-0.01m per step to keep things smooth. I'll use a proportional controller to compute position deltas, clamping them to stay within reasonable bounds. For orientation, I need to calculate the rotation error between target and current orientations using quaternions, then convert that to an axis-angle representation for the delta rotation.

The tricky part is handling the quaternion format conversion—SAPIEN uses (w, x, y, z) while scipy expects (x, y, z, w), so I need to reorder the components before computing the error quaternion.

Converting quaternion formats...

Actually, I think using absolute pose control with `pd_ee_pose` is cleaner than delta control—I can just set the target end-effector pose directly each step and let the controller handle the inverse kinematics internally. I'm trying to figure out the rotation parameterization for the EE pose controller. In ManiSkill, it can use either quaternions in 4D or axis-angle in 3D format. Looking at the `arm_pd_ee_pose` configuration, it's set up with `use_delta=False` and `normalize_action=False`, which means the action directly specifies the target end-effector pose without normalization. I'm not sure about the rotation parameterization. Let me look at the PDEEPoseController implementation.

Actually, let me look at the controller configs more carefully. The `PDEEPoseControllerConfig` for delta pose has rotation bounds of [-0.1, 0.1], which suggests 3D axis-angle representation rather than quaternions. For the absolute pose controller, the rotation might be quaternion-based instead.

I'll use `pd_ee_delta_pose` since I know it's 7D with 3 position deltas, 3 axis-angle rotation deltas, and 1 gripper command, all clamped to [-0.1, 0.1]. Now I'm implementing a simple tracking controller that computes position and orientation errors, then clamps them to the action bounds.

Writing controller logic... I'm working through the rotation matrix construction for the TCP frame. When pushing along +X, the approaching direction is [1, 0, 0], the closing direction is [0, 0, 1], and I need to verify how these map to the rotation matrix columns using the cross product and stacking operation. closing direction should point vertically, so I'll set approaching = [1, 0, 0] for pushing along +X and closing = [0, 0, -1]. Computing the cross product gives me the orthogonal axis, which lets me construct the rotation matrix. Now I'm writing code to build the grasp pose...

For the +Y push direction, I'm using approaching = [0, 1, 0] with the same closing vector, verifying the vectors are perpendicular and normalized...

Now I'm setting up the animation function that will handle the pushing motion, extracting the necessary environment components and determining the block's geometry based on its level.

Defining the push parameters based on whether the block's long axis is aligned with X or Y, then calculating the contact point and approach direction accordingly.

Now I'm setting up the end-effector positioning sequence: first moving to a pre-contact location slightly before the block, then approaching the contact point itself, and finally executing the push motion while tracking the block's position.

Implementing the push loop...

Now I need to think through the action format more carefully. The controller expects a delta pose action with position and rotation deltas clamped to specific ranges, plus a gripper command, and I'm trying to figure out whether these deltas are applied in the world frame or the base frame. Computing the action deltas by taking the difference between target and current states, then clipping the position delta to a reasonable range and converting the quaternions to compute the rotation error in axis-angle form.

Now I need to convert this to a torch tensor since the environment expects that format, and I should be careful about the delta magnitude per step—keeping it around 0.005m at 20Hz gives smooth 0.1m/s motion that the controller can actually track.

For the full demo pipeline, I'm setting up the environment with delta pose control, then for each block I'll freeze everything as kinematic, disable collisions on the target, approach from the side, execute the push, and clean up. But there's a tension here: if all blocks are kinematic, I lose the gravity redistribution effect that would make the tower actually settle and prove it's structurally sound.

Looking back at the requirements, the user wants stability during the push but then wants to see the tower wobble slightly after extraction due to gravity. So the approach is to keep blocks kinematic while pushing, then unfreeze them afterward to let physics settle and capture that natural motion. I see the distinction now—phase 1 required everything frozen except the moving block, but phase 2 explicitly asks for that subtle wobble after extraction to prove it's a real physics simulation. So I need to unfreeze blocks after each block disappears, let gravity settle for about 30 frames, then refreeze for the next push. The tower should stay stable since we're removing the safest blocks, and the wobble will come naturally from the physics redistribution. I'm checking whether I can toggle blocks back from kinematic to dynamic...

Checking the physics body configuration...

Actually, rereading the user's request more carefully—they want the wobble recorded only at the very end after all extractions, not after each individual block removal. So the approach is: keep everything frozen during all the pushes, then unfreeze everything at the end and let the tower settle with that final wobble to demonstrate it's real physics.

Wait, but looking at it again, the instructions say to record the settling after each block is pushed out and the arm retracts. So I need to unfreeze briefly after each extraction for the physics to settle, then re-freeze before the next push. This could look visually awkward though with repeated freezing and unfreezing.

Actually, maybe the cleaner approach is to keep everything dynamic throughout and rely on the fact that we're selecting safe blocks that won't destabilize the tower. The user did mention using the freeze trick from phase 1, so let me re-examine what they're actually asking for in the implementation. 为 Kinematic（防止由于机械臂微小误差导致木块乱飞）" - they want the SELECTED block to be kinematic during push. They DON'T say to make OTHER blocks kinematic.

But from the previous conversation, they wanted other blocks to not move. Let me re-read the previous requirement: "除了移动的木块，塔的其他木块始终保持稳定不能移动"

That was for phase1 (demo without robot). For phase2 (with robot), the user specifically says to record tower settling. So for phase2:
- Selected block: kinematic during push (prevent physics explosion)
- Other blocks: stay dynamic (natural physics)
- After extraction: tower may settle/wobble slightly

Since we're selecting blocks with potentiality=1.0, the tower should stay stable with just minor settling.

During the push, the robot arm might collide with other blocks and knock them over, so I should disable collision on the selected block to prevent it from pushing others during extraction, be careful with the end-effector trajectory, and maybe disable collision between the robot and tower blocks for safety. Actually, the simplest approach for a demo would be to freeze all other blocks during the push phase, then unfreeze them after the push and retraction, letting physics settle for about 30 frames before the next push. This gives stability during extraction while still allowing natural wobbling afterward.

But there's a problem: when blocks are kinematic and the robot pushes against one, the kinematic block won't move, so the robot's joint torques will spike. would spike, and the arm might jerk. But since the selected block's collision is disabled, the arm shouldn't actually contact any block.

Hmm, but during the approach and push, the robot arm (links) might collide with non-selected blocks. If those blocks are kinematic, they won't move (good), but the robot arm would bounce off them (bad for smooth motion).

Let me think about this differently. For the cleanest demo:

1. Freeze all blocks during robot motion (kinematic)
2. Disable collision on the selected block
3. Push the selected block (kinematic + pose controlled)
4. After push, disappear the block
5. After arm retracts, unfreeze all remaining blocks briefly for settling
6. Re-freeze for next push

This should work well. Let me implement this.

When I unfreeze the blocks, they'll have zero velocity since they were kinematic. Gravity will start acting on them, and if the tower structure is still intact, they should settle slightly under their own weight. The blocks are already at equilibrium positions, so the settling motion should be very subtle—exactly the gentle wobble the user is looking for.

Now I'm writing the function to orchestrate the Franka pushing motion... I'm setting up the target end-effector orientation using the approaching and closing vectors, then calculating the contact point and pre-contact position for the push. Now I'm executing the push sequence in phases: first approaching the pre-contact position, then moving to contact, and finally pushing the block by incrementally updating its target position. Writing simulation logic...

I'm realizing I should simplify this by breaking it into helper functions for computing delta actions and moving the end effector, then use those in the main animation function. For the retraction phase, I need to capture the initial rest pose from the robot's keyframe configuration and reset to it after each push. the robot's qpos directly.

Hmm, but that would be a sudden jump. For smooth retraction, I should interpolate back. Let me use the same delta control approach to move back to a "home" position.

But what's the "home" position? The initial TCP position after reset. Let me save it.

Actually, for the cleanest approach:
1. Before starting, record the robot's initial TCP pose as "home"
2. After each push + disappear, move the EE back to home
3. Then proceed to next block

Let me think about the home position. After env.reset(), the robot is at the rest keyframe, which puts the arm in a typical resting configuration with the TCP somewhere above and behind the tower. I'll just read it from the agent after reset.

For the retract, I can move the EE backward away from the tower first, then up, then to home. This avoids colliding with the tower. The simplest approach would be to reverse the approach: move back along the negative push direction by about 0.05m, then move up by 0.1m, then move to the home position. Though since I'm already using pd_ee_delta_pose mode, I'll stick with that for consistency.

For a clean retraction path, I'll use three waypoints: pull back 5cm along the negative push direction, then move to a waypoint above the tower, then return to home. Each segment uses the existing move function.

Now I'm thinking through the timing for the demo at 30fps—approach takes about 3-4 seconds, contact and push another 3-4 seconds, retraction another 3-4 seconds, with settling in between. That puts each block extraction at around 12-14 seconds, so five blocks would be roughly a minute of video, which should look good.

I also need to remember the gripper stays closed throughout, which means setting that action dimension to -0.01.

Writing the extraction logic... I'm checking the action space dimensions for the pd_ee_delta_pose controller—the arm takes 6D input (3 for position, 3 for rotation) and the gripper uses a mimic configuration that collapses to 1D, giving me 7D total. Now I'm thinking through potential failure modes, particularly IK failures when the delta pose controller tries to reach unreachable targets for the blocks we're pushing.

The bigger concern is orientation tracking—if the end effector's current orientation is far from the target, the axis-angle delta could exceed the clamp limits, requiring multiple steps to converge. There's also the risk of arm-tower collisions during approach and retract, so I need to plan a safe trajectory that avoids the tower structure given the robot's starting position and the tower's height.

For a straightforward approach, I'll move the end effector directly to the pre-contact position using the P-controller and let the IK solver find the path, increasing max_steps if needed to ensure convergence.

I should set the target orientation from the start so the arm gradually rotates into position while approaching. Now I'm writing the movement function that computes actions iteratively, checking for convergence...

Still writing action computation... I need to correct the gripper action value—it should be -0.01 for closed, not -1. Looking at the controller config, the gripper range is [-0.01, 0.04], but since action normalization defaults to True, the normalized action space maps -1 to -0.01 (closed) and 1 to 0.04 (open).

Now I'm checking the controller configuration more carefully to understand how `normalize_action` is handled for the gripper across different control modes in ManiSkill3. = open (upper = 0.04), so I'll use -1 for a closed gripper. For the full action in `pd_ee_delta_pose` mode, I need a 7D vector: 6D for arm deltas (position and rotation) plus 1D for the gripper command. Let me verify that the controller expects this as a concatenated flat tensor.

Configuring the controller...

I need to figure out whether the delta pose is applied in the end-effector frame or the base frame—this affects how the actions translate to actual movements. ManiSkill3 likely has a `frame` parameter for this, but I'm not certain of the default behavior, so I'll proceed assuming it's in the base/world frame for now.

There's also a `use_target` parameter that changes how the controller interprets deltas: when it's False, the controller maintains an internal target and applies deltas to that, but when it's True, deltas are applied directly to the current end-effector pose. The EE gets driven towards that target, which acts like a velocity command in task space. With `pd_ee_target_delta_pose`, the delta applies directly to the current pose rather than accumulating, so if the EE lags behind, the target might overshoot. I think `pd_ee_delta_pose` with a proportional controller makes more sense—I'll set delta as a clipped gain times the error, letting the internal target gradually converge to the goal. But I realize I don't have direct access to the internal target state to verify this approach.

Instead, I'll compute the delta directly from the current EE pose and goal, clamping it to reasonable bounds. With a gain of 1, the delta equals the error up to the clamp limit, so the controller drives smoothly toward the goal. A gain of 2 would converge faster but risks oscillation. I'm leaning toward gain=1 or 2, paired with a stiffness around 1e3.

Actually, I'm noticing a potential issue: if I use the delta-pose mode without tracking the actual EE position, the internal target could drift away from reality, especially if the arm hits joint limits or lags. This might cause instability. Using the mode where delta is relative to the current EE pose would be more robust for tracking, since it keeps the target anchored to where the end-effector actually is.

Let me check if that controller variant is available in the config—yes, `pd_ee_target_delta_pose` is there. I'll use that instead, clamping the deltas to keep commands reasonable.

Defining the action space for this controller... At 20Hz with a 0.1 clamp, that gives me a max velocity of 2 m/s, which is more than enough—for smooth tracking I'd send much smaller deltas around 0.005. I'm going to go with `pd_ee_target_delta_pose` since it naturally converges: each step the delta shrinks as the end effector gets closer to the goal, and the PD controller smoothly drives toward the target.

The key thing I need to get right is the frame parameter—whether the delta is interpreted in the end effector's local frame or the world frame. Since I'm tracking a world-space target position, I need to make sure the delta is expressed in the base frame.

Let me check what the default frame is for this controller and handle the coordinate transformation if needed...

Writing controller configuration...

Actually, I realize I can't easily modify the pre-defined controller configs in the environment, so I'll need to work with the delta pose controller as-is and handle any coordinate frame conversions in my error calculation if the controller expects base frame inputs. the frame might default to "base").

Actually wait. Let me re-read the ManiSkill3 controller configs more carefully. In the PDEEPoseControllerConfig, the parameter `frame` might be:
- "base": deltas in robot base frame (world frame for fixed-base)
- "ee": deltas in end-effector frame

Looking at the ManiSkill3 source for PDEEPoseController... I don't have direct access, but typically:
- For `use_delta=True`, the delta is in the frame specified by `frame`
- Default frame is often "ee" for delta controllers

If the default is "ee", I need to transform my world-frame error to the EE frame.

But actually, for a push along a world-frame axis (like +X), the EE orientation is fixed (facing +X), so the transformation is just a rotation. Since I'm controlling the orientation to face the push direction, the EE's local axes align with specific world directions. For a push along +X, the EE's Z-axis points along the approach direction, Y-axis along the closing direction, and X-axis perpendicular to both—which I'm verifying through the cross product calculation.

Once I have the rotation matrix mapping EE frame to world frame, I can transform the world-frame error into the EE frame by applying the transpose of that rotation matrix. For a push along +X with the calculated orientation, a world-frame error along +X maps cleanly into the EE frame.

Rather than wrestling with delta pose transformations, I could simplify by using absolute pose control instead—just specify the target position and orientation directly without computing deltas. Let me check if the config supports this approach with the pose controller...

Configuring the controller...

Quaternion is likely the rotation parameterization for absolute pose mode, making the action space 8D total (3 position + 4 quaternion + 1 gripper), though axis-angle would be 7D. Rather than guessing, I should test `pd_ee_pose` directly to see what the actual action dimensions are. Using `pd_joint_pos` with precomputed IK seems overly complex, so let me explore the `pd_ee_pose` controller more carefully.

I'll go with `pd_ee_delta_pose` and test both world and end-effector frames to see which one works correctly. I could also use SLERP for smoother rotation interpolation instead of axis-angle error. At this point I'm overthinking it—let me just implement the code with `pd_ee_delta_pose` using a 7D action space (3D position delta, 3D rotation delta, 1D gripper) and debug from there if needed.

My approach: compute position delta as the clipped difference between target and current position in world frame, compute rotation delta as axis-angle error and clip it, set gripper to -1 for closed. If the controller expects end-effector frame instead, I'll add a transformation. Let me search the ManiSkill3 source to check if there's a frame parameter in the controller config, but honestly I should just write the code and iterate based on what happens.

Writing the implementation... Still writing action computation... Setting up the camera and determining the push direction based on the block's level—if it's even, I push along the x-axis, otherwise along the y-axis, with corresponding contact offsets and approach vectors.

Now I'm calculating the grasp pose using the approach and closing directions, then finding the contact point and a pre-contact position slightly before it. I'm saving the agent's home position and orientation, freezing all blocks while disabling collision on the selected one, then starting the approach phase by moving to a standoff position above the pre-contact point.

Continuing with the movement sequence... Finishing the velocity reset, then retracting the end effector back to the home position before unfreezing all the blocks to let them settle naturally.

Now I'm thinking through the physics simulation timing—when I call env.step() with the robot action, the kinematic blocks stay frozen in place while the dynamic robot articulation responds, and the selected block gets repositioned via set_pose before each step. The concern is whether the collision handling works correctly when the block is moved to its new position right before the physics step runs. I think the approach is solid: set the block pose, then call env.step() which handles physics and rendering in sequence, so both the robot and block end up at their correct positions in the captured frame. I should verify that env.step() automatically triggers the render update when needed, rather than requiring a separate call. The physics simulation maintains kinematic body poses, so the block should stay where I set it after env.step() runs. For the capture function, I'm setting up the render update and camera observation to grab the frame after the physics step completes.

Now I'm looking at the render_rgb_array method to understand how it captures images from the human render cameras, and considering whether to call env.render() directly or use the lower-level camera capture methods for my implementation. calls render_rgb_array for render_mode="rgb_array")
2. Manually capture from the human render camera

Using `env.render()` is simpler since it handles the render update logic automatically. The function returns a numpy array with shape (H, W, 3) in uint8 format for "rgb_array" mode, though ManiSkill3's batched environment might return (B, H, W, 3) which I'd need to squeeze if B=1.

Let me just use `env.render()` and handle whatever shape it returns, then write the code to capture frames after each step. I'm realizing that `env.step()` applies the action and runs physics simulation, but doesn't trigger rendering for state-based observations—that only happens when I explicitly call `env.render()`. So the sequence works: I set the block's pose, step the environment to update the robot, then render to capture both the block at its correct position and the robot at its new pose.

Now I need to verify that `env.render()` with "rgb_array" mode actually returns what I expect. Looking at the ManiSkill3 implementation, it dispatches based on render_mode and for "rgb_array" it calls a function that returns the rendered image from the human camera. The return type appears to be a tensor rather than a numpy array, so I'll need to convert it.

Checking the render function signature...

Writing conversion logic... I'm going to use this same approach from the phase1 demo. Now let me map out the complete flow for extracting multiple blocks—for 5 blocks, I'd create the environment, reset and warm up, then greedily select which blocks to extract. For each one, I'd freeze the others, disable collision on the target, execute the extraction sequence (approach, contact, push, disappear, retract), then unfreeze and let everything settle before re-freezing for the next block. After all extractions, I'd do a final settle with everything unfrozen and save the video. There's a slight complication where blocks shift position after unfreezing and settling, but that's actually fine—it just means they're at their new resting positions when I freeze them again for the next extraction.

Now I need to verify the kinematic flag behavior. When I set `body.kinematic = True` on a PhysxRigidDynamicComponent, it becomes kinematic, and setting it back to `False` should restore it to dynamic behavior. The thing is, kinematic bodies don't track velocity, so when I switch back to dynamic, the body starts with zero velocity and just falls under gravity. For blocks that are resting on other blocks, the contact forces should support them, so they should remain stable.

For the retraction phase, after pushing the block, I need to pull the end effector back along the opposite direction of the push, then move up and return to home. I'm thinking about whether to go back to home or move directly to the next pre-contact position, but for now I'll keep it simple and always return to home between blocks. There's also the question of what happens during physics simulation when the robot arm collides with kinematic blocks—they won't move, but the collision still registers.

To avoid jerky movements from collisions with the tower during approach and retract, I could disable collisions between the robot and tower blocks, but that's complex. Instead, I'll design the approach and retract paths to naturally avoid the tower. Since the robot starts on the -X side and the tower is on the +X side, approaching from the -X direction is safe. For blocks on odd levels where I push along +Y, I need to be careful not to pass too close to the tower during approach. My retraction strategy will be to pull back 5cm along the negative push direction, then move up above the tower to a safe clearance position, from which I can either move to the next approach position or return home.

For the standoff position, I'm computing it relative to the contact point by moving backward along the push direction and then raising it above the contact level. Now I'm writing the complete implementation, including a function to unfreeze any remaining blocks that haven't been moved off the board.

Writing the implementation... I see the unfreeze logic is straightforward—just set kinematic back to False for the remaining blocks since their collision groups were never changed. Now I'm working through the gripper orientation more carefully, specifically how the cross product works when pushing along the X-axis to determine the orthogonal direction.

I'm verifying the rotation matrix and then using the grasp pose builder to compute the quaternion directly rather than calculating it manually. I'm verifying that `Pose.q` returns a numpy array in (w, x, y, z) format, which should work for setting the target orientation. Rather than getting bogged down in frame parameter details, I'll start with world-frame deltas and adjust if needed—that's practical for a demo.

Looking at the `pd_ee_pose` controller config, I see it supports absolute pose mode with `use_delta=False`, which means I can directly specify the target position and quaternion without worrying about coordinate transformations. The action would be 8-dimensional: three position values, four quaternion components, and one gripper command.

Checking the controller configuration...

Actually, I'm going to stick with `pd_ee_delta_pose` since the delta approach is more straightforward and avoids potential quaternion ordering issues between SAPIEN's (w, x, y, z) format and what the controller expects. My final plan is to use delta pose control with 7 dimensions: three for position deltas, three for rotation axis-angle deltas, and one for the gripper.

Now I'll implement this with a world-frame P-controller, and if the end-effector moves in the wrong direction, I can switch to an EE-frame transform. For the demo, I'll start with a simple two-waypoint trajectory—moving from home to an approach position above and behind the contact point, then from there to the actual contact point—rather than a more complex multi-waypoint path.

Writing the control code... Finalizing the waypoint sequence for the push task: approach from home through a standoff position above the block, descend to pre-contact, then move forward to make contact. For the retract phase, I'll pull back and rise above before returning to home or moving to the next block. Now I need to check how the `pd_ee_delta_pose` controller interprets rotation deltas to make sure my pose calculations are correct.</details></think>

---



---



---



---

<think><details><summary>Thought Process</summary>Excellent! Now I have the full picture of the EE pose controller. Key findings:

1. **Frame**: Default is `"root_translation:root_aligned_body_rotation"`, meaning:
   - Position delta is in the **root frame** (robot base frame)
   - Rotation delta is **root-aligned body rotation** (rotation applied relative to root frame)

2. **Delta computation** (line 239-254):
   ```python
   delta_pos, delta_rot = action[:, 0:3], action[:, 3:6]
   delta_quat = matrix_to_quaternion(euler_angles_to_matrix(delta_rot, "XYZ"))
   ```
   - Position: `p = prev_pose.p + delta_pos` (root translation)
   - Rotation: `q = quaternion_multiply(delta_quat, prev_pose.q)` (root-aligned body rotation)
   - Delta rotation is Euler angles XYZ, converted to quaternion, then left-multiplied

3. **Normalization**: The action is normalized by default (`normalize_action=True`)
   - Position: scaled to [-pos_lower, pos_upper] = [-0.1, 0.1]
   - Rotation: normalized by norm, then scaled by rot_lower

4. **use_ee_frame** flag controls whether deltas are applied in end-effector frame or root frame, and the default behavior applies incremental updates to the previous pose with proper quaternion composition.

5. For the Panda configuration, I'm setting up a PD end-effector delta pose controller with position and rotation bounds of ±0.1, where the normalized action space gets scaled to these bounds.
   ```

python
   arm_pd_ee_delta_pose = PDEEPoseControllerConfig(
       pos_lower=-0.1, pos_upper=0.1,
       rot_lower=-0.1, rot_upper=0.1,
       ...
   )
   ```
   Since `normalize_action` defaults to True, the action space is 7D: 3D position deltas, 3D rotation deltas in Euler angles, and 1D gripper control (where -1 closes and 1 opens).

Now I'm tracing through the action clipping and scaling logic to understand exactly how the normalized [-1, 1] actions map to the actual control bounds. I'm seeing the issue now—after normalizing the rotation action to a unit ball, it's being scaled by `rot_lower` which is -0.1, and that would flip the sign of positive values, which doesn't make sense. Let me trace through the logic: first the code constrains the norm to be at most 1, then multiplies by what should probably be `rot_upper` (0.1) to scale it to the actual action bounds, not the negative lower bound. I think `rot_lower` represents the maximum absolute rotation magnitude in radians—so the rotation delta per step is capped at 0.1 radians. For position, the `clip_and_scale_action` function maps normalized actions from [-1, 1] to the range [-0.1, 0.1] meters per step. So I need to send normalized actions in [-1, 1] to my tracking controller.

For the actual control strategy, I'll compute the position error and divide by 0.1 to get the normalized action (clipped to [-1, 1]), and do the same for rotation error—either per axis or enforcing a norm constraint of 1. Let me test this approach and see if the rotation direction works out correctly.

Actually, I'm realizing the rotation might be inverted because `rot_lower` is negative. When the config sets `rot_lower=-0.1` and the code multiplies the action by this value, an action of 1 gets scaled to -0.1 instead of 0.1. That's backwards. Let me trace through how `clip_and_scale_action` actually uses these bounds to understand if this is intentional or a bug. So the scaled rotation ends up being [-0.05, 0, 0], which means positive action values get inverted to negative—that's definitely a sign flip happening. But then in `compute_target_pose`, the delta_rot is already the final scaled rotation in radians, so if I want a positive rotation I'd actually need to send a negative action value to compensate for this inversion. Let me trace through the `_clip_and_scale_action` function more carefully to understand what's really happening. I'm tracing through the class hierarchy to understand where `_clip_and_scale_action` gets called—it flows through `_preprocess_action` into `set_action`, then the result goes to `compute_target_pose`. I'm realizing `rot_lower` might represent the magnitude constraint on rotation rather than a signed bound, so let me test this empirically instead of guessing. Actually, I think using `pd_ee_delta_pose` with action normalization would be cleaner—I'd compute the position error as the difference between target and current position in the root frame, then normalize it.

For rotation I'd compute the desired Euler angle delta and account for the sign convention by normalizing against the 0.1 magnitude bound. But rather than wrestling with all this normalization logic, let me look for a control mode that doesn't require it. I'm checking the available configurations: `pd_ee_delta_pose` has normalization enabled by default, `pd_ee_target_delta_pose` is the same, and `pd_ee_pose` uses absolute positioning instead of deltas.

The `pd_ee_pose` controller with `normalize_action=False` gives me raw action values without scaling, where the action directly specifies the target pose in the root frame as Euler angles for rotation and position coordinates. Looking at the preprocessing logic, it seems `_clip_and_scale_action` only gets called when `normalize_action=True`, which means for `pd_ee_pose` with `normalize_action=False`, the raw action values in meters and radians are passed through directly without scaling.

The gripper has its own separate controller with its own `normalize_action` setting that defaults to True, so the gripper action stays normalized to [-1, 1] regardless of the arm's setting. So the full 7D action is the target position in meters, target rotation in Euler angles, and a normalized gripper command.

Using `pd_ee_pose` is much cleaner than dealing with delta actions and normalization—I just send the absolute target pose and let the PD controller with IK handle the convergence. I'm building the action by concatenating the target position, converting the grasp quaternion to Euler XYZ angles, and setting the gripper to -1 for closed.

Now I need to account for the coordinate frame: the controller expects poses in the root frame (the robot's base frame), but my target is in the world frame. Since the Panda base is positioned at (-0.615, 0, 0) in world coordinates, I need to transform the target accordingly. I need to reconsider the pose composition math here. When I invert the root pose and apply it to the end-effector pose, I'm not just adding the position vectors—I need to account for how pose transformations actually work with the inverse operation. Let me work through this more carefully to get the correct transformation from world frame to the root link's local frame. Now I'm working through the inverse transformation—the root pose has position [-0.615, 0, 0], and when inverted it becomes [0.615, 0, 0] with identity rotation. Composing this with the end-effector pose gives me the final position at the base frame as [ee_x + 0.615, ee_y, ee_z] with the end-effector's original orientation preserved.

For the actual target, I need to shift the world coordinates by that same 0.615m offset in X, putting the contact point at (0.74, 0, z)—well within the robot's reach. Now I'm converting the grasp quaternion to Euler XYZ angles using scipy's rotation utilities. I need to verify the scipy convention matches ManiSkill's approach—uppercase 'XYZ' should give me intrinsic rotations, which is what I need, but I should double-check whether ManiSkill's `euler_angles_to_matrix` follows PyTorch3D conventions to ensure compatibility. Now I'm working out the rotation matrices for the push actions. For pushing along the X-axis, I need to construct a rotation matrix from the approach direction, closing direction, and their orthogonal complement, then convert that to Euler angles in XYZ order.

Computing the rotation matrix...

Now I'm using scipy to convert this rotation matrix to Euler angles in XYZ order. For the second case with pushing along +Y, I need to recalculate the orthogonal vector using the cross product of the closing and approaching directions.

Let me verify the rotation matrix is valid by checking its determinant equals 1, then I'll think through whether the Panda arm's workspace can actually reach the contact points on the blocks.

For the even-level blocks positioned at the standard location, the contact point translates to about 0.74 meters from the base in the horizontal plane, which is well within the arm's 0.855-meter reach. For the odd-level blocks, I need to check if the offset positions still keep the contact points reachable.

Now I'm verifying the action space dimensions for the `pd_ee_pose` controller to make sure I'm using the right format for the arm commands. I see that when `pos_lower` and `pos_upper` are `None`, calling `np.broadcast_to(None, 3)` would fail, but since `pd_ee_pose` is actually used in practice, there must be some handling for `None` values that I'm missing. Let me trace through what actually happens with `None` in this context. Looking at how `PDEEPoseController` handles the action space initialization, it seems like passing `None` for position limits would cause `np.broadcast_to` to fail, so there must be some preprocessing that converts `None` values to actual bounds—likely very large default limits that effectively mean "no constraint." I'm going to stop overthinking the action space details and just go with `pd_ee_delta_pose` since I understand how it works. I'll handle the normalization pragmatically by clipping position errors to a reasonable range and scaling rotation deltas with a fixed coefficient.

For the rotation side, I need to work backwards from what the controller expects—since it applies a negative scaling factor of -0.1 to the action, I'll invert that to get the action I need to send. negated Euler delta divided by 0.1... wait that doesn't seem right either.

Actually, let me re-check. The `rot_action` after clipping and scaling ends up as the original action multiplied by -0.1 after norm clipping. So if I want a final rotation of 0.05 (5 degrees around X), I'd need the original action to be -0.5, which seems backwards but it's just a sign convention.

For my tracking controller, I'm computing the rotation error between target and current orientation, converting that to Euler angles, then inverting and scaling by the negative rotation factor to get the action I need to send.

Let me verify this with a concrete example: if the end effector currently faces down (+Z) and I want it to face right (+X), that's a 90-degree rotation around Y. After converting to Euler angles and applying the sign inversion and scaling, I get a clipped action that when scaled back produces the expected rotation delta. [0, -15.7, 0] after clipping becomes [0, -1, 0], then scaling by -0.1 gives [0, 0.1, 0]—a correct 0.1 radian rotation around Y. I'm refining the formula to compute the rotation error as the difference between target and current orientation, convert it to Euler angles, then normalize by clipping the norm to stay within [-1, 1] rather than clipping per-axis.

Handling norm-based clipping...

Now I'm realizing that with `use_target=False`, the target pose gets recomputed each step from the current EE pose plus the action delta, so the PD controller is effectively commanding incremental movements relative to the current position rather than absolute targets.

The key insight is that this creates a proportional control loop: the delta gets clipped based on limits, then added to the current pose to form the target, and the PD controller drives toward it. Since the actual EE position lags behind due to PD dynamics, the next step's delta is computed from wherever the EE actually is, not where it was commanded to go. This means the system converges through repeated small corrections rather than trying to reach a fixed target in one step.

I'm now deciding between two approaches: using the delta from the current pose versus the delta from the last target. The current pose approach is better for tracking because it always corrects based on where the end effector actually is right now, not where I previously commanded it to go. I see the distinction now—with `use_target=False`, the target is always relative to the current EE pose, so there's no accumulation and tracking stays tight. With `use_target=True`, targets accumulate if the EE can't reach them, which causes drift. For this tracking task, I should use `pd_ee_delta_pose` with `use_target=False` to get direct position commands. Now I'm working through the action computation logic...

Writing action computation code... Now I'm computing the position error in world frame and normalizing it to an action range, then calculating the rotation error by converting both quaternions to rotation matrices and finding the difference between them.

Wait, I'm realizing there's a frame mismatch—the controller expects position deltas in the base frame, but I'm computing the action in the world frame. I need to reconcile this discrepancy in how the position delta is being applied. Since the root frame is aligned with the world frame (just offset in position), the delta I computed in world coordinates translates directly to the base frame. The controller adds this delta to the current base position, which correctly gives me the target position in base frame coordinates. For rotation, the same logic applies—since the root orientation is identity, the rotation delta works the same way in both frames, and the controller applies it by multiplying the delta quaternion with the previous orientation.

Now I'm working through the inverse calculation for my tracking controller. I need to figure out what delta quaternion to apply given a target orientation, which means solving for delta_quat from the equation delta_quat * current_quat = target_quat, so delta_quat should be target_quat * current_quat inverse. Then I convert that back to Euler angles to get the rotation action.

But there's a subtlety with how quaternion multiplication works in the controller—the delta rotation gets applied in the world frame first before the existing rotation, which affects...

Actually, let me verify the rotation error computation is consistent with this. I'm computing `rot_error = target_R * current_R.inv()`, which gives me the world-frame rotation needed to go from current to target when applied as left-multiplication. That matches the controller's behavior, so the math is sound.

Now I need to double-check the quaternion multiplication convention in ManiSkill to make sure everything aligns...

Finalizing the implementation structure: I'll greedily select 5 blocks, then for each one freeze everything else, execute the manipulation sequence (approach, contact, push, disappear, retract), unfreeze for settling, and record the video. For the approach phase, I need to generate waypoints to guide the gripper into position. Retracting the end effector backward and upward, then starting the next approach from that position. I'll add some static frames between blocks for visual pacing, and I've calculated the timing—roughly 10 seconds per block across 5 blocks, totaling about 60 seconds of video. The gripper stays fully closed throughout, and I need to make sure the end effector orientation is correct before I start writing the code.

Now I'm thinking through the collision avoidance during the approach phase. For even-level blocks pushing along the X-axis, the approach comes from the robot's side without risk. For odd-level blocks pushing along the Y-axis, the approach path needs to stay clear of the tower as well.

The arm needs to reach around the tower from its base position, and since blocks are kinematic during approach, I need to plan the path carefully to avoid getting blocked by the tower structure itself.

I'm simplifying the approach into three main waypoints: first moving to a position behind and above the contact point, then descending to the block height while staying 2cm back, and finally moving in for contact. This avoids the complexity of routing around the tower while keeping the arm clear of obstacles.

For the retraction sequence, I'll pull back 5cm from the push point at the same height, then lift up vertically to clear the tower.

Actually, for a demo I might not need all these waypoints—a direct path through joint space using the P-controller could work fine, and minor interpenetration during approach is acceptable as long as the push and retract motions look clean.

Writing the demo code... I need to manually call `update_render()` before capturing frames since `env.step()` with state-only observations doesn't trigger rendering, so my `_capture_frame` function handles this by updating the render and then grabbing the RGB data from the camera.

Now I'm ready to write the actual implementation, and I'm deciding whether to use the greedy block selection from before or hardcode specific blocks for faster iteration—I'll go with hardcoding the blocks from the previous run to speed up development.

I'm also thinking through the reachability constraints for blocks at different levels, calculating their heights based on the layer positions to make sure they're actually reachable by the robot arm. The Panda's base sits on the table, with its first link extending upward before the arm reaches outward—for a horizontal push at around 20cm height, the arm needs to stay relatively horizontal, which should be within its workspace. Higher blocks like those at 50cm would be easier to reach, but the mid-tower selection (L5-L7) offers a better visual demonstration, so let me test that approach first, though I'm also considering mixing blocks from different heights for a more dramatic effect.

Adding logic to penalize or skip blocks from already-selected layers...

Actually, let me just go with the greedy selection as-is for now and move forward with implementation. I need to verify the control mode first — ManiSkill3 defaults to `pd_joint_pos`, but I'll need to specify `pd_ee_delta_pose` when creating the environment to use end-effector control. Let me check the available controller configurations...

Checking controller options...

Good, `pd_ee_delta_pose` is available. Now I'm ready to write the actual code. One thing to keep in mind: with `pd_ee_delta_pose`, the action space is 7-dimensional (6 for the arm plus 1 for the gripper), and I need to understand how the observations are structured when I call `env.step()`.

Setting up the environment with the correct configuration...

For the gripper control, I need to send -1 to keep it closed in `pd_ee_delta_pose` mode, and the robot starts from a rest keyframe position.

Configuring initial state...

I'm realizing the gripper needs to be closed before the pushing motion starts. The simplest approach is to send a gripper command of -1 from the beginning, which will close it within a few steps. By the time the arm reaches the contact point dozens of steps later, it'll be fully closed. Now I need to think through the geometry—the TCP sits between the fingertips, and the closed gripper's flat surface has some physical extent that matters for how it contacts the block.

Writing the demo script... Writing utility functions... Still writing geometry calculations... Writing the action computation function... Still writing the movement function... Writing the push demo sequence... Writing the push and retraction phases... Writing greedy selection logic... Finishing the pose update and then stepping the simulation to let the selected block settle, then I'm printing out which block was chosen this round along with its stability score, resetting the scene state, and returning the list of selected blocks. After that I'm setting up the argument parser for the main script with options like the number of blocks to extract.

Setting up the Jenga environment...

Now I'm performing a greedy selection algorithm to determine which blocks to extract, then resetting the environment and preparing to record the demonstration with the camera positioned for rendering.

Building the recording loop... Writing the main loop...

I'm realizing there's a state management issue—after the pushing animation completes, the blocks remain frozen, so I need to unfreeze them before the settling phase. Then before each subsequent push, I'll need to re-freeze all the blocks to set up for the next action.

The key insight is that `animate_franka_pushing` already handles freezing at the start, so when it's called again in the next iteration, it'll automatically re-freeze everything. The block positions get updated during settling, and since I'm reading the current pose when computing push geometry, the arm will push from the correct location each time.

During the settling phase, the robot stays at the retreat position while I send zero actions—the arm holds its position through PD control, though I need to think through what the zero gripper action actually does.

Setting the gripper to -1 keeps it closed during settling, while the arm's zero action translates to zero deltas that maintain the current end-effector pose.

During settling, the blocks unfreeze and fall under gravity while the robot stays stationary, then the next push phase refreezes them and approaches the next block. When `env.step()` runs during pushing, the frozen blocks remain kinematic and don't move from physics, only from the explicit `set_pose` calls.

Now I'm realizing the episode might truncate early since we're running way more than the 100-step limit across all five blocks—the approach phase alone takes 80+ steps. When truncation happens, the environment resets automatically, which would break the multi-block sequence.

I can work around this by either increasing the max_episode_steps parameter when creating the environment, or using the unwrapped environment to bypass the TimeLimit wrapper entirely.

Looking at how gymnasium handles this, I should be able to pass a larger max_episode_steps value directly to gym.make() to override the default of 100 steps set during registration. Setting it to something like 100,000 should give me plenty of room for the demo without triggering early termination.

One more thing to handle: after calling env.step(), I need to make sure the render is updated before capturing frames, since for state-based observations the step might not trigger a visual update automatically. My capture function should handle calling scene.update_render()...

Still writing capture logic... I'm tracing through the push loop sequence: I set the block pose, then step the environment which runs physics for 25 sub-steps while the kinematic block stays fixed, then capture the frame showing both the robot at its target position and the block where I placed it. The key insight is that during those physics sub-steps, the block remains stationary since it's kinematic, so the final captured state should have everything positioned correctly.

However, there's a timing concern—the robot's end effector might not fully reach the exact target position in just one control step because the PD controller has limited stiffness and only 0.05 seconds to converge. For the push motion itself, the block moves incrementally (about 2.5mm per frame across 60 frames), and the end effector needs to track this. If the EE lags slightly behind, there'll be a small gap, but it should remain visually convincing. The P-controller continuously aims for the current contact point, so any lag should stay bounded rather than accumulating.

For a visual demo, a few millimeters of gap is imperceptible anyway—the gripper is right next to the block, and from the camera's perspective at 512x512 resolution, it won't be noticeable. I'm satisfied this approach will work, so let me start writing the final implementation.

Actually, I need to verify the `_compute_ee_action` function logic. Checking how the position error is calculated in world coordinates and then scaled...

Verifying action scaling... So when the position error exceeds 0.1m, the delta gets clamped to 0.1m in that direction. For rotation, I'm computing the euler angle error between target and current orientation, then normalizing and clipping the rotation action to prevent overshooting. I'm working through the two cases to verify the math: when the error is small, the rotation delta equals the error itself, and when it's large, it gets clamped to a fixed magnitude of 0.1 radians in the error direction.

Now I need to finalize the code and think through the settle phase—specifically whether to keep the arm frozen or retract it during settling to avoid collisions with falling blocks. I'm calculating the end-effector positions for the retract sequence—pulling back 5cm along the push direction and lifting 5cm, then another 10cm upward to clear the tower. This should position the arm safely above the structure before the next block placement.

Now I'm realizing the arm might collide with the kinematic tower blocks during approach and retract movements. Since kinematic bodies act like immovable walls, the arm would bounce off them rather than passing through, which could cause jerky, unpredictable motion. I could disable collisions on the tower blocks entirely so the arm can move freely through them, or design approach paths that avoid the tower, though disabling collisions is simpler for a demo. the target block. For blocks at mid-levels like L6-L7, approaching from above means the arm could clip through the blocks stacked above them. Instead, I should approach horizontally from the side where the robot is positioned, moving the arm forward at the same height as the contact point rather than descending into the tower. This means setting up waypoints: first moving to a pre-approach position that's at block height but further back, then moving forward to just behind the contact face, and finally making contact. I'm calculating the pre-approach position as a point behind the pre-contact location along the push direction.

Calculating approach waypoints... # 8cm further back

For even-level pushes along +X, the pre-approach position is around 0.025m in X, which puts the arm about 0.64m from the base—well within reach. For odd-level pushes along +Y, I need the arm at roughly 0.2m in X and -0.175m in Y, giving a distance of about 0.83m from the base, which is tight but feasible.

I'm considering a multi-step approach: move to a high standoff position first to orient the arm safely above the tower, then descend to block height and approach the contact point. But this is getting complicated for a demo. Let me simplify—I can move directly to the pre-contact position from wherever the arm is now, and if there's a collision with the kinematic blocks, the arm will just push through since they don't move anyway.

Actually, wait. The blocks are all kinematic, which means they have collision but won't move. That means the arm would get blocked and pushed back, preventing it from reaching the target at all. I need to disable collision on the blocks during the approach and push phases, then re-enable it afterward. I'm thinking about how to manage this with PhysX collision groups—either temporarily disabling collision on all blocks except the one being pushed, or using a collision group setup that allows block-to-block interaction but prevents block-to-robot blocking.

Handling collision groups with bitwise operations...

Saving and restoring collision groups... I'm disabling collisions for all actors by setting their collision groups to zero, but that adds unnecessary complexity. Instead, I'll use a simpler standoff approach where the robot approaches blocks from outside the tower—for even-level blocks, pushing along the X-axis from the robot's side, and for odd-level blocks, pushing along the Y-axis with the arm positioned to avoid tower collisions. to go around the tower to reach the -Y face
- This is where collision might occur

For odd-level blocks, the arm might collide with even-level blocks at the same or similar heights while reaching around to the -Y face.

Hmm, for the demo, maybe I should only push even-level blocks along +X (from the robot's direct line of sight). For odd-level blocks, I could push from the -X side face instead (but then the push direction is +X, not +Y, which doesn't match the extraction direction).

Actually, the user said: "选择推的接触面为机器人面向塔的那一面或者面向塔的左面". For ALL blocks, I could push from the -X face (the face facing the robot). For even-level blocks, this IS the end face (long axis along X), so pushing from -X along +X extracts the block along its long axis. ✓

For odd-level blocks, the -X face is the SIDE face (long axis along Y). Pushing from the -X side along +X would push the block out of the tower to the +X side, which is a sideways extraction rather than the standard along-the-long-axis approach. This simplifies the demo since I can use a consistent push direction and contact face for all blocks, even if it means odd-level blocks get extracted sideways instead of along their long axis. So I'm settling on a unified approach: push every block from the -X face along the +X direction. For even-level blocks oriented along X, this extracts them along their long axis as expected. For odd-level blocks oriented along Y, it extracts them sideways instead, which is less conventional but much simpler for the robot's reachability—the arm just extends forward without needing to reach around the tower. The contact point and push direction stay consistent throughout. x = 0.35 is well outside the even-level tower bounds of x = 0.125 to 0.275, so the extracted block clears the tower completely.

The edge blocks at offset_x = ±0.055 follow the same pattern and also push out successfully. Since all blocks can be pushed from their -X faces along the +X direction, I can simplify the approach significantly.

Now I'm setting up the geometry computation to always use +X as the push direction...

Still writing geometry code...

Actually, I'm realizing the extraction direction should align with the push direction—if I'm pushing along +X for odd-level blocks, that's where they'll slide. In standard Jenga the extraction follows the long axis, but pushing from the side along the short axis is also valid as long as the block clears the tower. For this demo, either approach works since the goal is just to show the block sliding out successfully.

Looking back at what the user specified—pushing from the face the robot is facing, which is the -X face for all blocks—this means pushing along +X for everything. That's actually the simplest and most reliable approach, and it matches exactly what the user asked for. This resolves the complexity nicely.

Since all blocks are pushed from the same face with the same end-effector orientation, the approach path is straightforward: move from the current position to 10cm behind the contact point at block height, then 2cm behind, then make contact. The arm just extends forward along the X-axis, and there's no collision risk with the tower since the approach comes from outside the block's footprint.

For odd-level blocks, I need to verify the approach height clears any even-level blocks below. The odd-level block's side face is around x = 0.175, and the approach at x = 0.155 passes above the even-level blocks since they're at a lower Z height.

Now I'm realizing there's another collision concern—the arm itself, not just the end effector, has physical width. When the arm extends along the positive X direction to approach the tower, the wrist and elbow links might dip to heights that could hit the tower structure, so I need to account for the full arm geometry, not just the end effector position.

Looking at the Panda's kinematics, the shoulder sits at z ≈ 0.333m while the end effector approaches at z ≈ 0.228m, so the arm descends from shoulder to target. Since the elbow and upper arm segments stay above the end effector height and the approach comes from behind and above, the arm should clear the tower blocks without collision.

Now I'm ready to implement this. I'll add some static frames before and after each push to improve the visual pacing, and slow down the pushing motion slightly for better effect. Writing the demo code... I'm defining helper functions to manage the physics state of the blocks—freezing them in place by making them kinematic and zeroing their velocities, unfreezing specific blocks after removal, and disabling collisions for actors. I'm also setting up the push direction constant for the block manipulation task.

Now I'm establishing the end-effector orientation for pushing—the robot approaches along the +X axis with fingers closing along -Z—and creating a lazy-initialized reference pose that converts the grasp orientation to Euler angles for use during pushing operations.

I'm defining helper functions to calculate contact offsets based on block level parity, then computing the 7D end-effector action by calculating position and rotation errors between the current TCP state and target pose.

Finishing the action computation...

Now I'm setting up the end-effector tracking loop that runs for a maximum number of steps, computing actions at each iteration to move toward the target position and orientation, capturing frames along the way, and stopping early if the end-effector gets close enough to the goal.

For the Franka pushing animation, I'm extracting the block's position and level, calculating the contact point and pre-contact approach position based on the push direction and offset, then setting up the sequence of movements.

Now I'm freezing all blocks except the target one, then orchestrating the approach phase with three waypoints—a standoff position above and behind, then moving to pre-contact, and finally to the contact point. After that, I'm computing the push trajectory by calculating the incremental displacement the block should move per step.

Executing the push loop...

Now I'm setting up the retraction phase—pulling the block back from the contact point with an upward offset, then moving to a hover position before returning the frames. After that, I'm defining a greedy selection function that will evaluate actors based on extraction safety, using simulation steps and a stability threshold.

For the selection loop, I'm initializing tensors for distance tracking and removed/excluded sets, then iterating through extraction rounds to find the safest blocks to remove, prioritizing top blocks first as a tiebreaker while evaluating each candidate's stability. Writing simulation and selection logic... Setting up the environment and resetting it with a seed, then running some initial steps with zero actions to stabilize the simulation. I'm using a greedy selection algorithm to determine which blocks to extract from the tower, then resetting again and accessing the camera for rendering.

Now I'm recording a Franka robot pushing demonstration by iterating through the selected blocks in order, animating each push action with specified parameters, and collecting all the rendered frames into a sequence.

After each block is pushed, I unfreeze the remaining blocks to let physics settle for a moment before re-freezing them for the next extraction, then I capture additional frames during this settling phase and a final outro sequence before writing everything to a video file.

Wait, I'm realizing there might be a synchronization issue here—when I call `env.step()` during the push phase and then `_capture_frame()` which updates the render state, there's a question of whether `env.step()` is already handling the visual sync internally or if I need to explicitly call `update_render()` depending on the observation mode.

My `_capture_frame()` explicitly calls `scene.update_render(update_human_render_cameras=True)` before capturing, so that should ensure the visual state is current regardless of what happened in `env.step()`. The code should work fine.

I'm going to skip adding extra validation checks for the action space and control mode—if something's misconfigured, the error from `env.step()` will be clear enough. But I'm wondering about the `max_episode_steps` parameter and how it's configured in ManiSkill3...

In gymnasium, I can pass `max_episode_steps` directly to `gym.make()` to override the registered default, but ManiSkill3 might handle this differently since it uses its own registration system. The `TimeLimit` wrapper should still respect the override, so passing a large value like 100000 should work to bypass the default 100-step limit. Limit` wrapper truncates episodes after `max_episode_steps` steps.

Overriding in `gym.make()` should work:
```python
env = gym.make("JengaTower-v1", max_episode_steps=100000, ...)
```

But I'm not 100% sure ManiSkill3's `gym.make` passes this to `TimeLimit`. Let me try and see.

Actually, in gymnasium, `max_episode_steps` in `gym.make()` overrides the registered value. So this should work.

If it doesn't work, I can use `env.unwrapped` to bypass the step limit. But then I'd need to call `env.unwrapped.step()` instead of `env.step()`, which might not apply the action correctly (since controllers are part of the env, not the wrapper).

Actually, the action gets processed by the environment's controller regardless of whether I call `step()` on the wrapper or unwrapped env. The key difference is that the wrapper checks for truncation. For my demo, I can just ignore the `truncated` flag since I never check it, so the environment won't auto-reset. Though I should verify how gymnasium's `TimeLimit` wrapper actually handles truncation...

In newer gymnasium versions, `TimeLimit` sets `truncated=True` but doesn't auto-reset—the user must call `reset()` manually. So I can safely ignore the flag and keep stepping. I'll set a large `max_episode_steps` value to avoid issues with some gymnasium versions that might error after truncation, then finalize the code and check how `gym.make()` handles the parameter. I'm realizing there's a collision risk during the descent phase—when the arm moves from standoff down to pre-contact height, it could clip through blocks at intermediate levels. For instance, pushing a block at level 5 means descending through level 6's height, which might have blocks in the way depending on their XY positions.

Actually, checking the geometry more carefully: the standoff position is well behind the tower's boundary in the X direction, so even as the arm descends vertically, it stays outside the tower's footprint and avoids collision with blocks at higher levels.

For the actual movement sequence, the `_move_ee` function handles transitions smoothly by computing the displacement from wherever the arm currently is to the target position, so subsequent block approaches naturally start from the previous retract position without needing explicit repositioning.

Now I need to verify the gripper behavior — with the normalized action space, sending -1 closes the gripper fully while 1 opens it, and since the rest keyframe starts with the gripper open at 0.04, it'll gradually close as I send the -1 commands during the grasp phase.

Before writing the final code, I'm checking whether the EE orientation is actually achievable given the Panda's joint limits. The push configuration I'm targeting (EE facing +X with vertical fingers) should work at the block heights, though IK might fail if the arm can't reach certain positions.

The selected blocks are quite low (17-23cm above the table), requiring the arm to extend nearly fully while reaching forward and down from the shoulder at 33cm height. This is within the workspace, but if IK fails during execution, I may need to adjust the block selection strategy. For now, I'll just run it and see what happens.

Writing the final code... Configuring the excluded range to keep only the middle layers (L8-L16) gives me 27 reachable blocks with heights between 0.26m and 0.50m, which should be both practical and visually compelling for the demo.

Now I'm thinking through what actually makes this look good: blocks need to be visible above the table, the arm movements should stay within comfortable joint ranges, and the extraction itself should be clear and satisfying. Mid-tower blocks around L8-L12 hit that sweet spot perfectly, so I'll adjust the exclusion to focus on that range.

Actually, I'm overthinking this—let me just exclude the bottom few layers for structural integrity and the very top, then let the greedy algorithm handle the rest. That means keeping L5 through L16, which gives plenty of options for the selection to find the safest blocks.

For the greedy selection, I'm realizing the safest picks are always the ones where the tower stays perfectly stable after removal—potentiality of 1.0. In a standard 18-layer tower, most blocks should meet that criterion...

I need to stop deliberating and actually write the code to test this. Let me hardcode the previously selected blocks for faster iteration...

Setting up argument parsing...

I should also account for the case where the arm doesn't reach the target within the step limit—the function will just return what it collected, and the demo continues even if there's a slight gap during the push phase.</details></think>

---

## Todo List

- [x] 理解 Panda 控制器 API (pd_ee_delta_pose) 和 EE 坐标系
- [x] 理解 PDEEPoseController action 归一化/缩放逻辑
- [-] 设计推块几何: 全部从 -X 面推 +X 方向 (robot 直线接触)
- [ ] 编写 demo_franka_push.py: EE tracking + 同步推 + 撤回
- [ ] 运行测试 + 迭代修复

---

<think><details><summary>Thought Process</summary>Now I've thoroughly analyzed the controller source code. Key findings:

1. **Frame**: `"root_translation:root_aligned_body_rotation"` — pos delta in root frame, rot delta in root-aligned frame
2. **Normalization**: `normalize_action=True` by default
   - Position: `clip_and_scale_action(action, -0.1, 0.1)` → maps [-1,1] to [-0.1, 0.1]
   - Rotation: norm-clip to unit ball, then multiply by `rot_lower=-0.1`
3. **Action dim**: 6 (arm) + 1 (gripper) = 7D


4. **Rotation uses Euler XYZ angles after scaling, while position operates in the root frame (essentially world frame since root orientation is identity)**
5. **The action scaling inverts the error signals: position actions get divided by 0.1 then clipped, and rotation actions get negated and divided by 0.1, both ultimately clamping the errors to ±0.1 magnitude**
6. **The sign inversion in rotation cancels out correctly**

For the pushing strategy, I'm using a consistent approach where all blocks get pushed along the +X direction from their -X face, with the contact offset depending on whether the level is even or odd to account for the block's orientation. The end effector always approaches along +X and closes along -Z, which keeps the robot from having to reach around the tower.</details></think>

---

通过深入阅读控制器源码，我现在完全掌握了 `pd_ee_delta_pose` 的归一化逻辑。关键洞察：

- **Position**: `clip_and_scale(action, -0.1, 0.1)` — 归一化 `[-1,1]` 映射到 `[-0.1, 0.1]` m/step
- **Rotation**: 先 norm-clip 到单位球，再乘 `rot_lower=-0.1` — 符号取反后自然取消
- **Frame**: `root_translation:root_aligned_body_rotation` — 位移和旋转都在 root frame（与 world frame 对齐）

**几何策略**：所有木块统一从 **-X 面** 沿 **+X** 方向推出（机器人直线接触），避免绕塔到达对面。

---

## Todo List

- [x] 理解 Panda 控制器 API (pd_ee_delta_pose) 和 EE 坐标系
- [x] 理解 PDEEPoseController action 归一化/缩放逻辑
- [x] 设计推块几何: 全部从 -X 面推 +X 方向 (robot 直线接触)
- [-] 编写 demo_franka_push.py: EE tracking + 同步推 + 撤回
- [ ] 运行测试 + 迭代修复

---



---



---

## Todo List

- [x] 理解 Panda 控制器 API (pd_ee_delta_pose) 和 EE 坐标系
- [x] 理解 PDEEPoseController action 归一化/缩放逻辑
- [x] 设计推块几何: 全部从 -X 面推 +X 方向 (robot 直线接触)
- [x] 编写 demo_franka_push.py: EE tracking + 同步推 + 撤回
- [-] 运行测试 + 迭代修复

---

先用一块做快速验证测试，跳过耗时的贪心选块。

---



---

<think><details><summary>Thought Process</summary>The error is `IndexError: list index out of range` at `uw.blocks[block_idx]` with `block_idx=46`. Let me check how many blocks exist. NUM_BLOCKS = NUM_LEVELS * 3. Let me check NUM_LEVELS.</details></think>

---

Block index 46 超出范围。让我先查看总块数。

---



---

<think><details><summary>Thought Process</summary>NUM_LEVELS = 9, so NUM_BLOCKS = 27. Block indices are 0-26. I used block 46 which doesn't exist. Let me pick a valid block. For 9 levels (L0-L8), safe choices from upper levels would be L7 (indices 21-23) or L6 (indices 18-20).

Also, I need to update the excluded range in `_greedy_select`:
- Current: `set(range(15))` excludes L0-L4 (indices 0-14), but NUM_BLOCKS=27, so L5-L7 (indices 15-23) + L8 (24-26, excluded as top layer) = only L5-L7 available.


- With only 9 layers, I should exclude less: `set(range(3))` (L0 only) and `set(range(n-3, n))` (L8). Let me fix the excluded range and try with a valid block index like block 22 from L7.</details></think>

---

NUM_LEVELS=9, 总共 27 块 (0-26)。需要修正排除范围和测试索引。

---



---



---

<think><details><summary>Thought Process</summary>The test ran successfully. The demo is 254 frames at 30 FPS = 8.5 seconds. There's a Gimbal lock warning from the Euler angle computation, which is expected when the rotation passes through certain orientations. I should switch to quaternion-based error computation to avoid this issue.

Let me check the video to see if the motion looks correct. But since I can't view the video directly, let me check the output file exists and has reasonable size.

Also, I should address the Gimbal lock warning. Instead of using Euler XYZ angles for the rotation error, I can use the rotation vector (axis-angle) representation, which doesn't have Gimbal lock issues.


Actually, looking at the controller code, the action's rotation component is already interpreted as Euler XYZ angles and converted to quaternions internally, so the Gimbal lock warning is coming from my error computation rather than the controller itself.

The real issue is that when I decompose the rotation error back to Euler XYZ using `(tgt_r * cur_r.inv()).as_euler('XYZ')`, I can hit Gimbal lock. For small rotations this might not be a practical problem, but I could avoid it by using axis-angle representation and converting that to Euler XYZ instead, which would align better with what the controller expects. The controller expects Euler XYZ angles, so if I pass rotation vectors directly, it'll misinterpret them as Euler angles—which works fine for small rotations but breaks down for larger ones. I'm realizing the real issue is that I need to either convert the rotation vector back to Euler XYZ (which reintroduces Gimbal lock) or find a way to make the controller work with a Gimbal-lock-free representation from the start.

Actually, I can work backwards from what the controller needs. Since it converts Euler XYZ to a quaternion delta and multiplies with the previous quaternion, I can compute the exact Euler XYZ angles required to produce the desired quaternion update by inverting the process: taking the target quaternion, multiplying by the inverse of the current quaternion, and converting that delta back to Euler XYZ. That's what I'm already doing with the current approach, so the math checks out—the issue must be elsewhere.

The Gimbal lock warning suggests I should switch to rotation vectors instead, which avoid the ambiguity. Since the controller clips rotations to at most 0.1 radians per step, small rotation vectors are nearly equivalent to Euler angles anyway, so the tracking should still converge even if the intermediate orientations differ slightly from the ideal path. Updating the rotation error calculation...

Since small rotations are nearly equivalent between Euler and rotation vector representations, I can switch to using rotvec for cleaner code and to avoid the warning. The magnitude will still be clamped to at most 0.1 radians per step, so the controller should converge properly. Let me make that change and then check the video to verify the motion looks correct.</details></think>

---

运行成功，生成了 254 帧 / 8.5 秒的测试视频。有一个 Gimbal lock 警告需要修复 — 改用 `rotvec`（旋转向量）代替 Euler 分解来避免万向节锁。

---



---

<think><details><summary>Thought Process</summary>256K is a reasonable size for a test video. Let me check the video visually by reading the first frame's shape and verifying it's 512x512.

Actually, I should look at the video to check if the robot motion is correct. But I can't directly view it. Let me check a few things:

1. Is the video file readable? (yes, it exists)
2. Does it have the right resolution? (should be 512x512)

Let me also fix the Gimbal lock issue and run the full 5-block demo.First, let me fix the rotation computation:</details></think>

---

修复 Gimbal lock 警告 — 用 `rotvec` 替代 `as_euler('XYZ')`：

---



---

现在运行完整 5 块 Demo。先用贪心选块，然后录制。

---



---

<think><details><summary>Thought Process</summary>The full demo ran successfully:
- 5 blocks extracted: L7[0], L7[1], L6[0], L6[1], L5[0]
- All with potentiality 1.0
- 1160 frames, 38.7 seconds at 30 FPS
- The Gimbal lock warning still appears (it's triggered during the initial approach when the orientation error is large)

Let me fix the remaining Gimbal lock warning. The warning comes from `_get_push_euler()` where I convert the target quaternion to Euler. But this is stored as the target euler and passed to `_ee_action`. The `_ee_action` then creates `tgt_r = R.from_euler('XYZ', target_euler)`. 

If I pass the target as a quaternion or rotvec instead, I avoid the Euler conversion entirely. Looking at the stacktrace more carefully, the warning is actually triggered inside `_ee_action` when computing the relative rotation and converting it to Euler angles, even though I already switched to using rotvec elsewhere. I need to trace through exactly where this conversion is happening in the call from line 312.

Actually, the real culprit is in `_get_push_euler()` which converts a quaternion directly to Euler XYZ angles. Let me check the actual orientation being computed—with an approach direction of [1,0,0] and closing direction of [0,0,-1], the resulting rotation matrix has a (0,2) element equal to 1, which means sin(β) = 1 for the Y rotation. This is exactly the Gimbal lock singularity for Euler XYZ decomposition, since the Y angle becomes ±90°.

The fundamental issue is that this particular orientation—where the approach direction is aligned with the X-axis—hits the XYZ Gimbal lock singularity. Instead of storing and computing with Euler angles, I should store the target orientation as a scipy Rotation object and compute rotation errors directly as rotation vectors, which avoids the problematic Euler conversion entirely. For the controller, since it only applies small rotations (≤0.1 rad per step), interpreting the 3D action vector as a rotation vector versus Euler angles makes negligible difference.

Refactoring the code...</details></think>

---

视频生成成功！1160 帧 / 38.7 秒。但还有 Gimbal lock 警告 — 原因是推块方向 (+X) 恰好是 Euler XYZ 的万向节锁奇点 (β = π/2)。彻底修复：改用 scipy `Rotation` 对象存储目标姿态，全程用 `rotvec` 避免 Euler 分解。

---



---



---

Now update all callers to use `target_rot` (Rotation object) instead of `target_euler`:

---



---



---



---

<think><details><summary>Thought Process</summary>Gimbal lock warning is gone. The test runs successfully with 264 frames / 8.8s. Let me now run the full 5-block demo with the fix.</details></think>

---

Gimbal lock 警告消除。现在运行完整的 5 块 Demo。

---



---



---

## Todo List

- [x] 理解 Panda 控制器 API (pd_ee_delta_pose) 和 EE 坐标系
- [x] 理解 PDEEPoseController action 归一化/缩放逻辑
- [x] 设计推块几何: 全部从 -X 面推 +X 方向 (robot 直线接触)
- [x] 编写 demo_franka_push.py: EE tracking + 同步推 + 撤回
- [x] 运行测试 + 迭代修复

---

清理测试文件。

---

undefined

---

完整 Demo 生成成功。让我查看最终代码确认一切干净。

---



---

完美，代码干净无警告。让我总结完成的工作：

---

**`demo_franka_push.py` 已完成，输出 `demo_phase2.mp4`（1160 帧 / 38.7s / 902KB）。**

### 核心设计决策

**1. 控制模式**: `pd_ee_delta_pose`（7D action = 3D pos + 3D rot + 1D gripper）
- 深入阅读了 `PDEEPoseController` 源码，精确理解了归一化/缩放管线：
  - Position: `action × 0.1` → 每步最大 0.1m 位移
  - Rotation: `action × rot_lower(-0.1)` → 符号取反，每步最大 0.1 rad 旋转
  - 据此推导出 `pos_act = clip(error/0.1, -1, 1)` 和 `rot_act = -rotvec_err/0.1` 的正确公式

**2. 推块几何**: 统一从 **-X 面沿 +X 方向** 推出
- 机器人在 `(-0.615, 0, 0)` 面朝塔 (+X 方向)，直线接触最可靠
- Even 层推 -X 端面（`offset = -BLOCK_L/2`），Odd 层推 -X 侧面（`offset = -BLOCK_W/2`）
- EE 姿态: `approaching=[1,0,0], closing=[0,0,-1]`（指尖朝 +X，夹爪竖直闭合）

**3. 万向节锁修复**: 推块方向 (+X) 恰好是 Euler XYZ 的奇点 (β=π/2)，改用 `scipy.Rotation.as_rotvec()` 避免，对 ≤0.1 rad 小旋转与 Euler XYZ 等价

**4. 动画流程** (每块):
| 阶段 | 描述 |
|---|---|
| Standoff | EE 移到接触点后方 7cm、上方 6cm |
| Pre-contact | 下降到木块高度，距接触面 2cm |
| Contact | 前进 2cm 贴合木块面 |
| Push | 60 步同步: kinematic 木块 `set_pose` + EE P-tracking 跟随接触点 |
| Disappear | 木块瞬移 `[999,999,999]` |
| Retract | EE 后撤 + 上提到安全高度 |
| Settle | 解冻全塔 → 30 帧物理 settling（重力微晃） |

**修改文件**: 仅新建 `/Users/jianzihang/Downloads/RL_Robot/demo_franka_push.py`，未修改任何已有文件。

---

_**User**_

但是这个demo中推的面不对，应该是木块面积小的面和方向，现在推到大面去了

---

_**Assistant**_

收到，爱学习的小龙虾！

你说得对。让我仔细分析几何关系：

---



---



---

<think><details><summary>Thought Process</summary>Now I see the issue clearly. Let me analyze the block geometry:

**Even level (level % 2 == 0):**
- `half = [BLOCK_L/2, BLOCK_W/2, BLOCK_H/2]` → X=long(0.075), Y=short(0.025), Z=height(0.015)
- Long axis is along X
- Blocks are at `pos = [center_x, center_y + offset, z]` (spread along Y)
- End caps (small faces) are along X axis (at ±BLOCK_L/2)
- **Small face = end face = at ±X direction** (size: BLOCK_W × BLOCK_H = 5cm × 3cm)


- **Large face = side face = ±Y direction** (size: BLOCK_L × BLOCK_H = 15cm × 3cm)

**Odd level (level % 2 == 1):**
- `half = [BLOCK_W/2, BLOCK_L/2, BLOCK_H/2]` → X=short(0.025), Y=long(0.075), Z=height(0.015)
- Long axis is along Y
- Blocks are at `pos = [center_x + offset, center_y, z]` (spread along X)
- End caps (small faces) are along Y axis, and the large faces are along the X direction with size BLOCK_L × BLOCK_H = 15cm × 3cm

I'm realizing the issue with my contact offset—using `[-BLOCK_W/2, 0, 0]` for odd-level blocks gives me the -X face, but I need to verify whether that's actually the large face or if the geometry is oriented differently than I thought. I see the issue now — for odd-level blocks, the small end faces are actually oriented along the Y axis, not X, so I need to push from the -Y face instead. The robot should approach from `(block_x, block_y - BLOCK_L/2, z)` and push in the +Y direction, which aligns with what the user meant about pushing the small face along its long axis.

Let me check if this is actually reachable from the robot's base position at (-0.615, 0, 0). For a block at (0.2 + offset_x, 0, z), the pre-contact point at the -Y face would be roughly 0.82 meters away, which should be within reach.

Now I'm thinking through the end-effector orientation when pushing in the +Y direction—the gripper needs to face +Y while the fingers stay vertical. But there's a potential collision problem: to reach the -Y face of blocks positioned along the X axis, the arm has to extend from the robot's position toward (0.2, -0.095, z), and the arm links might interfere with the tower structure as they pass near the -X side.

The blocks at even levels are clustered around x ≈ 0.2 along the Y axis, while odd-level blocks spread along the X axis near y ≈ 0. The arm's path from the robot base through joint space toward the target would pass through the southwest region relative to the tower, which should avoid direct collisions with the blocks themselves. However, since the blocks are kinematically frozen during the push phase, they act as immovable obstacles—if the arm hits them, it won't reach the target position.

I'm reconsidering the standoff approach: starting from a point above the -Y face and descending to make contact.

Updating the code to define push directions and contact offsets... I see the issue—the approach direction and contact offsets are hardcoded for even levels, but odd levels need different push directions entirely. I need to refactor the code to compute push direction, standoff, and pre-contact positions dynamically for each block rather than using a global constant, since the approach comes from different axes depending on the level.

For the rotation matrices, I'm working out the orthogonal vector by taking the cross product of the closing and approaching directions—for odd levels where we approach along the Y-axis and close along Z, that gives us the X-axis as the orthogonal component.

Now I'm constructing the full rotation matrix from these three vectors and checking whether each orientation hits gimbal lock. The even-level case does run into gimbal lock when the approaching vector aligns with the Z-axis, which is why using rotation vectors handles both cases correctly without singularities.

Now I'm defining a helper function that returns the push direction, contact offset, and target rotation based on whether the block level is even or odd.

Finishing the rotation calculation...

Now I need to verify the reachability for odd-level blocks at specific positions to make sure the gripper can actually access them. _x ∈ {-0.055, 0, 0.055} for i=0,1,2
- For block at i=0: pos = (0.145, 0, 0.228)
- Contact point: (0.145, -0.075, 0.228)
- Pre-contact: (0.145, -0.095, 0.228)
- Distance from base (-0.615, 0): sqrt(0.76² + 0.095²) ≈ 0.766m — within reach ✓
- For block at i=1: pos = (0.2, 0, 0.228)
- Contact: (0.2, -0.075, 0.228)
- Distance from base: sqrt(0.815² + 0.075²) ≈ 0.818m — within reach ✓
- For block at i=2: pos = (0.255, 0, 0.228)
- Contact: (0.255, -0.075, 0.228)
- Distance from base: sqrt(0.87² + 0.075²) ≈ 0.873m — TIGHT, close to 0.855m limit!

The rightmost block at i=2 is pushing against the Panda's reach limit at 0.873m, which exceeds the nominal 0.855m maximum. Though I'm using straight-line distance here, the actual reachability depends on the arm's joint configuration—at this lower height of 0.228m relative to the shoulder, the arm might still be able to extend far enough, but it's definitely at the boundary.

Looking at the greedy selection strategy, blocks #21 and #22 from the L7 layer (with offsets at i=0 and i=1) are comfortably reachable, while block #23 (i=2) is marginal. Since the greedy algorithm prioritizes blocks with better potential scores, it'll naturally favor the safer positions, so reachability shouldn't be a practical issue.

Now I need to refine the code to properly account for the per-block push geometry.</details></think>

---

你说得完全对。让我分析清楚几何关系：

**Even 层**: `half = [BLOCK_L/2, BLOCK_W/2, BLOCK_H/2]`，长轴沿 X → 小端面在 ±X 方向 → 推 -X 面 ✓

**Odd 层**: `half = [BLOCK_W/2, BLOCK_L/2, BLOCK_H/2]`，长轴沿 Y → 小端面在 ±Y 方向 → 但我代码推的是 -X 面（那是 **大侧面** `BLOCK_L × BLOCK_H`）✗

修复方案：**Odd 层改为从 -Y 小端面沿 +Y 方向推**。

---



---

<think><details><summary>Thought Process</summary>Now I need to refactor to have per-block push direction and EE orientation. The key changes:

1. Remove global `PUSH_DIR` and `_get_push_rot()`
2. Add `_push_geometry(level)` that returns `(push_dir, contact_offset, target_rot)` per block
3. Update `animate_franka_pushing` to use per-block geometry

Let me write the updated code:</details></think>

---

现在重构代码 — 把全局 `PUSH_DIR` 和固定 `_get_push_rot()` 替换为根据 level 动态计算的推块几何：

---



---

Now update `animate_franka_pushing` to use per-block geometry instead of the old globals:

---



---



---

<think><details><summary>Thought Process</summary>Good, the test passed for the odd-level block (L7[1]). No Gimbal lock warnings. 335 frames / 11.2s. The extra steps (compared to the previous 264) are because the approach to the -Y face takes more steps (the arm needs to reach further around the tower).

Now let me also test with an even-level block to make sure both work:</details></think>

---

Odd 层测试通过。再测 even 层确认两个方向都正确。

---



---

两个方向都正确运行，无警告。运行完整 5 块 Demo。

---



---

<think><details><summary>Thought Process</summary>Exit code 139 means SIGSEGV (segfault). This happened with sandbox restrictions. The `rm` command might be causing issues with the sandbox. Let me try without the sandbox.</details></think>

---



---



---

已修复，`demo_phase2.mp4` 重新生成（1325 帧 / 44.2s / 812KB）。

### 修改内容

**问题根因**：Odd 层木块的长轴沿 Y，半尺寸为 `[BLOCK_W/2, BLOCK_L/2, BLOCK_H/2]`。之前统一从 -X 面推，对 Odd 层来说推到的是 **大侧面**（`BLOCK_L × BLOCK_H = 15cm × 3cm`），而非小端面。

**修复**：用 `_push_geometry(level)` 根据层奇偶动态计算：

| 层类型 | 长轴 | 小端面方向 | push_dir | contact_offset | EE approaching |
|---|---|---|---|---|---|
| Even | X | ±X | `[1,0,0]` | `[-BLOCK_L/2, 0, 0]` | `[1,0,0]` |
| Odd | Y | ±Y | `[0,1,0]` | `[0, -BLOCK_L/2, 0]` | `[0,1,0]` |

所有 standoff、pre-contact、push、retract 路径都基于 `push_dir` 动态计算，两个方向的 EE 姿态分别缓存。

---

_**User**_

现在选择的木块会让塔倒，选择一定不会倒的5个木块

---

_**Assistant**_

收到，爱学习的小龙虾！

让我先看看当前贪心选块的逻辑，理解为什么 potentiality=1.0 的块组合仍然会导致塔倒。

---



---

我找到了三个关键问题：

1. **模拟步数太少** (80 步)：每层只有 3 块，抽中间那块后塔顶可能需要更长时间才表现出不稳定（缓慢倾斜），80 步不够暴露延迟崩塌。
2. **贪心不考虑组合效应**：每轮只移除 1 块做模拟，但连续移除 5 块的累积效应没有被正确传播 — 因为每轮结束时用 `scene.step()` 让塔 settle，但这个 settle 后的状态可能已经亚稳态了，下一轮的 `get_sim_state` 保存的是这个亚稳态。
3. **每层只抽中间块最安全**：Jenga 经典策略是每层只抽中间那块（index % 3 == 1），两侧块仍支撑上层。目前没有优先中间块的逻辑。

**最稳妥的修复策略**：增加模拟步数 + 每轮选块后做累积验证 + 优先中间块 + 不选同层 2 块。

---



---

