# V2-Suction: æ™ºèƒ½æœºå™¨äººå¸ç›˜æŠ“å–å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**V2-Suction** æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„é«˜çº§æœºå™¨äººå¸ç›˜æŠ“å–ç¯å¢ƒï¼Œä½¿ç”¨ManiSkillæ¡†æ¶æ„å»ºã€‚è¯¥é¡¹ç›®å®ç°äº†å¤æ‚çš„å¤šç‰©ä½“å †å åœºæ™¯ä¸­çš„æ™ºèƒ½æŠ“å–ä»»åŠ¡ï¼Œæ”¯æŒå¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒï¼Œå…·å¤‡å®Œæ•´çš„8çŠ¶æ€æŠ“å–æµç¨‹å’Œå…ˆè¿›çš„å¸ç›˜çº¦æŸæœºåˆ¶ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **ğŸ¤– æ™ºèƒ½æŠ“å–ç­–ç•¥**ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”ç‰©ä½“é€‰æ‹©å’ŒæŠ“å–
- **âš¡ å¤šç¯å¢ƒå¹¶è¡Œ**ï¼šæ”¯æŒé«˜æ•ˆçš„å¤šç¯å¢ƒåŒæ­¥è®­ç»ƒ
- **ğŸ”§ åŒåŠ¨ä½œæ¨¡å¼**ï¼šåŒæ—¶æ”¯æŒç¦»æ•£å’Œè¿ç»­åŠ¨ä½œç©ºé—´
- **ğŸ“Š ä¸°å¯Œå¥–åŠ±ç³»ç»Ÿ**ï¼šå¤šç»´åº¦å¥–åŠ±è®¾è®¡ä¼˜åŒ–æŠ“å–è¡Œä¸º
- **ğŸ® çµæ´»é…ç½®**ï¼šå®Œæ•´çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- **ğŸ“¹ å¯è§†åŒ–æ”¯æŒ**ï¼šè®­ç»ƒè¿‡ç¨‹å½•åˆ¶å’Œåˆ†æå·¥å…·

### ğŸ—ï¸ æŠ€æœ¯æ¶æ„

```mermaid
graph TD
    A[EnvClutterç¯å¢ƒ] --> B[å¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ]
    A --> C[8çŠ¶æ€æŠ“å–æµç¨‹]
    A --> D[å¸ç›˜çº¦æŸæœºåˆ¶]
    
    B --> E[è‡ªå®šä¹‰PPOè®­ç»ƒ]
    B --> F[Stable-Baselines3è®­ç»ƒ]
    
    C --> G[çŠ¶æ€1: å®šä½ç›®æ ‡]
    C --> H[çŠ¶æ€2-8: æŠ“å–æ‰§è¡Œ]
    
    D --> I[Driveçº¦æŸç³»ç»Ÿ]
    D --> J[ç‰©ç†çº¦æŸéªŒè¯]
    
    K[é…ç½®ç³»ç»Ÿ] --> A
    K --> E
    K --> F
    
    L[å·¥å…·æ¨¡å—] --> M[æ€§èƒ½åˆ†æ]
    L --> N[è§†é¢‘å½•åˆ¶]
    L --> O[è¯„ä¼°æŒ‡æ ‡]
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
v2-suction/
â”œâ”€â”€ ğŸ”§ æ ¸å¿ƒç¯å¢ƒ
â”‚   â”œâ”€â”€ env_clutter.py              # ä¸»è¦ç¯å¢ƒå®ç° (1583è¡Œ)
â”‚   â”œâ”€â”€ config.py                   # é…ç½®ç®¡ç†ç³»ç»Ÿ (359è¡Œ)
â”‚   â””â”€â”€ utils.py                    # å·¥å…·å‡½æ•°é›†åˆ (497è¡Œ)
â”‚
â”œâ”€â”€ ğŸ¯ è®­ç»ƒç³»ç»Ÿ
â”‚   â”œâ”€â”€ train_sb3.py                # SB3è®­ç»ƒè„šæœ¬ (413è¡Œ)
â”‚   â”œâ”€â”€ training.py                 # è‡ªå®šä¹‰PPOè®­ç»ƒ (566è¡Œ)
â”‚   â””â”€â”€ inference.py                # æ¨¡å‹æ¨ç†è„šæœ¬ (317è¡Œ)
â”‚
â”œâ”€â”€ ğŸ”— ç¯å¢ƒåŒ…è£…å™¨
â”‚   â””â”€â”€ wrappers/
â”‚       â”œâ”€â”€ mask_wrapper.py         # åŠ¨ä½œæ©ç åŒ…è£…å™¨
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“Š æµ‹è¯•ä¸åˆ†æ
â”‚   â”œâ”€â”€ test_quick_fix.py           # å¿«é€Ÿæµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ debug_wrapper.py            # è°ƒè¯•åŒ…è£…å™¨
â”‚   â””â”€â”€ example.py                  # ä½¿ç”¨ç¤ºä¾‹
â”‚
â”œâ”€â”€ ğŸ“‹ æ–‡æ¡£ä¸é…ç½®
â”‚   â”œâ”€â”€ requirements_sb3.txt        # ä¾èµ–åˆ—è¡¨
â”‚   â”œâ”€â”€ PROBLEM_ANALYSIS_SOLUTION.md
â”‚   â”œâ”€â”€ INDEX_FIXES_SUMMARY.md
â”‚   â”œâ”€â”€ PERFORMANCE_ANALYSIS.md
â”‚   â”œâ”€â”€ suction.md                  # å¸ç›˜æœºåˆ¶æ–‡æ¡£
â”‚   â”œâ”€â”€ parallel-training.md        # å¹¶è¡Œè®­ç»ƒæ–‡æ¡£
â”‚   â””â”€â”€ README_*.md                 # å„ç§è¯´æ˜æ–‡æ¡£
â”‚
â””â”€â”€ ğŸ’¾ è¾“å‡ºç›®å½•
    â”œâ”€â”€ models/                     # è®­ç»ƒæ¨¡å‹
    â”œâ”€â”€ logs/                       # è®­ç»ƒæ—¥å¿—
    â””â”€â”€ test_videos/                # æµ‹è¯•è§†é¢‘
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
cd /path/to/RL_RobotArm-main/v2-suction

# å®‰è£…ä¾èµ–
pip install -r requirements_sb3.txt

# å®‰è£…ManiSkillç¯å¢ƒï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
pip install mani-skill
```

### 2. å¿«é€Ÿæµ‹è¯•

```bash
# è¿è¡ŒåŸºç¡€æµ‹è¯•
python example.py

# è¿è¡Œè°ƒè¯•æµ‹è¯•
python test_quick_fix.py
```

### 3. å¼€å§‹è®­ç»ƒ

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨Stable-Baselines3 (æ¨è)

```bash
# å¿«é€Ÿè®­ç»ƒï¼ˆå°è§„æ¨¡ï¼‰
python train_sb3.py --num_envs 32 --total_timesteps 100000

# é«˜è´¨é‡è®­ç»ƒï¼ˆå¤§è§„æ¨¡ï¼‰
python train_sb3.py --num_envs 128 --total_timesteps 1000000 \
    --record_video --log_dir ./logs/high_quality

# è¯„ä¼°æ¨¡å‹
python train_sb3.py --mode eval --model_path ./models/best_model.zip
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰PPO

```bash
# å¼€å§‹è®­ç»ƒ
python training.py --config fast_train --epochs 500

# å¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ
python training.py --config multi_env --num_envs 8
```

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„è¯¦è§£

### 1. ğŸŒ ç¯å¢ƒç³»ç»Ÿ (`env_clutter.py`)

#### æ ¸å¿ƒç±»ï¼š`EnvClutterEnv`

```python
@register_env("EnvClutter-v1", asset_download_ids=["ycb"], max_episode_steps=200)
class EnvClutterEnv(BaseEnv):
    """
    å¤æ‚å †å æŠ“å–ç¯å¢ƒæ ¸å¿ƒç±»
    
    ä¸»è¦ç‰¹æ€§ï¼š
    - æ”¯æŒ3-18ä¸ªYCBç‰©ä½“çš„å †å åœºæ™¯
    - Panda/Fetchæœºå™¨äººæ”¯æŒ
    - 8çŠ¶æ€æ™ºèƒ½æŠ“å–æµç¨‹
    - å¤šç¯å¢ƒå¹¶è¡Œä¼˜åŒ–
    """
```

#### å…³é”®åŠŸèƒ½æ¨¡å—

1. **ğŸ² åœºæ™¯ç”Ÿæˆç³»ç»Ÿ**
   ```python
   def _load_scene(self):
       """åŠ¨æ€ç”Ÿæˆå¤šç‰©ä½“å †å åœºæ™¯"""
       # æ‰˜ç›˜å’Œç‰©ä½“éšæœºæ”¾ç½®
       # æ”¯æŒ1-16ä¸ªç¯å¢ƒå¹¶è¡Œ
       # YCBæ•°æ®é›†ç‰©ä½“åŠ è½½
   ```

2. **ğŸ¯ 8çŠ¶æ€æŠ“å–æµç¨‹**
   ```python
   def _pick_object_8_states(self, obj_idx: int, env_idx: int = 0):
       """
       çŠ¶æ€1: é€‰æ‹©ç›®æ ‡ç‰©ä½“
       çŠ¶æ€2-3: ç§»åŠ¨åˆ°ç‰©ä½“ä¸Šæ–¹
       çŠ¶æ€4-5: ä¸‹é™æ¥è§¦ç‰©ä½“
       çŠ¶æ€6: æ¿€æ´»å¸ç›˜çº¦æŸ
       çŠ¶æ€7-8: æŠ“èµ·å¹¶æ”¾ç½®
       """
   ```

3. **ğŸ”§ å¸ç›˜çº¦æŸæœºåˆ¶**
   ```python
   def create_suction_constraint(self, target_object, env_idx):
       """åˆ›å»ºç‰©ç†å¸ç›˜çº¦æŸ"""
       constraint = Drive.create_from_actors_or_links(
           scene=self.scene,
           entities0=self.agent.tcp,     # æœºæ¢°è‡‚æœ«ç«¯
           entities1=target_object,      # ç›®æ ‡ç‰©ä½“
           pose0=sapien.Pose(),
           pose1=sapien.Pose(),
           scene_idxs=torch.tensor([env_idx], device=self.device)
       )
   ```

### 2. âš™ï¸ é…ç½®ç³»ç»Ÿ (`config.py`)

#### åˆ†å±‚é…ç½®æ¶æ„

```python
@dataclass
class Config:
    env: EnvConfig           # ç¯å¢ƒå‚æ•°é…ç½®
    training: TrainingConfig # è®­ç»ƒè¶…å‚æ•°é…ç½®  
    model: ModelConfig       # ç½‘ç»œæ¶æ„é…ç½®
    reward: RewardConfig     # å¥–åŠ±å‡½æ•°é…ç½®
    evaluation: EvaluationConfig  # è¯„ä¼°æŒ‡æ ‡é…ç½®
```

#### é¢„è®¾é…ç½®æ–¹æ¡ˆ

```python
PRESET_CONFIGS = {
    "default": {...},           # æ ‡å‡†è®­ç»ƒé…ç½®
    "fast_train": {...},        # å¿«é€Ÿè®­ç»ƒé…ç½®
    "high_quality": {...},      # é«˜è´¨é‡è®­ç»ƒé…ç½®
    "sparse_reward": {...},     # ç¨€ç–å¥–åŠ±é…ç½®
    "multi_env": {...},         # å¤šç¯å¢ƒé…ç½®
    "large_scene": {...},       # å¤§åœºæ™¯é…ç½®(18ç‰©ä½“)
    "small_scene": {...}        # å°åœºæ™¯é…ç½®(6ç‰©ä½“)
}
```

### 3. ğŸ¯ è®­ç»ƒç³»ç»Ÿ

#### Stable-Baselines3é›†æˆ (`train_sb3.py`)

```python
# æ”¯æŒMaskablePPOå’Œæ ‡å‡†PPO
if MASKABLE_AVAILABLE:
    model = MaskablePPO("MlpPolicy", vec_env, **model_kwargs)
else:
    model = PPO("MlpPolicy", vec_env, **model_kwargs)

# å¤šç¯å¢ƒå‘é‡åŒ–
vec_env = ManiSkillSB3VectorEnv(env)
```

#### è‡ªå®šä¹‰PPOå®ç° (`training.py`)

```python
class PPOAgent:
    """å®Œæ•´çš„PPOæ™ºèƒ½ä½“å®ç°"""
    def __init__(self, state_dim, action_dim, lr_actor=3e-4, lr_critic=3e-4):
        self.actor = PPOActor(state_dim, action_dim)
        self.critic = PPOCritic(state_dim)
        # æ”¯æŒç¦»æ•£åŠ¨ä½œçš„Categoricalåˆ†å¸ƒ
```

### 4. ğŸ”„ ç¯å¢ƒåŒ…è£…å™¨ç³»ç»Ÿ (`wrappers/`)

#### åŠ¨ä½œæ©ç åŒ…è£…å™¨
```python
class ExtractMaskWrapper(gym.ObservationWrapper):
    """å°†è§‚æµ‹æ‹†åˆ†ä¸º[action_mask, state_features]"""
    
class ActionConversionWrapper(gym.ActionWrapper):  
    """è¿ç»­åŠ¨ä½œåˆ°ç¦»æ•£åŠ¨ä½œè½¬æ¢"""
    
class SB3CompatWrapper(gym.Wrapper):
    """ç¡®ä¿ä¸Stable-Baselines3å®Œå…¨å…¼å®¹"""
```

---

## ğŸ® åŠ¨ä½œç©ºé—´è®¾è®¡

### ç¦»æ•£åŠ¨ä½œæ¨¡å¼ (æ¨è)

```python
# åŠ¨ä½œç©ºé—´ï¼šDiscrete(15) 
# åŠ¨ä½œå«ä¹‰ï¼šé€‰æ‹©æŠ“å–å“ªä¸ªç‰©ä½“
action_space = gym.spaces.Discrete(15)

# åŠ¨ä½œæ‰§è¡Œæµç¨‹
def _discrete_step(self, action):
    """
    1. æ ¹æ®actionç´¢å¼•é€‰æ‹©ç›®æ ‡ç‰©ä½“
    2. æ‰§è¡Œ8çŠ¶æ€æŠ“å–æµç¨‹
    3. è¿”å›å¥–åŠ±å’Œå®ŒæˆçŠ¶æ€
    """
```

### è¿ç»­åŠ¨ä½œæ¨¡å¼

```python
# åŠ¨ä½œç©ºé—´ï¼šBox(7) - [ä½ç½®(3) + æ—‹è½¬(4)]
action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(7,))

# ç›´æ¥æ§åˆ¶æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€
def _continuous_step(self, action):
    """ç›´æ¥æ§åˆ¶TCPä½ç½®å’Œæ–¹å‘"""
```

---

## ğŸ“Š å¥–åŠ±ç³»ç»Ÿè¯¦è§£

### å¤šç»´åº¦å¥–åŠ±å‡½æ•°

```python
def compute_dense_reward(self):
    """å¯†é›†å¥–åŠ±è®¡ç®—"""
    
    # 1. æ¥è¿‘å¥–åŠ± - é¼“åŠ±æ¥è¿‘ç›®æ ‡ç‰©ä½“
    reaching_reward = -distance_to_target * self.config.reward.reaching_weight
    
    # 2. æŠ“å–å¥–åŠ± - æˆåŠŸæŠ“å–ç‰©ä½“
    grasping_reward = grasp_success * self.config.reward.grasping_weight
    
    # 3. æ”¾ç½®å¥–åŠ± - æˆåŠŸæ”¾ç½®åˆ°ç›®æ ‡ä½ç½®  
    placing_reward = place_success * self.config.reward.placing_weight
    
    # 4. æ‰°åŠ¨æƒ©ç½š - å‡å°‘å¯¹å…¶ä»–ç‰©ä½“çš„å¹²æ‰°
    displacement_penalty = -other_objects_moved * self.config.reward.displacement_weight
    
    # 5. æ—¶é—´æ•ˆç‡å¥–åŠ± - é¼“åŠ±å¿«é€Ÿå®Œæˆ
    time_reward = -episode_length * self.config.reward.time_weight
    
    # 6. é™æ­¢å¥–åŠ± - é¼“åŠ±ç¨³å®šæŠ“å–
    static_reward = robot_stillness * self.config.reward.static_weight
    
    # 7. æˆåŠŸå¥–åŠ± - ä»»åŠ¡å®Œæˆå¤§å¥–åŠ±
    success_reward = task_completed * self.config.reward.success_weight
    
    total_reward = (reaching_reward + grasping_reward + placing_reward + 
                   displacement_penalty + time_reward + static_reward + success_reward)
```

### ç¨€ç–å¥–åŠ±æ¨¡å¼

```python
def compute_sparse_reward(self):
    """ç¨€ç–å¥–åŠ± - åªåœ¨æˆåŠŸæ—¶ç»™äºˆå¥–åŠ±"""
    if self.check_success():
        return self.config.reward.sparse_success_reward
    else:
        return self.config.reward.sparse_displacement_weight * displacement_penalty
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. å¤šç¯å¢ƒå¹¶è¡Œè®­ç»ƒ

```python
# ç¯å¢ƒåˆ›å»º
env = gym.make(
    "EnvClutter-v1",
    num_envs=128,           # 128ä¸ªå¹¶è¡Œç¯å¢ƒ
    parallel_in_single_scene=True,  # å•åœºæ™¯å¹¶è¡Œä¼˜åŒ–
    sim_backend="gpu",      # GPUåŠ é€Ÿä»¿çœŸ
)

# æ‰¹é‡å¤„ç†æœºåˆ¶
def _discrete_step(self, actions):
    """å¤„ç†å¤šç¯å¢ƒçš„æ‰¹é‡åŠ¨ä½œ"""
    # actions: [env0_action, env1_action, ..., env127_action]
    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ç¯å¢ƒçš„åŠ¨ä½œ
```

### 2. æ™ºèƒ½ç‰©ä½“é€‰æ‹©

```python
def _update_remaining_objects(self):
    """åŠ¨æ€æ›´æ–°å¯æŠ“å–ç‰©ä½“åˆ—è¡¨"""
    valid_objects = []
    for obj_idx, obj in enumerate(self.all_objects):
        if self._is_object_graspable(obj):
            valid_objects.append(obj_idx)
    self.remaining_indices = valid_objects
```

### 3. ç‰©ç†çº¦æŸéªŒè¯

```python
def check_contact(self, target_object, env_idx):
    """æ£€æŸ¥TCPä¸ç‰©ä½“çš„æ¥è§¦çŠ¶æ€"""
    distance = self._compute_tcp_object_distance(target_object, env_idx)
    return distance < self.SUCTION_DISTANCE_THRESHOLD

def check_grasp_success(self, target_object, env_idx):
    """éªŒè¯æŠ“å–æ˜¯å¦æˆåŠŸ"""
    return self.check_contact(target_object, env_idx) and \
           self._is_object_lifted(target_object)
```

### 4. æ€§èƒ½ä¼˜åŒ–

```python
# GPUå†…å­˜ä¼˜åŒ–é…ç½®
def _configure_gpu_memory(self):
    """ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨"""
    sim_cfg = SimConfig()
    sim_cfg.gpu_memory_config = GPUMemoryConfig(
        max_rigid_contact_count=2**20,
        max_rigid_patch_count=2**18,
    )
    return sim_cfg
```

---

## ğŸ§ª å®éªŒé…ç½®

### åŸºç¡€å®éªŒé…ç½®

```python
# å°åœºæ™¯å¿«é€Ÿå®éªŒ
config = {
    "env": {
        "num_objects_per_type": 2,      # æ¯ç±»2ä¸ªç‰©ä½“
        "total_objects_per_env": 6,     # æ€»å…±6ä¸ªç‰©ä½“
        "max_episode_steps": 100,
    },
    "training": {
        "epochs": 300,
        "num_envs": 16,
    }
}
```

### é«˜è´¨é‡è®­ç»ƒé…ç½®

```python
# å¤§åœºæ™¯é«˜è´¨é‡è®­ç»ƒ
config = {
    "env": {
        "num_objects_per_type": 6,      # æ¯ç±»6ä¸ªç‰©ä½“
        "total_objects_per_env": 18,    # æ€»å…±18ä¸ªç‰©ä½“
        "max_episode_steps": 300,
    },
    "training": {
        "epochs": 2000,
        "num_envs": 128,
        "lr_actor": 1e-4,
        "lr_critic": 1e-4,
    },
    "model": {
        "actor_hidden_dims": [512, 512, 256],
        "critic_hidden_dims": [512, 512, 256],
    }
}
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### TensorBoardç›‘æ§

```bash
# å¯åŠ¨è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è®°å½•åˆ°./logs/
tensorboard --logdir ./logs --port 6006
```

ç›‘æ§æŒ‡æ ‡åŒ…æ‹¬ï¼š
- **è®­ç»ƒå¥–åŠ±**ï¼šepisode_reward, mean_reward
- **æˆåŠŸç‡**ï¼šsuccess_rate, grasp_success_rate  
- **æ•ˆç‡æŒ‡æ ‡**ï¼šepisode_length, time_efficiency
- **ç‰©ç†æŒ‡æ ‡**ï¼šdisplacement_penalty, constraint_violations

### CSVæ—¥å¿—

```python
# è‡ªåŠ¨ä¿å­˜è®­ç»ƒæ•°æ®åˆ°CSV
logger = CsvLogger('./logs/training_log.csv')
logger.log({
    'epoch': epoch,
    'mean_reward': mean_reward,
    'success_rate': success_rate,
    'episode_length': mean_episode_length
})
```

---

## ğŸ¬ å¯è§†åŒ–å’Œå½•åˆ¶

### è®­ç»ƒè¿‡ç¨‹å½•åˆ¶

```python
# å¯ç”¨è§†é¢‘å½•åˆ¶
env = RecordEpisode(
    env,
    output_dir="./videos",
    save_video=True,
    trajectory_name="training_demo",
    max_steps_per_video=200,
    video_fps=30,
)
```

### æ¨ç†æ¼”ç¤º

```python
# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†æ¼”ç¤º
python inference.py --model_path ./models/best_model.pth \
    --render --record_video --num_episodes 5
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¡¬ä»¶é…ç½®ä¼˜åŒ–

```python
# GPUå†…å­˜é…ç½®
sim_cfg.gpu_memory_config = GPUMemoryConfig(
    max_rigid_contact_count=2**20,    # æœ€å¤§åˆšä½“æ¥è§¦æ•°
    max_rigid_patch_count=2**18,      # æœ€å¤§åˆšä½“è¡¥ä¸æ•°
)

# å¤šæ ¸CPUä¼˜åŒ–
training_config.num_workers = 4      # æ•°æ®åŠ è½½å¹¶è¡Œåº¦
```

### 2. è®­ç»ƒç­–ç•¥ä¼˜åŒ–

```python
# æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
training_config.batch_size = 2048     # å¤§æ‰¹æ¬¡è®­ç»ƒ
training_config.n_steps = 256         # æ¯ç¯å¢ƒæ­¥æ•°

# å­¦ä¹ ç‡è°ƒåº¦
training_config.lr_schedule = "cosine_annealing"
```

### 3. å†…å­˜ä½¿ç”¨ä¼˜åŒ–

```python
# ç¯å¢ƒé…ç½®ä¼˜åŒ–
env_config.num_envs = 64              # æ ¹æ®GPUå†…å­˜è°ƒæ•´
env_config.max_episode_steps = 200    # æ§åˆ¶episodeé•¿åº¦
```

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç´¢å¼•è¶Šç•Œé”™è¯¯**
   ```python
   # é—®é¢˜ï¼šå¤šç¯å¢ƒç´¢å¼•ä¸åŒ¹é…
   # è§£å†³ï¼šä½¿ç”¨å®‰å…¨ç´¢å¼•è®¿é—®
   if env_idx < tensor.shape[0]:
       value = tensor[env_idx]
   else:
       value = tensor[0]  # å®‰å…¨å›é€€
   ```

2. **ç›¸å¯¹å¯¼å…¥é”™è¯¯**
   ```python
   # é—®é¢˜ï¼šImportError: attempted relative import
   # è§£å†³ï¼šä½¿ç”¨å…¼å®¹å¯¼å…¥
   try:
       from .config import Config
   except ImportError:
       from config import Config
   ```

3. **å†…å­˜ä¸è¶³**
   ```bash
   # è§£å†³ï¼šå‡å°‘å¹¶è¡Œç¯å¢ƒæ•°é‡
   python train_sb3.py --num_envs 32  # ä»128å‡å°‘åˆ°32
   ```

4. **è®­ç»ƒä¸æ”¶æ•›**
   ```python
   # è§£å†³ï¼šè°ƒæ•´å¥–åŠ±æƒé‡å’Œå­¦ä¹ ç‡
   reward_config.success_weight = 20.0     # å¢åŠ æˆåŠŸå¥–åŠ±
   training_config.lr_actor = 1e-4        # é™ä½å­¦ä¹ ç‡
   ```

---

## ğŸ“š æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰ç‰©ä½“

```python
# åœ¨config.pyä¸­æ·»åŠ æ–°çš„YCBç‰©ä½“
env_config.box_objects = [
    "004_sugar_box",
    "006_mustard_bottle", 
    "008_pudding_box",
    "YOUR_CUSTOM_OBJECT",  # æ·»åŠ æ–°ç‰©ä½“
]
```

### è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
# ç»§æ‰¿å¹¶é‡å†™å¥–åŠ±è®¡ç®—
class CustomEnvClutter(EnvClutterEnv):
    def compute_dense_reward(self):
        base_reward = super().compute_dense_reward()
        custom_reward = self._compute_custom_reward()
        return base_reward + custom_reward
```

### æ·»åŠ æ–°çš„æœºå™¨äºº

```python
# æ‰©å±•æ”¯æŒçš„æœºå™¨äººç±»å‹
SUPPORTED_ROBOTS = ["panda", "fetch", "your_robot"]

def _load_agent(self):
    if self.robot_uids == "your_robot":
        return YourCustomRobot(...)
```

---

## ğŸ“„ è®¸å¯è¯

æ­¤é¡¹ç›®éµå¾ª MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚è§ LICENSE æ–‡ä»¶ã€‚

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤é—®é¢˜æŠ¥å‘Šã€åŠŸèƒ½è¯·æ±‚å’ŒPull Requestï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. Forkå¹¶å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/RL_RobotArm-main.git

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-feature

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements_sb3.txt
pip install -e .

# 4. è¿è¡Œæµ‹è¯•
python test_quick_fix.py
```

---

## ğŸ“ æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹[æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)éƒ¨åˆ†
2. æ£€æŸ¥å·²æœ‰çš„Issue
3. åˆ›å»ºæ–°çš„Issueå¹¶æä¾›è¯¦ç»†ä¿¡æ¯

---

*ğŸ¯ è¿™ä¸ªREADMEæ–‡æ¡£è¯¦ç»†ä»‹ç»äº†V2-Suctioné¡¹ç›®çš„å®Œæ•´æ¶æ„ã€ä½¿ç”¨æ–¹æ³•å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚å¸Œæœ›èƒ½å¤Ÿå¸®åŠ©ä½ å¿«é€Ÿç†è§£å’Œä½¿ç”¨è¿™ä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼*
