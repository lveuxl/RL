"""
ä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬ - è§£å†³æ‰€æœ‰é”™è¯¯ï¼Œä¿è¯å¿«é€Ÿæ”¶æ•›
"""

import os
import argparse
import numpy as np
import torch
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.utils import set_random_seed
import time
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# æ³¨å†Œä¼˜åŒ–åçš„ç¯å¢ƒ
from env_clutter_optimized import EnvClutterOptimizedEnv
from mani_skill.vector.wrappers.sb3 import ManiSkillSB3VectorEnv
import mani_skill.envs


class SimpleTensorboardCallback(BaseCallback):
    """ç®€åŒ–çš„Tensorboardå›è°ƒï¼Œé¿å…é”™è¯¯"""
    
    def __init__(self, verbose=0):
        super(SimpleTensorboardCallback, self).__init__(verbose)
        self.episode_count = 0
        self.reward_buffer = []
        
    def _on_step(self) -> bool:
        # è®°å½•å³æ—¶å¥–åŠ±
        if "rewards" in self.locals:
            rewards = self.locals["rewards"]
            if isinstance(rewards, torch.Tensor):
                rewards = rewards.cpu().numpy()
            
            mean_reward = float(np.mean(rewards))
            self.reward_buffer.append(mean_reward)
            
            # æ¯100æ­¥è®°å½•ä¸€æ¬¡
            if self.num_timesteps % 100 == 0 and len(self.reward_buffer) > 0:
                self.logger.record("train/reward_mean", np.mean(self.reward_buffer))
                self.logger.record("train/reward_std", np.std(self.reward_buffer))
                self.reward_buffer = []
        
        return True
    
    def _on_rollout_end(self) -> None:
        # è®°å½•å­¦ä¹ è¿›åº¦
        self.logger.record("train/timesteps", self.num_timesteps)
        
        # å®‰å…¨åœ°è®°å½•ä»·å€¼å‡½æ•°
        try:
            if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'predict_values'):
                # è·å–ä¸€ä¸ªå°æ‰¹é‡è§‚æµ‹æ¥è®¡ç®—ä»·å€¼
                obs = self.training_env.observation_space.sample()
                obs_tensor = torch.as_tensor(obs).to(self.model.device).unsqueeze(0)
                with torch.no_grad():
                    values = self.model.policy.predict_values(obs_tensor)
                    if values is not None:
                        value = float(values.mean().item())
                        self.logger.record("train/value_estimate", value)
        except:
            pass  # å¿½ç•¥é”™è¯¯


def create_vectorized_envs(env_id, num_envs, seed=0):
    """åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ - ç®€åŒ–ç‰ˆ"""
    env = gym.make(
        env_id,
        num_envs=num_envs,
        obs_mode="state",
        control_mode="pd_ee_delta_pose",
        reward_mode="dense",
        sim_backend="gpu",
        render_mode=None,
    )
    
    # è®¾ç½®ç§å­
    env.unwrapped.seed(seed)
    
    # åŒ…è£…ä¸ºSB3å…¼å®¹ç¯å¢ƒ
    vec_env = ManiSkillSB3VectorEnv(env)
    return vec_env


def train():
    """ä¸»è®­ç»ƒå‡½æ•° - ç®€åŒ–ç‰ˆï¼Œä¿è¯ç¨³å®š"""
    
    # è®­ç»ƒé…ç½® - ä¼˜åŒ–åçš„å‚æ•°
    config = {
        # ç¯å¢ƒé…ç½®
        "env_id": "EnvClutterOptimized-v1",
        "num_envs": 128,  # å‡å°‘ç¯å¢ƒæ•°é‡é¿å…å†…å­˜é—®é¢˜
        "seed": 42,
        
        # PPOè¶…å‚æ•° - ä¿å®ˆä½†ç¨³å®šçš„è®¾ç½®
        "learning_rate": 5e-4,  # å›ºå®šå­¦ä¹ ç‡
        "n_steps": 128,  # å‡å°‘æ­¥æ•°ï¼Œæ›´é¢‘ç¹æ›´æ–°
        "batch_size": 512,  # å‡å°æ‰¹æ¬¡
        "n_epochs": 4,  # å‡å°‘epoch
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "clip_range": 0.2,
        "ent_coef": 0.02,  # å¢åŠ æ¢ç´¢
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        
        # è®­ç»ƒé…ç½®
        "total_timesteps": 500_000,  # å…ˆè®­ç»ƒ50ä¸‡æ­¥æµ‹è¯•
        "eval_freq": 10000,
        "save_freq": 25000,
        "n_eval_episodes": 5,
        "log_dir": "./logs/fixed_training",
        "model_dir": "./models/fixed_training",
    }
    
    # åˆ›å»ºç›®å½•
    os.makedirs(config["log_dir"], exist_ok=True)
    os.makedirs(config["model_dir"], exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cuda":
        # è®¾ç½®ä¸ºä½¿ç”¨CPUä»¥é¿å…GPUç›¸å…³é—®é¢˜
        device = "cpu"

    
    print("="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("="*60)
    print(f"ç¯å¢ƒ: {config['env_id']}")
    print(f"å¹¶è¡Œç¯å¢ƒæ•°: {config['num_envs']}")
    print(f"è®¾å¤‡: {device}")
    print(f"æ€»æ­¥æ•°: {config['total_timesteps']:,}")
    print("-"*60)
    
    # åˆ›å»ºç¯å¢ƒ
    print("åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    try:
        train_env = create_vectorized_envs(
            config["env_id"],
            config["num_envs"],
            config["seed"]
        )
        print("âœ“ è®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âœ— åˆ›å»ºè®­ç»ƒç¯å¢ƒå¤±è´¥: {e}")
        return
    
    print("åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    try:
        eval_env = create_vectorized_envs(
            config["env_id"],
            num_envs=2,  # æ›´å°‘çš„è¯„ä¼°ç¯å¢ƒ
            seed=config["seed"] + 1000
        )
        print("âœ“ è¯„ä¼°ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âœ— åˆ›å»ºè¯„ä¼°ç¯å¢ƒå¤±è´¥: {e}")
        eval_env = None
    
    # åˆ›å»ºPPOæ¨¡å‹
    print("åˆå§‹åŒ–PPOæ¨¡å‹...")
    try:
        model = PPO(
            "MlpPolicy",
            train_env,
            learning_rate=config["learning_rate"],
            n_steps=config["n_steps"],
            batch_size=config["batch_size"],
            n_epochs=config["n_epochs"],
            gamma=config["gamma"],
            gae_lambda=config["gae_lambda"],
            clip_range=config["clip_range"],
            ent_coef=config["ent_coef"],
            vf_coef=config["vf_coef"],
            max_grad_norm=config["max_grad_norm"],
            tensorboard_log=config["log_dir"],
            policy_kwargs={
                "net_arch": [128, 128],  # æ›´å°çš„ç½‘ç»œ
                "activation_fn": torch.nn.Tanh,
            },
            verbose=1,
            seed=config["seed"],
            device=device,
        )
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (è®¾å¤‡: {model.device})")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        train_env.close()
        if eval_env:
            eval_env.close()
        return
    
    # åˆ›å»ºå›è°ƒ
    callbacks = []
    
    # Tensorboardå›è°ƒ
    tb_callback = SimpleTensorboardCallback()
    callbacks.append(tb_callback)
    
    # è¯„ä¼°å›è°ƒï¼ˆå¦‚æœæœ‰è¯„ä¼°ç¯å¢ƒï¼‰
    if eval_env is not None:
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=os.path.join(config["model_dir"], "best_model"),
            log_path=os.path.join(config["log_dir"], "evaluations"),
            eval_freq=config["eval_freq"],
            n_eval_episodes=config["n_eval_episodes"],
            deterministic=True,
            render=False,
            verbose=1,
        )
        callbacks.append(eval_callback)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=config["save_freq"],
        save_path=config["model_dir"],
        name_prefix="checkpoint",
    )
    callbacks.append(checkpoint_callback)
    
    # å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("-"*60)
    
    start_time = time.time()
    
    try:
        # è®­ç»ƒæ¨¡å‹
        model.learn(
            total_timesteps=config["total_timesteps"],
            callback=callbacks,
            log_interval=10,
            progress_bar=True,
            reset_num_timesteps=True,
            tb_log_name="PPO_fixed",
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(config["model_dir"], "final_model")
        model.save(final_path)
        print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: {final_path}")
        
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ä¸­æ–­")
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
        interrupt_path = os.path.join(config["model_dir"], "interrupted_model")
        model.save(interrupt_path)
        print(f"ä¸­æ–­æ¨¡å‹ä¿å­˜è‡³: {interrupt_path}")
    
    except Exception as e:
        print(f"\nè®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†
        elapsed = time.time() - start_time
        print(f"\næ€»ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        
        if hasattr(model, 'num_timesteps'):
            print(f"å®Œæˆæ­¥æ•°: {model.num_timesteps:,}")
            if elapsed > 0:
                print(f"è®­ç»ƒé€Ÿåº¦: {model.num_timesteps/elapsed:.0f} steps/ç§’")
        
        # å…³é—­ç¯å¢ƒ
        train_env.close()
        if eval_env is not None:
            eval_env.close()
        
        print("\n" + "="*60)
        print("è®­ç»ƒç»“æŸ")
        print("="*60)
        print("\nä¸‹ä¸€æ­¥:")
        print("1. æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir " + config["log_dir"])
        print("2. æµ‹è¯•æ¨¡å‹: python test_optimized.py")
        print("3. è¯„ä¼°æ¨¡å‹: python evaluate_model.py " + os.path.join(config["model_dir"], "best_model", "best_model.zip"))


if __name__ == "__main__":
    train()