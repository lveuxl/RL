import os
from typing import Any, Dict, List, Union, Tuple, Optional
import numpy as np
import sapien
import torch
import cv2
import random
import sys

# å¯¼å…¥é…ç½®
try:
    from .config import Config, get_config
except ImportError:
    # å¤„ç†ç›´æ¥è¿è¡Œæ—¶çš„ç›¸å¯¹å¯¼å…¥é—®é¢˜
    from config import Config, get_config

# å¯¼å…¥AnyGraspç›¸å…³æ¨¡å—
try:
    # æ·»åŠ AnyGraspè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
    anygrasp_path = "/home/linux/jzh/RL_Robot/anygrasp_sdk/grasp_detection"
    if anygrasp_path not in sys.path:
        sys.path.insert(0, anygrasp_path)
    from gsnet import AnyGrasp
    from graspnetAPI import GraspGroup
    ANYGRASP_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: AnyGraspæœªèƒ½å¯¼å…¥ - {e}")
    print("å°†è·³è¿‡æŠ“å–ç‚¹æ£€æµ‹åŠŸèƒ½")
    ANYGRASP_AVAILABLE = False

import mani_skill.envs.utils.randomization as randomization
from mani_skill import ASSET_DIR
from mani_skill.agents.robots import Fetch, Panda
from mani_skill.envs.sapien_env import BaseEnv
from mani_skill.sensors.camera import CameraConfig
from mani_skill.utils import sapien_utils
from mani_skill.utils.building import actors
from mani_skill.utils.building.actor_builder import ActorBuilder
from mani_skill.utils.io_utils import load_json
from mani_skill.utils.registration import register_env
from mani_skill.utils.scene_builder.table import TableSceneBuilder
from mani_skill.utils.structs import Actor, Pose
from mani_skill.utils.structs.types import GPUMemoryConfig, SimConfig

# æ–°å¢ï¼šIKå’Œæ§åˆ¶å™¨ç›¸å…³å¯¼å…¥
# from mani_skill.agents.controllers.pd_ee_pose import PDEEPoseController

import sapien.physx as physx


@register_env(
    "EnvClutter-v1",
    asset_download_ids=["ycb"],
    max_episode_steps=200,
)
class EnvClutterEnv(BaseEnv):
    """
    **ä»»åŠ¡æè¿°:**
    å¤æ‚å †å æŠ“å–ç¯å¢ƒï¼ŒåŒ…å«å„ç§å½¢çŠ¶çš„YCBç‰©ä½“å †ç§¯åœ¨æ‰˜ç›˜ä¸­ã€‚
    æœºæ¢°è‡‚éœ€è¦æŒ‘é€‰æœ€é€‚åˆæŠ“å–çš„ç‰©ä½“ï¼Œå¹¶å°†å…¶æ”¾åˆ°æŒ‡å®šä½ç½®ã€‚
    
    **éšæœºåŒ–:**
    - ç‰©ä½“åœ¨æ‰˜ç›˜å†…éšæœºç”Ÿæˆ
    - ç‰©ä½“åˆå§‹å§¿æ€éšæœºåŒ–
    - ç›®æ ‡ä½ç½®å›ºå®šåœ¨æ‰˜ç›˜å³ä¾§
    
    **æˆåŠŸæ¡ä»¶:**
    - ç›®æ ‡ç‰©ä½“è¢«æˆåŠŸæŠ“å–
    """
    
    SUPPORTED_REWARD_MODES = ["dense", "sparse"]
    SUPPORTED_ROBOTS = ["panda", "fetch"]
    agent: Union[Panda, Fetch]
    
    # æ‰˜ç›˜å‚æ•° (åŸºäºtraybox.urdfçš„å°ºå¯¸)
    tray_size = [0.6, 0.6, 0.15]  # æ‰˜ç›˜å†…éƒ¨å°ºå¯¸ (é•¿xå®½xé«˜)
    tray_spawn_area = [0.23, 0.23]  # æ‰˜ç›˜å†…ç‰©ä½“ç”ŸæˆåŒºåŸŸ (è€ƒè™‘è¾¹ç•Œå¢™å’Œå®‰å…¨è¾¹è·)
    
    # æ³¨æ„ï¼šç‰©ä½“ç›¸å…³å‚æ•°ç°åœ¨ä»configä¸­åŠ¨æ€è·å–
    # BOX_OBJECTS, num_objects_per_type, MAX_N, MAX_EPISODE_STEPS ç­‰
    # éƒ½åœ¨ __init__ æ–¹æ³•ä¸­ä»é…ç½®æ–‡ä»¶åˆå§‹åŒ–
    
    # æ–°å¢ï¼šå¸ç›˜çº¦æŸç›¸å…³å¸¸é‡
    SUCTION_DISTANCE_THRESHOLD = 0.1  # å¸ç›˜æ¿€æ´»è·ç¦»é˜ˆå€¼ 
    SUCTION_STIFFNESS = 1e6  # å¸ç›˜çº¦æŸåˆšåº¦
    SUCTION_DAMPING = 1e4    # å¸ç›˜çº¦æŸé˜»å°¼
    
    # AnyGraspç›¸å…³é…ç½® - ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹demoçš„é«˜è´¨é‡å‚æ•°
    ANYGRASP_CHECKPOINT = "/home/linux/jzh/RL_Robot/anygrasp_sdk/grasp_detection/log/checkpoint_detection.tar"  # æ¨¡å‹æƒé‡è·¯å¾„
    ANYGRASP_MAX_GRIPPER_WIDTH = 0.1   # ğŸ”§ ä¿®å¤ï¼šå¢åŠ åˆ°10cmï¼Œä¸å®˜æ–¹demoä¸€è‡´
    ANYGRASP_GRIPPER_HEIGHT = 0.03     # ğŸ”§ ä¿®å¤ï¼šå‡å°‘åˆ°3cmï¼Œä¸å®˜æ–¹demoä¸€è‡´
    ANYGRASP_TOP_DOWN_GRASP = True     # æ˜¯å¦ä¼˜å…ˆé¡¶éƒ¨æŠ“å–
    
    def __init__(
        self,
        *args,
        robot_uids="panda",
        robot_init_qpos_noise=0.02,
        num_envs=1,
        use_discrete_action=False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ç¦»æ•£åŠ¨ä½œ
        use_ideal_oracle=True,      # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ç†æƒ³åŒ–ç¥è°•æŠ“å–
        config_preset="default",    # æ–°å¢ï¼šé…ç½®é¢„è®¾åç§°
        custom_config=None,         # æ–°å¢ï¼šè‡ªå®šä¹‰é…ç½®å¯¹è±¡
        **kwargs,
    ):
        self.robot_init_qpos_noise = robot_init_qpos_noise
        self.use_discrete_action = use_discrete_action
        self.use_ideal_oracle = use_ideal_oracle
        
        # åˆå§‹åŒ–é…ç½®
        if custom_config is not None:
            self.config = custom_config
        else:
            self.config = get_config(config_preset)
        
        # ä»é…ç½®ä¸­è·å–ç‰©ä½“ç›¸å…³å‚æ•°
        self.BOX_OBJECTS = self.config.env.box_objects
        self.num_objects_per_type = self.config.env.num_objects_per_type
        self.num_object_types = self.config.env.num_object_types
        self.total_objects_per_env = self.config.env.total_objects_per_env
        self.goal_thresh = self.config.env.goal_thresh  # æˆåŠŸé˜ˆå€¼
        
        # è®¾ç½®åŠ¨æ€è®¡ç®—çš„å±æ€§
        self.MAX_N = self.total_objects_per_env  # æœ€å¤§ç‰©ä½“æ•°é‡
        self.MAX_EPISODE_STEPS = self.config.env.max_episode_steps_discrete  # æœ€å¤§episodeæ­¥æ•°
        
        # åˆå§‹åŒ–ç¦»æ•£åŠ¨ä½œç›¸å…³å˜é‡ - ä¿®æ”¹ä¸ºå¤šç¯å¢ƒæ”¯æŒ
        self.remaining_indices = []  # æ¯ä¸ªç¯å¢ƒçš„å‰©ä½™å¯æŠ“å–ç‰©ä½“ç´¢å¼• [[env0_indices], [env1_indices], ...]
        self.step_count = []  # æ¯ä¸ªç¯å¢ƒçš„å½“å‰æ­¥æ•° [env0_steps, env1_steps, ...]
        self.grasped_objects = []  # æ¯ä¸ªç¯å¢ƒå·²æŠ“å–çš„ç‰©ä½“ [[env0_grasped], [env1_grasped], ...]
        
        # æ–°å¢ï¼šå¹¶è¡Œæœ‰é™çŠ¶æ€æœºå˜é‡
        self.env_stage = None      # [num_envs] å½“å‰æ‰€å¤„çŠ¶æ€ 0~7
        self.env_target = None     # [num_envs] æ¯ä¸ªç¯å¢ƒæ­£åœ¨å¤„ç†çš„ç‰©ä½“ç´¢å¼•
        self.env_busy = None       # [num_envs] True=æµç¨‹è¿›è¡Œä¸­ï¼ŒFalse=æœ¬å›åˆå·²ç»“æŸæˆ–ç­‰å¾…æ–°æŒ‡ä»¤
        self.stage_tick = None     # [num_envs] åœ¨æŸçŠ¶æ€ä¸­å·²ç»èµ°äº†å¤šå°‘å¾®æ­¥
        self.stage_positions = None # [num_envs, 3] æ¯ä¸ªç¯å¢ƒå½“å‰çŠ¶æ€çš„ç›®æ ‡ä½ç½®
        
        # æ–°å¢ï¼šåˆå§‹åŒ–å¸ç›˜çº¦æŸç›¸å…³å˜é‡
        self.suction_constraints = {}  # å­˜å‚¨çº¦æŸå¯¹è±¡çš„å­—å…¸ {object_name: constraint}
        self.is_suction_active = [False] * num_envs  # æ¯ä¸ªç¯å¢ƒçš„å¸ç›˜æ¿€æ´»çŠ¶æ€
        self.current_suction_object = [None] * num_envs  # æ¯ä¸ªç¯å¢ƒå½“å‰å¸é™„çš„ç‰©ä½“
        
        # æ–°å¢ï¼šç†æƒ³åŒ–ç¥è°•æŠ“å–ç›¸å…³å˜é‡
        self.oracle_attached_object = [None] * num_envs  # æ¯ä¸ªç¯å¢ƒå½“å‰ç†æƒ³åŒ–æŠ“å–çš„ç‰©ä½“
        # ç†æƒ³æŠ“å–å§¿æ€å­—å…¸ï¼šç‰©ä½“ç±»åˆ« -> å±€éƒ¨åæ ‡ç³»ä¸‹çš„ç†æƒ³æŠ“å–å§¿æ€
        self.ideal_grasp_poses_local = {
            # YCBå¸¸è§ç‰©ä½“çš„ç†æƒ³æŠ“å–å§¿æ€ï¼ˆå±€éƒ¨åæ ‡ç³»ï¼‰
            '003_cracker_box': sapien.Pose(p=[0, 0, 0.05], q=[1, 0, 0, 0]),  # é¡¶æŠ“ï¼Œä¸Šæ–¹5cm
            '004_sugar_box': sapien.Pose(p=[0, 0, 0.06], q=[1, 0, 0, 0]),   # é¡¶æŠ“ï¼Œä¸Šæ–¹6cm  
            '005_tomato_soup_can': sapien.Pose(p=[0, 0, 0.04], q=[1, 0, 0, 0]),  # é¡¶æŠ“ï¼Œä¸Šæ–¹4cm
            '006_mustard_bottle': sapien.Pose(p=[0, 0, 0.05], q=[1, 0, 0, 0]),   # é¡¶æŠ“ï¼Œä¸Šæ–¹5cm
            '009_gelatin_box': sapien.Pose(p=[0, 0, 0.04], q=[1, 0, 0, 0]),      # é¡¶æŠ“ï¼Œä¸Šæ–¹4cm
            'default': sapien.Pose(p=[0, 0, 0.05], q=[1, 0, 0, 0])  # é»˜è®¤ï¼šé¡¶æŠ“ï¼Œæœä¸‹ï¼Œä¸Šæ–¹5cm
        }
        
        # åˆå§‹åŒ–AnyGraspï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        self.anygrasp_model = None
        self.anygrasp_enabled = ANYGRASP_AVAILABLE
        if self.anygrasp_enabled:
            self._init_anygrasp()
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°æ­£ç¡®ä¼ é€’ç»™çˆ¶ç±»
        super().__init__(
            *args,
            robot_uids=robot_uids,
            num_envs=num_envs,
            **kwargs,
        )
        
        # åœ¨çˆ¶ç±»åˆå§‹åŒ–ååˆå§‹åŒ–FSMçŠ¶æ€å¼ é‡
        if self.use_discrete_action:
            self._init_fsm_states()

    @property
    def _default_sim_config(self):
        return SimConfig(
            gpu_memory_config=GPUMemoryConfig(
                max_rigid_contact_count=2**21,
                max_rigid_patch_count=2**19
            )
        )

    @property
    def _default_sensor_configs(self):
        # ğŸ”§ å¹³è¡¡ç›¸æœºä½ç½®ï¼šæ—¢è¦è·å¾—åˆç†çš„æ·±åº¦å€¼ï¼Œåˆè¦ä¿è¯AnyGraspèƒ½æ£€æµ‹åˆ°æŠ“å–
        # æ‰˜ç›˜ä½ç½®åœ¨ [-0.2, 0.0, 0.006]ï¼Œç‰©ä½“åœ¨z=0.04-0.1å·¦å³
        # ä½¿ç”¨é€‚ä¸­çš„ç›¸æœºé«˜åº¦å’Œè·ç¦»ï¼Œæ–œå‘ä¿¯è§†è§’åº¦æœ‰åˆ©äºæŠ“å–æ£€æµ‹
        pose = sapien_utils.look_at(eye=[0.0, 0, 0.35], target=[-0.2, 0.0, 0.05])
        return [
            CameraConfig(
                "base_camera",
                pose=pose,
                width=640,   # ğŸ”§ ä¿®å¤ï¼šæé«˜åˆ†è¾¨ç‡åˆ°640x480ï¼Œæ¥è¿‘å®˜æ–¹demo
                height=480,
                fov=np.pi / 2,
                near=0.01,
                far=100,
            )
        ]

    @property
    def _default_human_render_camera_configs(self):
        pose = sapien_utils.look_at([0.6, 0.7, 0.6], [0.0, 0.2, 0.35])
        return CameraConfig(
            "render_camera", 
            pose=pose, 
            width=512, 
            height=512, 
            fov=1, 
            near=0.01, 
            far=100
        )

    def _load_agent(self, options: dict):
        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))
        
    # å¸ç›˜çº¦æŸç³»ç»Ÿå®ç°
    def _create_suction_constraint(self, target_object: Actor, env_idx: int = 0) -> bool:
        """
        åˆ›å»ºå¸ç›˜çº¦æŸ
        
        Args:
            target_object: ç›®æ ‡ç‰©ä½“
            env_idx: ç¯å¢ƒç´¢å¼•ï¼ˆå¤šç¯å¢ƒæ”¯æŒï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ›å»ºçº¦æŸ
        """
        if self.is_suction_active[env_idx]:
            print(f"ç¯å¢ƒ{env_idx}: å¸ç›˜å·²ç»æ¿€æ´»ï¼Œæ— æ³•åˆ›å»ºæ–°çº¦æŸ")
            return False
            
        # æ£€æŸ¥æ˜¯å¦ä¸ç‰©ä½“æ¥è§¦
        if not self._is_contacting_object(target_object, self.SUCTION_DISTANCE_THRESHOLD, env_idx):
            print(f"ç¯å¢ƒ{env_idx}: ç‰©ä½“è·ç¦»è¿‡è¿œï¼Œæ— æ³•æ¿€æ´»å¸ç›˜")
            return False
        
        try:
            # å¯¼å…¥Driveç±»
            from mani_skill.utils.structs.drive import Drive
            
            print(f"ç¯å¢ƒ{env_idx}: åˆ›å»ºå¸ç›˜çº¦æŸ: TCPé“¾æ¥ -> ç‰©ä½“ {target_object.name}")
            
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨çš„å¤šç¯å¢ƒå¯¹è±¡é€‰æ‹©
            # 1. éªŒè¯ç›®æ ‡ç‰©ä½“çš„ç¯å¢ƒå½’å±
            target_scene_idxs = target_object._scene_idxs
            if len(target_scene_idxs) == 0:
                print(f"ç¯å¢ƒ{env_idx}: ç›®æ ‡ç‰©ä½“æ²¡æœ‰åœºæ™¯ç´¢å¼•")
                return False
            
            target_env_idx = target_scene_idxs[0].item()
            print(f"ç¯å¢ƒ{env_idx}: ç›®æ ‡ç‰©ä½“å®é™…å±äºç¯å¢ƒ{target_env_idx}")
            
            # éªŒè¯ç¯å¢ƒç´¢å¼•ä¸€è‡´æ€§
            if target_env_idx != env_idx:
                print(f"ç¯å¢ƒ{env_idx}: ç¯å¢ƒç´¢å¼•ä¸åŒ¹é…ï¼Œç›®æ ‡ç‰©ä½“å±äºç¯å¢ƒ{target_env_idx}")
                return False
            
            # 2. ğŸ”§ ä¿®å¤ï¼šé€šè¿‡scene_idxså®‰å…¨è·å–TCPå®ä½“
            tcp_objs = self.agent.tcp._objs
            tcp_scene_idxs = self.agent.tcp._scene_idxs
            
            # æ‰¾åˆ°å±äºtarget_env_idxç¯å¢ƒçš„TCPå¯¹è±¡
            tcp_mask = (tcp_scene_idxs == target_env_idx)
            if not tcp_mask.any():
                print(f"ç¯å¢ƒ{env_idx}: æ‰¾ä¸åˆ°å¯¹åº”ç¯å¢ƒçš„TCPå¯¹è±¡")
                return False
            
            tcp_indices = torch.where(tcp_mask)[0]
            if len(tcp_indices) == 0:
                print(f"ç¯å¢ƒ{env_idx}: TCPç´¢å¼•åˆ—è¡¨ä¸ºç©º")
                return False
                
            tcp_idx = tcp_indices[0].item()  # è·å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
            tcp_entity = tcp_objs[tcp_idx].entity
            print(f"ç¯å¢ƒ{env_idx}: æ‰¾åˆ°TCPå¯¹è±¡ï¼Œç´¢å¼•={tcp_idx}ï¼Œç¯å¢ƒ={target_env_idx}")
            
            # 3. ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è·å–ç›®æ ‡ç‰©ä½“å®ä½“
            if len(target_object._objs) == 0:
                print(f"ç¯å¢ƒ{env_idx}: ç›®æ ‡ç‰©ä½“æ²¡æœ‰å®ä½“å¯¹è±¡")
                return False
            
            # åœ¨å½“å‰è®¾è®¡ä¸­ï¼Œæ¯ä¸ªç‰©ä½“é€šå¸¸åªæœ‰ä¸€ä¸ªå®ä½“
            target_entity = target_object._objs[0]
            print(f"ç¯å¢ƒ{env_idx}: ç›®æ ‡ç‰©ä½“å®ä½“æ•°é‡={len(target_object._objs)}")
            
            print(f"ç¯å¢ƒ{env_idx}: ä½¿ç”¨TCPå®ä½“[ç´¢å¼•{tcp_idx}]å’Œç›®æ ‡å®ä½“åˆ›å»ºçº¦æŸ")
            
            # å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨SAPIENçš„create_driveæ–¹æ³•ï¼Œç»•è¿‡DriveåŒ…è£…å™¨çš„æ‰¹é‡å¤„ç†
            # è¿™æ ·å¯ä»¥é¿å…scene_idxså’Œbodiesç´¢å¼•ä¸åŒ¹é…çš„é—®é¢˜
            sub_scene = self.scene.sub_scenes[target_env_idx]
            physx_drive = sub_scene.create_drive(
                tcp_entity,           # TCPå®ä½“
                sapien.Pose(),        # çˆ¶ä½“æœ¬åœ°å§¿æ€ - ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨sapien.Pose()
                target_entity,        # ç›®æ ‡ç‰©ä½“å®ä½“
                sapien.Pose()         # å­ä½“æœ¬åœ°å§¿æ€ - ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨sapien.Pose()
            )

            # æ‰‹åŠ¨åˆ›å»ºDriveåŒ…è£…å™¨ä»¥ä¾¿åç»­ç®¡ç†
            constraint = Drive(
                _objs=[physx_drive],
                _scene_idxs=torch.tensor([target_env_idx], device=self.device),
                pose_in_child=sapien.Pose(),
                pose_in_parent=sapien.Pose(),
                scene=self.scene
            )
            
            # è®¾ç½®çº¦æŸå‚æ•°ä½¿å…¶è¡¨ç°ä¸ºå›ºå®šçº¦æŸï¼ˆç±»ä¼¼PyBulletçš„JOINT_FIXEDï¼‰
            # ç›´æ¥è°ƒç”¨åº•å±‚PhysxDriveComponentçš„æ–¹æ³•ï¼ˆè¿™äº›æ–¹æ³•æ²¡æœ‰@before_gpu_inité™åˆ¶ï¼‰
            try:
                # çº¿æ€§çº¦æŸï¼ˆX, Y, Zæ–¹å‘ï¼‰
                physx_drive.set_drive_property_x(stiffness=self.SUCTION_STIFFNESS, damping=self.SUCTION_DAMPING)
                physx_drive.set_drive_property_y(stiffness=self.SUCTION_STIFFNESS, damping=self.SUCTION_DAMPING)
                physx_drive.set_drive_property_z(stiffness=self.SUCTION_STIFFNESS, damping=self.SUCTION_DAMPING)
                print(f"ç¯å¢ƒ{env_idx}: âœ… å·²è®¾ç½®é©±åŠ¨å±æ€§")
            except Exception as drive_error:
                print(f"ç¯å¢ƒ{env_idx}: âŒ è®¾ç½®é©±åŠ¨å±æ€§å¤±è´¥: {drive_error}")
                return False
            
            # è®¾ç½®ä½ç½®é™åˆ¶æ¥æ¨¡æ‹Ÿå›ºå®šçº¦æŸ
            try:
                physx_drive.set_limit_x(0, 0)  # ä¸å…è®¸Xæ–¹å‘ç§»åŠ¨
                physx_drive.set_limit_y(0, 0)  # ä¸å…è®¸Yæ–¹å‘ç§»åŠ¨
                physx_drive.set_limit_z(0, 0)  # ä¸å…è®¸Zæ–¹å‘ç§»åŠ¨
                print(f"ç¯å¢ƒ{env_idx}: âœ… å·²è®¾ç½®ä½ç½®é™åˆ¶")
            except Exception as limit_error:
                print(f"ç¯å¢ƒ{env_idx}: âš ï¸ è®¾ç½®é™åˆ¶å¤±è´¥: {limit_error}")
                # ç»§ç»­æ‰§è¡Œï¼Œä»…ä½¿ç”¨é©±åŠ¨å±æ€§
            
            # å­˜å‚¨çº¦æŸ - ä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„é”®
            constraint_key = f"{target_object.name}_env_{env_idx}"
            self.suction_constraints[constraint_key] = constraint
            self.is_suction_active[env_idx] = True
            self.current_suction_object[env_idx] = target_object
            
            print(f"ç¯å¢ƒ{env_idx}: âœ… å¸ç›˜çº¦æŸåˆ›å»ºæˆåŠŸ: {constraint_key}")
            return True
            
        except Exception as e:
            print(f"ç¯å¢ƒ{env_idx}: âŒ åˆ›å»ºå¸ç›˜çº¦æŸå¤±è´¥: {e}")
            import traceback
            print(f"ç¯å¢ƒ{env_idx}: è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            return False

    def _remove_suction_constraint(self, env_idx: int = 0) -> bool:
        """
        ç§»é™¤å¸ç›˜çº¦æŸ
        
        Args:
            env_idx: ç¯å¢ƒç´¢å¼•ï¼ˆå¤šç¯å¢ƒæ”¯æŒï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤çº¦æŸ
        """
        if not self.is_suction_active[env_idx] or self.current_suction_object[env_idx] is None:
            #print("æ²¡æœ‰æ¿€æ´»çš„å¸ç›˜çº¦æŸéœ€è¦ç§»é™¤")
            return False
        
        try:
            # è·å–çº¦æŸå¯¹è±¡ - ä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„é”®
            constraint_key = f"{self.current_suction_object[env_idx].name}_env_{env_idx}"
            if constraint_key in self.suction_constraints:
                constraint = self.suction_constraints[constraint_key]
                
                print(f"ç¯å¢ƒ{env_idx}: æ­£åœ¨ç§»é™¤å¸ç›˜çº¦æŸ: {constraint_key}")
                
                # å…³é”®ä¿®å¤ï¼šç›´æ¥æ“ä½œåº•å±‚PhysxDriveComponentå¯¹è±¡
                physx_drive = constraint._objs[0]  # è·å–åº•å±‚çš„PhysxDriveComponentå¯¹è±¡
                
                # æ–¹æ³•1: é€šè¿‡è®¾ç½®åˆšåº¦ä¸º0æ¥ç¦ç”¨çº¦æŸï¼ˆæœ€æœ‰æ•ˆï¼‰
                try:
                    print(f"ç¯å¢ƒ{env_idx}: è®¾ç½®çº¦æŸåˆšåº¦ä¸º0...")
                    physx_drive.set_drive_property_x(stiffness=0.0, damping=0.0)
                    physx_drive.set_drive_property_y(stiffness=0.0, damping=0.0)
                    physx_drive.set_drive_property_z(stiffness=0.0, damping=0.0)
                    print(f"ç¯å¢ƒ{env_idx}: âœ… æˆåŠŸç¦ç”¨çº¦æŸé©±åŠ¨å±æ€§")
                except Exception as disable_error:
                    print(f"ç¯å¢ƒ{env_idx}: âŒ ç¦ç”¨çº¦æŸé©±åŠ¨å±æ€§å¤±è´¥: {disable_error}")
                    return False
                
                # æ–¹æ³•2: é‡ç½®çº¦æŸé™åˆ¶ï¼ˆè¾…åŠ©æ–¹æ³•ï¼‰
                try:
                    print(f"ç¯å¢ƒ{env_idx}: é‡ç½®çº¦æŸé™åˆ¶...")
                    # è®¾ç½®éå¸¸å¤§çš„é™åˆ¶èŒƒå›´ï¼Œç›¸å½“äºå–æ¶ˆé™åˆ¶
                    physx_drive.set_limit_x(-1000, 1000)
                    physx_drive.set_limit_y(-1000, 1000)
                    physx_drive.set_limit_z(-1000, 1000)
                    print(f"ç¯å¢ƒ{env_idx}: âœ… æˆåŠŸé‡ç½®çº¦æŸé™åˆ¶")
                except Exception as limit_error:
                    print(f"ç¯å¢ƒ{env_idx}: âš ï¸ é‡ç½®çº¦æŸé™åˆ¶å¤±è´¥: {limit_error}")
                    # é™åˆ¶é‡ç½®å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼Œç»§ç»­æ‰§è¡Œ
                    pass
                
                # æ¸…ç†çº¦æŸå¼•ç”¨
                del self.suction_constraints[constraint_key]
                print(f"ç¯å¢ƒ{env_idx}: âœ… çº¦æŸå¼•ç”¨å·²æ¸…ç†: {constraint_key}")
            else:
                print(f"ç¯å¢ƒ{env_idx}: âš ï¸ æœªæ‰¾åˆ°çº¦æŸå¯¹è±¡: {constraint_key}")
                pass
            
            # åˆ é™¤è¢«å¸å–çš„ç‰©ä½“
            try:
                if self.current_suction_object[env_idx] is not None:
                    target_obj = self.current_suction_object[env_idx]
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºGPUä»¿çœŸ
                    if self.scene.gpu_sim_enabled:
                        # GPUä»¿çœŸï¼šå°†ç‰©ä½“ç§»åŠ¨åˆ°è¿œå¤„ï¼ˆæ¨¡æ‹Ÿåˆ é™¤æ•ˆæœï¼‰
                        target_obj.set_pose(Pose.create_from_pq(p=[100.0, 100.0, 100.0]))
                        print(f"ç¯å¢ƒ{env_idx}: âœ… GPUä»¿çœŸæ¨¡å¼ - å°†ç‰©ä½“{target_obj.name}ç§»åŠ¨åˆ°è¿œå¤„")
                    else:
                        # CPUä»¿çœŸï¼šç‰©ç†åˆ é™¤ç‰©ä½“
                        target_obj.remove_from_scene()
                        print(f"ç¯å¢ƒ{env_idx}: âœ… CPUä»¿çœŸæ¨¡å¼ - å·²ä»åœºæ™¯ä¸­åˆ é™¤ç‰©ä½“{target_obj.name}")
                        
            except Exception as remove_error:
                print(f"ç¯å¢ƒ{env_idx}: âš ï¸ åˆ é™¤ç‰©ä½“å¤±è´¥: {remove_error}")
                # åˆ é™¤å¤±è´¥ä¸å½±å“ä¸»è¦æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
                pass
            
            # é‡ç½®å¸ç›˜çŠ¶æ€
            self.is_suction_active[env_idx] = False
            self.current_suction_object[env_idx] = None
            
            print(f"ç¯å¢ƒ{env_idx}: âœ… å¸ç›˜çŠ¶æ€å·²é‡ç½®")
            
            return True
            
        except Exception as e:
            print(f"ç¯å¢ƒ{env_idx}: âŒ ç§»é™¤å¸ç›˜çº¦æŸå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # å³ä½¿ç§»é™¤å¤±è´¥ï¼Œä¹Ÿè¦é‡ç½®çŠ¶æ€
            self.is_suction_active[env_idx] = False
            self.current_suction_object[env_idx] = None
            return False

    def _is_contacting_object(self, target_object: Actor, threshold: float = 0.05, env_idx: int = 0) -> bool:
        """
        æ£€æµ‹TCPæ˜¯å¦ä¸ç‰©ä½“æ¥è§¦
        
        Args:
            target_object: ç›®æ ‡ç‰©ä½“
            threshold: è·ç¦»é˜ˆå€¼
            env_idx: ç¯å¢ƒç´¢å¼•ï¼ˆå¤šç¯å¢ƒæ”¯æŒï¼‰
            
        Returns:
            bool: æ˜¯å¦æ¥è§¦
        """
        try:
            # è®¡ç®—TCPåˆ°ç‰©ä½“çš„è·ç¦» - ä½¿ç”¨å¯¹åº”ç¯å¢ƒçš„TCPä½ç½®
            tcp_pos = self.agent.tcp.pose.p
            if tcp_pos.dim() > 1:
                if env_idx < tcp_pos.shape[0]:
                    tcp_pos = tcp_pos[env_idx]
                else:
                    tcp_pos = tcp_pos[0]
                    print(f"âš ï¸ ç¯å¢ƒ{env_idx}: TCPä½ç½®ç´¢å¼•è¶Šç•Œï¼Œä½¿ç”¨ç¯å¢ƒ0çš„ä½ç½®")
            
            # æ­£ç¡®è·å–å¤šç¯å¢ƒä¸‹çš„ç‰©ä½“ä½ç½®
            obj_pos = target_object.pose.p
            obj_pos = obj_pos[0]
            
            # è®¡ç®—è·ç¦»
            raw_distance = torch.linalg.norm(tcp_pos - obj_pos).item()
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´åˆç†çš„åŠå¾„ä¼°è®¡å€¼
            # TCPåŠå¾„çº¦2cmï¼Œç‰©ä½“å¹³å‡åŠå¾„çº¦3cmï¼Œæ€»è®¡çº¦5cm
            estimated_radius = 0.05  # 5cmçš„åŠå¾„ä¼°è®¡ï¼Œä¸_check_suction_grasp_successä¿æŒä¸€è‡´
            distance = raw_distance - estimated_radius
            
            print(f"ç¯å¢ƒ{env_idx}: TCPåˆ°ç‰©ä½“è·ç¦»æ£€æµ‹: åŸå§‹è·ç¦»={raw_distance:.4f}m, è°ƒæ•´åè·ç¦»={distance:.4f}m, é˜ˆå€¼={threshold:.4f}m, æ¥è§¦={'æ˜¯' if distance <= threshold else 'å¦'}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ¥è§¦é˜ˆå€¼å†…
            return distance <= threshold
            
        except Exception as e:
            print(f"ç¯å¢ƒ{env_idx}: æ£€æµ‹æ¥è§¦å¤±è´¥: {e}")
            return False

    def _check_suction_grasp_success(self, target_object: Actor, env_idx: int = 0) -> bool:
        """
        æ£€æŸ¥å¸ç›˜æŠ“å–æ˜¯å¦æˆåŠŸ
        
        Args:
            target_object: ç›®æ ‡ç‰©ä½“
            env_idx: ç¯å¢ƒç´¢å¼•ï¼ˆå¤šç¯å¢ƒæ”¯æŒï¼‰
            
        Returns:
            bool: æŠ“å–æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ–¹æ³•1ï¼šæ£€æŸ¥å¸ç›˜çŠ¶æ€
            if (self.is_suction_active[env_idx] and 
                self.current_suction_object[env_idx] is not None and 
                self.current_suction_object[env_idx].name == target_object.name):
                
                # æ–¹æ³•2ï¼šæ£€æŸ¥ç‰©ä½“æ˜¯å¦ä»åœ¨TCPé™„è¿‘
                tcp_pos = self.agent.tcp.pose.p
                if tcp_pos.dim() > 1:
                    if env_idx < tcp_pos.shape[0]:
                        tcp_pos = tcp_pos[env_idx]
                    else:
                        tcp_pos = tcp_pos[0]
                        print(f"âš ï¸ ç¯å¢ƒ{env_idx}: TCPä½ç½®ç´¢å¼•è¶Šç•Œï¼Œä½¿ç”¨ç¯å¢ƒ0çš„ä½ç½®")
                
                obj_pos = target_object.pose.p
                obj_pos = obj_pos[0]
                
                raw_distance = torch.linalg.norm(tcp_pos - obj_pos).item()
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸æ¥è§¦æ£€æµ‹ä¸€è‡´çš„åŠå¾„ä¼°è®¡
                estimated_radius = 0.05  # 5cmçš„åŠå¾„ä¼°è®¡ï¼Œä¸_is_contacting_objectä¿æŒä¸€è‡´
                distance = raw_distance - estimated_radius
                
                # è·ç¦»å°äº5cmè®¤ä¸ºæŠ“å–æˆåŠŸ
                success_threshold = 0.05
                success = distance < success_threshold
                
                print(f"ç¯å¢ƒ{env_idx}: æŠ“å–æˆåŠŸæ£€æµ‹ - åŸå§‹è·ç¦»={raw_distance:.4f}m, è°ƒæ•´åè·ç¦»={distance:.4f}m, æˆåŠŸ={'æ˜¯' if success else 'å¦'}")
                
                return success
            else:
                print(f"ç¯å¢ƒ{env_idx}: å¸ç›˜æœªæ¿€æ´»æˆ–ç‰©ä½“ä¸åŒ¹é…")
                return False
                
        except Exception as e:
            print(f"ç¯å¢ƒ{env_idx}: æ£€æŸ¥å¸ç›˜æŠ“å–æˆåŠŸå¤±è´¥: {e}")
            return False
    
    
    
    def _low_level_step(self, delta_pose: torch.Tensor):
        """å•æ­¥æ‰§è¡Œdelta poseï¼Œåªæ¨è¿›ä»¿çœŸï¼Œä¸èµ°ç¦»æ•£é€»è¾‘"""
        # è°ƒç”¨çˆ¶ç±»çš„stepæ–¹æ³•æ‰§è¡Œè¿ç»­åŠ¨ä½œ
        super().step(delta_pose)
    
    
    def _is_object_blocked(self, target_obj) -> bool:
        """
        ç®€åŒ–çš„é®æŒ¡æ£€æµ‹ï¼Œå¯¹åº”PyBulletçš„å°„çº¿æ£€æµ‹
        æ£€æŸ¥ç‰©ä½“ä¸Šæ–¹æ˜¯å¦æœ‰å…¶ä»–ç‰©ä½“
        """
        try:
            target_pos = target_obj.pose.p
            if target_pos.dim() > 1:
                target_pos = target_pos[0]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç‰©ä½“åœ¨ç›®æ ‡ç‰©ä½“ä¸Šæ–¹
            for obj in self.all_objects:
                if obj == target_obj:
                    continue
                
                obj_pos = obj.pose.p
                if obj_pos.dim() > 1:
                    obj_pos = obj_pos[0]
                
                # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡ç‰©ä½“ä¸Šæ–¹ï¼ˆxyå¹³é¢è·ç¦»å°äº5cmï¼Œzé«˜åº¦å¤§äºç›®æ ‡ç‰©ä½“ï¼‰
                xy_distance = torch.linalg.norm(obj_pos[:2] - target_pos[:2])
                if xy_distance < 0.05 and obj_pos[2] > target_pos[2]:
                    return True
            
            return False
            
        except Exception as e:
            #print(f"é®æŒ¡æ£€æµ‹å¤±è´¥: {e}")
            return False

    def _is_path_occluded_by_geometry(self, target_obj) -> bool:
        """
        å‡ ä½•é®æŒ¡æ£€æŸ¥ï¼šæ£€æŸ¥é¢„æŠ“å–è·¯å¾„æ˜¯å¦è¢«é˜»æŒ¡
        ä¼˜å…ˆä½¿ç”¨ç°æœ‰çš„ _is_object_blocked ä½œä¸ºå…œåº•
        
        Args:
            target_obj: ç›®æ ‡ç‰©ä½“
            
        Returns:
            bool: Trueè¡¨ç¤ºè¢«é®æŒ¡ï¼ŒFalseè¡¨ç¤ºè·¯å¾„ç•…é€š
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ç°æœ‰çš„ç®€åŒ–é®æŒ¡æ£€æµ‹
            return self._is_object_blocked(target_obj)
            
        except Exception as e:
            print(f"é®æŒ¡æ£€æŸ¥å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶ä¿å®ˆè¿”å›è¢«é®æŒ¡

    def _is_supporting_others(self, target_obj) -> bool:
        """
        æ”¯æ’‘æ£€æŸ¥ï¼šæ£€æŸ¥ç›®æ ‡ç‰©ä½“æ˜¯å¦æ­£åœ¨æ”¯æ’‘å…¶ä»–ç‰©ä½“
        ä½¿ç”¨ManiSkillç‰©ç†æŸ¥è¯¢æ£€æµ‹æ¥è§¦åŠ›å’Œä½ç½®å…³ç³»
        
        Args:
            target_obj: ç›®æ ‡ç‰©ä½“
            
        Returns:
            bool: Trueè¡¨ç¤ºæ­£åœ¨æ”¯æ’‘å…¶ä»–ç‰©ä½“ï¼ŒFalseè¡¨ç¤ºæ²¡æœ‰æ”¯æ’‘å…³ç³»
        """
        try:
            target_pos = target_obj.pose.p
            if target_pos.dim() > 1:
                target_pos = target_pos[0]
            
            # æ£€æŸ¥ä¸æ‰€æœ‰å…¶ä»–ç‰©ä½“çš„æ¥è§¦å…³ç³»
            for other_obj in self.all_objects:
                if other_obj == target_obj:
                    continue
                
                other_pos = other_obj.pose.p
                if other_pos.dim() > 1:
                    other_pos = other_pos[0]
                
                # é¦–å…ˆæ£€æŸ¥ä½ç½®å…³ç³»ï¼šå…¶ä»–ç‰©ä½“æ˜¯å¦åœ¨ç›®æ ‡ç‰©ä½“ä¸Šæ–¹
                if other_pos[2] <= target_pos[2]:
                    continue  # å…¶ä»–ç‰©ä½“ä¸åœ¨ä¸Šæ–¹ï¼Œè·³è¿‡
                
                try:
                    # ä½¿ç”¨ManiSkillçš„æ¥è§¦åŠ›æŸ¥è¯¢
                    contact_forces = self.scene.get_pairwise_contact_forces(target_obj, other_obj)
                    
                    if contact_forces is not None:
                        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ¥è§¦åŠ›
                        force_magnitude = torch.linalg.norm(contact_forces, dim=-1)
                        
                        # å¦‚æœæœ‰æœ‰æ•ˆçš„æ¥è§¦åŠ›ï¼ˆé˜ˆå€¼è®¾ä¸º0.1Nï¼‰
                        if torch.any(force_magnitude > 0.1):
                            # æ£€æŸ¥åŠ›çš„æ–¹å‘ï¼šå‘ä¸‹çš„åŠ›è¡¨æ˜ç›®æ ‡ç‰©ä½“åœ¨æ”¯æ’‘å…¶ä»–ç‰©ä½“
                            if contact_forces.dim() > 1:
                                # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„åŠ›å‘é‡
                                force_vec = contact_forces[0]
                            else:
                                force_vec = contact_forces
                            
                            # æ£€æŸ¥zæ–¹å‘çš„åŠ›ï¼šå¦‚æœåŠ›å‘ä¸‹ï¼ˆè´Ÿzæ–¹å‘ï¼‰ï¼Œè¯´æ˜ç›®æ ‡ç‰©ä½“åœ¨æ”¯æ’‘
                            if len(force_vec) >= 3 and force_vec[2] < -0.05:  # é˜ˆå€¼-0.05N
                                return True
                                
                except Exception as contact_error:
                    # æ¥è§¦æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨å‡ ä½•ä½ç½®ä½œä¸ºå…œåº•åˆ¤æ®
                    # å¦‚æœå…¶ä»–ç‰©ä½“åœ¨ç›®æ ‡ç‰©ä½“æ­£ä¸Šæ–¹å¾ˆè¿‘çš„è·ç¦»ï¼ˆ<3cmï¼‰ï¼Œè®¤ä¸ºæœ‰æ”¯æ’‘å…³ç³»
                    xy_distance = torch.linalg.norm(other_pos[:2] - target_pos[:2])
                    z_distance = other_pos[2] - target_pos[2]
                    if xy_distance < 0.08 and z_distance < 0.03:  # xyè·ç¦»<8cm, zè·ç¦»<3cm
                        return True
            
            return False
            
        except Exception as e:
            print(f"æ”¯æ’‘æ£€æŸ¥å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶ä¿å®ˆè¿”å›æ­£åœ¨æ”¯æ’‘

    def _get_ideal_world_grasp_pose(self, target_obj):
        """
        è®¡ç®—ç†æƒ³çš„ä¸–ç•Œåæ ‡ç³»æŠ“å–ä½å§¿
        ä»å±€éƒ¨åæ ‡ç³»çš„ç†æƒ³æŠ“å–å§¿æ€è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
        
        Args:
            target_obj: ç›®æ ‡ç‰©ä½“
            
        Returns:
            tuple: (grasp_pose, pre_grasp_pose) - æŠ“å–ä½å§¿å’Œé¢„æŠ“å–ä½å§¿
        """
        try:
            # è·å–ç‰©ä½“åç§°ï¼Œå°è¯•åŒ¹é…å·²çŸ¥çš„ç†æƒ³æŠ“å–å§¿æ€
            obj_name = target_obj.name
            local_grasp_pose = None
            
            # å°è¯•ä»å·²çŸ¥ç‰©ä½“ç±»å‹ä¸­æŸ¥æ‰¾
            for key in self.ideal_grasp_poses_local:
                if key in obj_name:
                    local_grasp_pose = self.ideal_grasp_poses_local[key]
                    break
            
            # å¦‚æœæœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æŠ“å–å§¿æ€
            if local_grasp_pose is None:
                local_grasp_pose = self.ideal_grasp_poses_local['default']
            
            # è·å–ç›®æ ‡ç‰©ä½“çš„ä¸–ç•Œä½å§¿
            target_world_pose = target_obj.pose
            if target_world_pose.p.dim() > 1:
                # å¤šç¯å¢ƒæƒ…å†µï¼Œå–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„å§¿æ€
                target_world_pose = sapien.Pose(
                    p=target_world_pose.p[0].cpu().numpy(),
                    q=target_world_pose.q[0].cpu().numpy()
                )
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥ç‰©ä½“ä½ç½®æ˜¯å¦åˆç†ï¼ˆé¿å…ç¬ç§»åçš„å¼‚å¸¸åæ ‡ï¼‰
            obj_pos = target_world_pose.p
            pos_magnitude = (obj_pos[0]**2 + obj_pos[1]**2 + obj_pos[2]**2)**0.5
            
            # å¦‚æœç‰©ä½“è·ç¦»åŸç‚¹è¶…è¿‡3ç±³ï¼Œè®¤ä¸ºæ˜¯å·²ç¬ç§»çš„ç‰©ä½“ï¼Œè·³è¿‡æŠ“å–
            if pos_magnitude > 3.0:
                print(f"âš ï¸ ç‰©ä½“ {target_obj.name} ä½ç½®å¼‚å¸¸ ({obj_pos})ï¼Œå¯èƒ½å·²è¢«ç¬ç§»ï¼Œè·³è¿‡æŠ“å–")
                # è¿”å›ä¸€ä¸ªå®‰å…¨çš„å·¥ä½œåŒºå†…ä½ç½®
                safe_pose = sapien.Pose(p=[0.0, 0.0, 0.5], q=[1, 0, 0, 0])
                safe_pre_pose = sapien.Pose(p=[0.0, 0.0, 0.58], q=[1, 0, 0, 0])
                return safe_pose, safe_pre_pose
            
            # è®¡ç®—ä¸–ç•Œåæ ‡ç³»ä¸‹çš„æŠ“å–ä½å§¿ï¼šworld_pose * local_pose
            world_grasp_pose = target_world_pose * local_grasp_pose
            
            # è®¡ç®—é¢„æŠ“å–ä½å§¿ï¼ˆåœ¨æŠ“å–ä½å§¿ä¸Šæ–¹å®‰å…¨è·ç¦»ï¼‰
            grasp_p = world_grasp_pose.p
            grasp_q = world_grasp_pose.q
            # ç¡®ä¿è½¬æ¢ä¸ºnumpyæ•°ç»„
            if hasattr(grasp_p, 'cpu'):
                grasp_p = grasp_p.cpu().numpy()
            if hasattr(grasp_q, 'cpu'):
                grasp_q = grasp_q.cpu().numpy()
            
            pre_grasp_pose = sapien.Pose(
                p=[grasp_p[0], grasp_p[1], grasp_p[2] + 0.08],  # ä¸Šæ–¹8cm
                q=grasp_q
            )
            
            return world_grasp_pose, pre_grasp_pose
            
        except Exception as e:
            print(f"è®¡ç®—ç†æƒ³æŠ“å–ä½å§¿å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›ç›®æ ‡ç‰©ä½“ä¸Šæ–¹çš„ç®€å•ä½å§¿
            obj_pos = target_obj.pose.p
            if obj_pos.dim() > 1:
                obj_pos = obj_pos[0]
            
            # ç®€å•çš„é¡¶æŠ“ä½å§¿ï¼ˆå‘ä¸‹ï¼‰
            # ç¡®ä¿obj_posè½¬æ¢ä¸ºnumpyæ•°ç»„
            if hasattr(obj_pos, 'cpu'):
                obj_pos = obj_pos.cpu().numpy()
            
            grasp_pose = sapien.Pose(
                p=[obj_pos[0], obj_pos[1], obj_pos[2] + 0.05],
                q=[1, 0, 0, 0]  # æœä¸‹
            )
            pre_grasp_pose = sapien.Pose(
                p=[obj_pos[0], obj_pos[1], obj_pos[2] + 0.13],
                q=[1, 0, 0, 0]  # æœä¸‹
            )
            
            return grasp_pose, pre_grasp_pose


    def _load_scene(self, options: dict):
        # æ„å»ºæ¡Œé¢åœºæ™¯
        self.scene_builder = TableSceneBuilder(
            self, robot_init_qpos_noise=self.robot_init_qpos_noise
        )
        self.scene_builder.build()
        
        # åŠ è½½æ‰˜ç›˜
        self._load_tray()
        
        # åˆ›å»ºç‰©ä½“åˆ—è¡¨
        self.all_objects = []
        self.selectable_objects = []
        self.object_info = []  # å­˜å‚¨ç‰©ä½“ä¿¡æ¯
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒåˆ›å»ºç‰©ä½“
        for env_idx in range(self.num_envs):
            env_objects = []
            env_selectable = []
            env_info = []
            
            # åˆ›å»ºæ¯ç§ç±»å‹çš„ç‰©ä½“
            for obj_type in self.BOX_OBJECTS:
                for i in range(self.num_objects_per_type):
                    # åˆ›å»ºç‰©ä½“
                    builder = actors.get_actor_builder(self.scene, id=f"ycb:{obj_type}")
                    

                    # åœ¨æ‰˜ç›˜å†…éšæœºç”Ÿæˆä½ç½®
                    x, y, z = self._generate_object_position_in_tray(i)
                    
                    # éšæœºå§¿æ€
                    quat = randomization.random_quaternions(1)[0]
                    initial_pose = sapien.Pose(p=[x, y, z], q=quat.cpu().numpy())
                    
                    builder.initial_pose = initial_pose
                    builder.set_scene_idxs([env_idx])
                    
                    obj_name = f"env_{env_idx}_{obj_type}_{i}"
                    obj = builder.build(name=obj_name)
                    
                    env_objects.append(obj)
                    env_selectable.append(obj)
                    
                    # å­˜å‚¨ç‰©ä½“ä¿¡æ¯
                    obj_info = {
                        'type': obj_type,
                        'size': self._get_object_size(obj_type),
                        'initial_pose': initial_pose,
                        'center': [x, y, z],
                        'exposed_area': 1.0,  # åˆå§‹æš´éœ²é¢ç§¯ï¼Œåç»­ä¼šè®¡ç®—
                    }
                    env_info.append(obj_info)
            
            self.all_objects.extend(env_objects)
            self.selectable_objects.append(env_selectable)
            self.object_info.append(env_info)
        
        # åˆå¹¶æ‰€æœ‰ç‰©ä½“
        if self.all_objects:
            self.merged_objects = Actor.merge(self.all_objects, name="all_objects")
        
        # åˆ›å»ºç›®æ ‡ä½ç½®æ ‡è®°
        self.goal_site = actors.build_sphere(
            self.scene,
            radius=self.goal_thresh,
            color=[0, 1, 0, 1],
            name="goal_site",
            body_type="kinematic",
            add_collision=False,
            initial_pose=sapien.Pose(),
        )
        self._hidden_objects.append(self.goal_site)
        
        # åˆå§‹åŒ–ç›®æ ‡ç‰©ä½“ç›¸å…³å˜é‡
        self.target_object = None
        self.target_object_indices = []

    def _load_tray(self):
        """åŠ è½½æ‰˜ç›˜URDFæ–‡ä»¶"""
        # è·å–æ‰˜ç›˜URDFæ–‡ä»¶è·¯å¾„
        tray_urdf_path = "/home/linux/jzh/RL_Robot/assets/tray/traybox.urdf"
        
        if not os.path.exists(tray_urdf_path):
            raise FileNotFoundError(f"æ‰˜ç›˜URDFæ–‡ä»¶æœªæ‰¾åˆ°: {tray_urdf_path}")
        
        # åˆ›å»ºURDFåŠ è½½å™¨
        loader = self.scene.create_urdf_loader()
        
        # è®¾ç½®æ‰˜ç›˜çš„ç‰©ç†å±æ€§
        loader.set_material(static_friction=0.8, dynamic_friction=0.6, restitution=0.05)
        loader.fix_root_link = True  # å›ºå®šæ‰˜ç›˜ä¸åŠ¨
        loader.scale = 1.0  # ä¿æŒåŸå§‹å°ºå¯¸
        
        # è§£æURDFæ–‡ä»¶
        parsed_result = loader.parse(tray_urdf_path)
        
        # åªä½¿ç”¨ actor_builders æ–¹å¼
        actor_builders = parsed_result.get("actor_builders", [])
        
        if not actor_builders:
            raise ValueError("æ‰˜ç›˜URDFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°actor_builders")
        
        self.trays = []
        
        # ä½¿ç”¨ actor_builders åŠ è½½æ‰˜ç›˜
        for env_idx in range(self.num_envs):
            builder = actor_builders[0]
            # è®¾ç½®æ‰˜ç›˜ä½ç½® (æ”¾åœ¨æ¡Œé¢ä¸Šï¼Œæœºå™¨äººå‰æ–¹)
            tray_position = [-0.2, 0.0, 0.006]  # æ¡Œé¢é«˜åº¦åŠ ä¸Šæ‰˜ç›˜åº•éƒ¨åšåº¦
            builder.initial_pose = sapien.Pose(p=tray_position)
            builder.set_scene_idxs([env_idx])
            
            # ä½¿ç”¨ build_static åˆ›å»ºé™æ€æ‰˜ç›˜ï¼Œç¡®ä¿ä¸ä¼šç§»åŠ¨
            tray = builder.build_static(name=f"tray_{env_idx}")
            self.trays.append(tray)
        
        # åˆå¹¶æ‰€æœ‰æ‰˜ç›˜
        if self.trays:
            self.merged_trays = Actor.merge(self.trays, name="all_trays")
        
        #print(f"æˆåŠŸåŠ è½½æ‰˜ç›˜ï¼Œå…± {len(self.trays)} ä¸ª")

    def _generate_object_position_in_tray(self, stack_level=0):
        """åœ¨æ‰˜ç›˜å†…ç”Ÿæˆç‰©ä½“ä½ç½®"""
        # æ‰˜ç›˜ä¸­å¿ƒä½ç½®
        tray_center_x = -0.2
        tray_center_y = 0.0
        tray_bottom_z = 0.02 + 0.02  # æ‰˜ç›˜åº•éƒ¨ + å°åç§»
        
        # æ‰˜ç›˜è¾¹ç•Œè®¡ç®—ï¼ˆåŸºäºURDFæ–‡ä»¶ä¸­çš„è¾¹ç•Œå¢™ä½ç½®ï¼‰
        # è¾¹ç•Œå¢™åœ¨æ‰˜ç›˜ä¸­å¿ƒçš„Â±0.2ç±³å¤„
        # å®é™…å¯ç”¨ç©ºé—´ï¼šä»ä¸­å¿ƒå‘ä¸¤è¾¹å„0.18ç±³ï¼ˆç•™å‡ºå®‰å…¨è¾¹è·ï¼‰
        safe_spawn_area_x = 0.18
        safe_spawn_area_y = 0.18
        
        # åœ¨æ‰˜ç›˜å†…éšæœºç”Ÿæˆxyä½ç½®
        x = tray_center_x + random.uniform(-safe_spawn_area_x, safe_spawn_area_x)
        y = tray_center_y + random.uniform(-safe_spawn_area_y, safe_spawn_area_y)
        
        # å †å é«˜åº¦
        z = tray_bottom_z + stack_level * 0.04  # æ¯å±‚é«˜åº¦
        
        return x, y, z

    def _get_object_size(self, obj_type):
        """è·å–ç‰©ä½“çš„å¤§å°ä¿¡æ¯"""
        # åŸºäºYCBæ•°æ®é›†çš„å®é™…ç‰©ä½“å°ºå¯¸ï¼ˆå•ä½ï¼šç±³ï¼‰
        sizes = {
            #"003_cracker_box": [0.16, 0.21, 0.07],         # é¥¼å¹²ç›’: 16cm x 21cm x 7cm
            "004_sugar_box": [0.09, 0.175, 0.044],         # ç³–ç›’: 9cm x 17.5cm x 4.4cm
            "006_mustard_bottle": [0.095, 0.095, 0.177],   # èŠ¥æœ«ç“¶: 9.5cm x 9.5cm x 17.7cm
            "008_pudding_box": [0.078, 0.109, 0.032],      # å¸ƒä¸ç›’: 7.8cm x 10.9cm x 3.2cm
            #"009_gelatin_box": [0.028, 0.085, 0.114],      # æ˜èƒ¶ç›’: 2.8cm x 8.5cm x 11.4cm  
            #"010_potted_meat_can": [0.101, 0.051, 0.051],  # ç½è£…è‚‰ç½å¤´: 10.1cm x 5.1cm x 5.1cm
           
        }
        return sizes.get(obj_type, [0.05, 0.05, 0.05])

    def _sample_target_objects(self):
        """éšæœºé€‰æ‹©ç›®æ ‡ç‰©ä½“"""
        target_objects = []
        self.target_object_indices = []
        
        for env_idx in range(self.num_envs):
            if env_idx < len(self.selectable_objects) and self.selectable_objects[env_idx]:
                # éšæœºé€‰æ‹©ä¸€ä¸ªå¯é€‰æ‹©çš„ç‰©ä½“
                target_idx = random.randint(0, len(self.selectable_objects[env_idx]) - 1)
                target_obj = self.selectable_objects[env_idx][target_idx]
                target_objects.append(target_obj)
                self.target_object_indices.append(target_idx)
        
        if target_objects:
            self.target_object = Actor.merge(target_objects, name="target_object")

    def _calculate_exposed_area(self, env_idx):
        """è®¡ç®—ç‰©ä½“çš„æš´éœ²é¢ç§¯"""
        # è¿™é‡Œæ˜¯ç®€åŒ–çš„æš´éœ²é¢ç§¯è®¡ç®—
        # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å‡ ä½•è®¡ç®—
        if env_idx < len(self.object_info):
            for i, obj_info in enumerate(self.object_info[env_idx]):
                # åŸºäºç‰©ä½“é«˜åº¦å’Œå‘¨å›´ç‰©ä½“æ•°é‡çš„ç®€å•ä¼°ç®—
                exposed_area = max(0.1, 1.0 - i * 0.1)  # è¶Šé«˜çš„ç‰©ä½“æš´éœ²é¢ç§¯è¶Šå¤§
                obj_info['exposed_area'] = exposed_area

    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):
        """åˆå§‹åŒ–æ¯ä¸ªepisode"""
        with torch.device(self.device):
            b = len(env_idx)
            self.scene_builder.initialize(env_idx)
            
            # é‡ç½®æ‰˜ç›˜ä½ç½®
            if hasattr(self, 'merged_trays'):
                # åœ¨GPUä»¿çœŸä¸­ï¼Œé™æ€å¯¹è±¡ä¸èƒ½æ”¹å˜ä½å§¿ï¼Œæ‰€ä»¥è·³è¿‡
                if not self.scene.gpu_sim_enabled:
                    if b == self.num_envs:
                        self.merged_trays.pose = self.merged_trays.initial_pose
                    else:
                        mask = torch.isin(self.merged_trays._scene_idxs, env_idx)
                        self.merged_trays.pose = self.merged_trays.initial_pose[mask]
                else:
                    #print("GPUä»¿çœŸæ¨¡å¼ä¸‹è·³è¿‡é™æ€æ‰˜ç›˜ä½å§¿é‡ç½®")
                    pass
            
            # é‡ç½®ç‰©ä½“åˆ°åˆå§‹ä½ç½®
            if hasattr(self, 'merged_objects'):
                if b == self.num_envs:
                    self.merged_objects.pose = self.merged_objects.initial_pose
                else:
                    mask = torch.isin(self.merged_objects._scene_idxs, env_idx)
                    self.merged_objects.pose = self.merged_objects.initial_pose[mask]
            
            # è®¾ç½®ç›®æ ‡ä½ç½® - å›ºå®šåœ¨æ‰˜ç›˜å³ä¾§
            goal_pos = torch.zeros((b, 3), device=self.device)
            
            # æ‰˜ç›˜ä¸­å¿ƒä½ç½®ï¼š[-0.2, 0.0, 0.02]
            # æ‰˜ç›˜å°ºå¯¸ï¼šé•¿0.6mï¼Œå®½0.6m
            # ç›®æ ‡ä½ç½®è®¾å®šåœ¨æ‰˜ç›˜å³ä¾§å¤–10cmå¤„ï¼Œé¿å…ä¸æ‰˜ç›˜è¾¹ç•Œå†²çª
            goal_pos[:, 0] = -0.4  # æ‰˜ç›˜å³ä¾§çš„å›ºå®šä½ç½®
            goal_pos[:, 1] = 0.4  
            goal_pos[:, 2] = 0.05  # æ¡Œé¢é«˜åº¦5cmï¼Œç¡®ä¿ç‰©ä½“ç¨³å®šæ”¾ç½®
            
            self.goal_pos = goal_pos
            self.goal_site.set_pose(Pose.create_from_pq(self.goal_pos))
            
            # è®°å½•åˆå§‹ç‰©ä½“ä½ç½®ï¼ˆç”¨äºè®¡ç®—ä½ç§»ï¼‰
            self.initial_object_positions = []
            for i in range(b):
                env_positions = []
                for obj in self.all_objects:
                    if hasattr(obj, '_scene_idxs') and len(obj._scene_idxs) > 0:
                        if obj._scene_idxs[0] == env_idx[i]:
                            env_positions.append(obj.pose.p.clone())
                self.initial_object_positions.append(env_positions)
            
            # è®¡ç®—æš´éœ²é¢ç§¯
            for i in range(b):
                self._calculate_exposed_area(env_idx[i])
            
            # é‡æ–°é€‰æ‹©ç›®æ ‡ç‰©ä½“ - åªåœ¨è¿ç»­åŠ¨ä½œæ¨¡å¼ä¸‹ä½¿ç”¨
            if not self.use_discrete_action:
                self._sample_target_objects()
            
            # æ–°å¢ï¼šåˆå§‹åŒ–ç¦»æ•£åŠ¨ä½œç›¸å…³å˜é‡
            if self.use_discrete_action:
                # ä¸ºæ¯ä¸ªç¯å¢ƒåˆå§‹åŒ–çŠ¶æ€
                if len(self.remaining_indices) != self.num_envs:
                    self.remaining_indices = [list(range(self.MAX_N)) for _ in range(self.num_envs)]
                    self.step_count = [0 for _ in range(self.num_envs)]
                    self.grasped_objects = [[] for _ in range(self.num_envs)]
                else:
                    # é‡ç½®æŒ‡å®šç¯å¢ƒçš„çŠ¶æ€
                    for i, env_id in enumerate(env_idx):
                        env_id_int = env_id.item() if hasattr(env_id, 'item') else int(env_id)
                        self.remaining_indices[env_id_int] = list(range(self.MAX_N))
                        self.step_count[env_id_int] = 0
                        self.grasped_objects[env_id_int] = []
                
                # æ–°å¢ï¼šé‡ç½®FSMçŠ¶æ€
                if hasattr(self, 'env_stage') and self.env_stage is not None:
                    if b == self.num_envs:
                        # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
                        self.env_stage.fill_(0)
                        self.env_target.fill_(-1)
                        self.env_busy.fill_(False)
                        self.stage_tick.fill_(0)
                        self.stage_positions.fill_(0)
                    else:
                        # é‡ç½®æŒ‡å®šç¯å¢ƒ
                        for i, env_id in enumerate(env_idx):
                            env_id_int = env_id.item() if hasattr(env_id, 'item') else int(env_id)
                            if env_id_int < self.num_envs:
                                self.env_stage[env_id_int] = 0
                                self.env_target[env_id_int] = -1
                                self.env_busy[env_id_int] = False
                                self.stage_tick[env_id_int] = 0
                                self.stage_positions[env_id_int].fill_(0)
            
            # æ–°å¢ï¼šé‡ç½®å¸ç›˜çº¦æŸçŠ¶æ€
            self.suction_constraints = {}
            self.is_suction_active = [False] * self.num_envs  # æ¯ä¸ªç¯å¢ƒçš„å¸ç›˜æ¿€æ´»çŠ¶æ€
            self.current_suction_object = [None] * self.num_envs  # æ¯ä¸ªç¯å¢ƒå½“å‰å¸é™„çš„ç‰©ä½“
            
            # æ–°å¢ï¼šé‡ç½®ç†æƒ³åŒ–ç¥è°•çŠ¶æ€
            self.oracle_attached_object = [None] * self.num_envs  # æ¸…ç©ºç†æƒ³åŒ–æŠ“å–è¿æ¥
            if hasattr(self, 'grasp_poses'):
                self.grasp_poses = [None] * self.num_envs  # æ¸…ç©ºæŠ“å–ä½å§¿ç¼“å­˜
            
            
            # ä½¿ç”¨æŒ‡å®šçš„æœºå™¨äººåˆå§‹å§¿æ€é‡ç½®
            # æŒ‡å®šçš„å…³èŠ‚ä½ç½®ï¼š[-1.6137, 1.3258, 1.9346, -0.8884, -1.6172, 1.0867, -3.0494, 0.04, 0.04]
            #target_qpos = np.array([-0.5370, 1.3258, 1.9346, -0.8884, -1.6172, 1.0867, -3.0494, 0.04, 0.04])
            #target_qpos = np.array([-1.6137, 1.3258, 1.9346, -0.8884, -1.6172, 1.0867, -3.0494, 0.04, 0.04])
            target_qpos = np.array([0.0, -0.785, 0.0, -2.356, 0.0, 1.571, 0.785, 0.04, 0.04])
            # qpos=np.array(
            #     [
            #         0.0,
            #         np.pi / 8,
            #         0,
            #         -np.pi * 5 / 8,
            #         0,
            #         np.pi * 3 / 4,
            #         np.pi / 4,
            #         0.04,
            #         0.04,
            #     ]
            # )
            # é‡ç½®æœºå™¨äººåˆ°æŒ‡å®šå§¿æ€
            self.agent.reset(target_qpos)
            #self.agent.reset()
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåˆå§‹åŒ–TCPä½ç½®è®°å½•æ•°ç»„ï¼ˆç¨ååœ¨åŠ¨ä½œæ‰§è¡Œæ—¶è®°å½•å®é™…ä½ç½®ï¼‰
            if not hasattr(self, 'initial_tcp_positions'):
                self.initial_tcp_positions = [None] * self.num_envs
            if not hasattr(self, 'tcp_recorded'):
                self.tcp_recorded = [False] * self.num_envs
            
            # é‡ç½®è®°å½•çŠ¶æ€
            for i in env_idx:
                if i < self.num_envs:
                    self.initial_tcp_positions[i] = None
                    self.tcp_recorded[i] = False

    def _get_obs_extra(self, info: Dict):
        """è·å–é¢å¤–è§‚æµ‹ä¿¡æ¯"""
        # è·å–æ‰¹æ¬¡å¤§å°
        batch_size = self.num_envs
        
        if not self.use_discrete_action:
            # è¿ç»­åŠ¨ä½œæ¨¡å¼ï¼šä¿æŒåŸæœ‰è§‚æµ‹ç»“æ„
            obs = dict(
                is_grasped=info["is_grasped"],
                tcp_pose=self.agent.tcp.pose.raw_pose,
                goal_pos=self.goal_site.pose.p,
            )
            
            if "state" in self.obs_mode:
                if hasattr(self, 'target_object') and self.target_object is not None:
                    obs.update(
                        target_obj_pose=self.target_object.pose.raw_pose,
                        tcp_to_obj_pos=self.target_object.pose.p - self.agent.tcp.pose.p,
                        obj_to_goal_pos=self.goal_site.pose.p - self.target_object.pose.p,
                    )
                else:
                    zero_pose = torch.zeros((batch_size, 7), device=self.device)
                    zero_pos = torch.zeros((batch_size, 3), device=self.device)
                    obs.update(
                        target_obj_pose=zero_pose,
                        tcp_to_obj_pos=zero_pos,
                        obj_to_goal_pos=zero_pos,
                    )
                
                obs.update(
                    num_objects=torch.tensor([len(self.all_objects)], device=self.device).repeat(batch_size),
                )
            return obs
        
        # ç¦»æ•£åŠ¨ä½œæ¨¡å¼ï¼šæ˜“æ”¶æ•›çš„baselineç‰¹å¾é›†
        
        # 1. å…¨å±€ç‰¹å¾ (æ¯ç¯å¢ƒ 3 ç»´)
        global_features = []
        for env_idx in range(batch_size):
            grasped_count = len(self.grasped_objects[env_idx])
            grasped_ratio = grasped_count / float(self.total_objects_per_env)  # å·²æŠ“æ•°é‡/æ€»æ•°é‡
            
            # ä½¿ç”¨æŠ“å–å°è¯•æ¬¡æ•°çš„æ¯”ä¾‹ä½œä¸ºç‰¹å¾
            attempt_ratio = min(self.step_count[env_idx] / float(self.total_objects_per_env), 1.0)  # æŠ“å–å°è¯•æ¬¡æ•°/æ€»æ•°é‡ï¼Œé™åˆ¶åœ¨[0,1]
            remaining_ratio = (self.total_objects_per_env - grasped_count) / float(self.total_objects_per_env)  # å‰©ä½™æ•°é‡/æ€»æ•°é‡
            
            global_features.append([grasped_ratio, attempt_ratio, remaining_ratio])
        
        global_features = torch.tensor(global_features, device=self.device, dtype=torch.float32)  # [batch_size, 3]
        
        # 2. æ¯ç‰©ä½“ç‰¹å¾ 
        # è·å–å·¥ä½œç©ºé—´èŒƒå›´ç”¨äºå½’ä¸€åŒ–
        workspace_min = torch.tensor([-0.5, -0.5, 0.0], device=self.device)  # å·¥ä½œç©ºé—´æœ€å°å€¼
        workspace_max = torch.tensor([0.5, 0.5, 0.3], device=self.device)     # å·¥ä½œç©ºé—´æœ€å¤§å€¼
        workspace_size = workspace_max - workspace_min
        
        # è·å–æœ€å¤§ç‰©ä½“å°ºå¯¸ç”¨äºå½’ä¸€åŒ–
        max_size = 0.2  # å‡è®¾æœ€å¤§ç‰©ä½“å°ºå¯¸ä¸º0.2m
        
        object_features = []  # [batch_size, 8, 8]
        action_mask = []      # [batch_size, 8]
        
        for env_idx in range(batch_size):
            env_obj_features = []
            env_mask = []
            
            for obj_idx in range(self.total_objects_per_env):  # åŠ¨æ€ç‰©ä½“æ•°é‡
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„ç‰©ä½“åˆ—è¡¨è€Œä¸æ˜¯å…¨å±€ç´¢å¼•
                if (env_idx < len(self.selectable_objects) and 
                    obj_idx < len(self.selectable_objects[env_idx])):
                    
                    # è·å–ç¯å¢ƒç‰¹å®šçš„ç‰©ä½“
                    target_obj = self.selectable_objects[env_idx][obj_idx]
                    obj_pose_p = target_obj.pose.p
                    
                    # å¤„ç†å¤šç¯å¢ƒä½ç½®æ•°æ®
                    if len(obj_pose_p.shape) > 1 and obj_pose_p.shape[0] > env_idx:
                        obj_pos = obj_pose_p[env_idx]  # [3]
                    elif len(obj_pose_p.shape) > 1 and obj_pose_p.shape[0] == 1:
                        obj_pos = obj_pose_p[0]
                    else:
                        obj_pos = obj_pose_p
                    
                    # ä½ç½®å½’ä¸€åŒ–åˆ° [0, 1]
                    pos_normalized = (obj_pos - workspace_min) / workspace_size
                    pos_normalized = torch.clamp(pos_normalized, 0.0, 1.0)
                    
                    # è·å–ç‰©ä½“å°ºå¯¸å¹¶å½’ä¸€åŒ–
                    obj_type_idx = obj_idx // self.num_objects_per_type
                    if obj_type_idx < len(self.BOX_OBJECTS):
                        obj_type = self.BOX_OBJECTS[obj_type_idx]
                        size = self._get_object_size(obj_type)
                        obj_size = torch.tensor(size, device=self.device)
                    else:
                        obj_size = torch.tensor([0.05, 0.05, 0.05], device=self.device)
                    
                    # å°ºå¯¸å½’ä¸€åŒ–åˆ° [0, 1]
                    size_normalized = obj_size / max_size
                    size_normalized = torch.clamp(size_normalized, 0.0, 1.0)
                    
                    # æŠ“å–æ ‡å¿—
                    grabbed_flag = 1.0 if obj_idx in self.grasped_objects[env_idx] else 0.0
                    
                    # é«˜åº¦ç‰¹å¾ (å¯é€‰ï¼Œå¦‚æœå·²åŒ…å«åœ¨pos_normalizedä¸­å¯ä»¥å»æ‰)
                    topness = pos_normalized[2]  # zåæ ‡å·²ç»å½’ä¸€åŒ–
                    
                    # ç»„åˆç‰¹å¾: [size_x, size_y, size_z, pos_x, pos_y, pos_z, grabbed_flag, topness]
                    obj_feature = torch.cat([
                        size_normalized,      # [3] - å°ºå¯¸
                        pos_normalized,       # [3] - ä½ç½®
                        torch.tensor([grabbed_flag], device=self.device),  # [1] - æŠ“å–æ ‡å¿—
                        torch.tensor([topness], device=self.device)        # [1] - é«˜åº¦ç‰¹å¾
                    ])  # æ€»å…±8ç»´
                    
                    # åŠ¨ä½œæ©ç ï¼šæœªæŠ“å–=1(å¯é€‰)ï¼Œå·²æŠ“å–=0(ä¸å¯é€‰)
                    mask_value = 0.0 if grabbed_flag > 0.5 else 1.0
                    
                else:
                    # å¡«å……é›¶ç‰¹å¾
                    obj_feature = torch.zeros(8, device=self.device)
                    mask_value = 0.0  # ä¸å­˜åœ¨çš„ç‰©ä½“ä¸å¯é€‰
                
                env_obj_features.append(obj_feature)
                env_mask.append(mask_value)
            
            object_features.append(torch.stack(env_obj_features))  # [8, 8]
            action_mask.append(torch.tensor(env_mask, device=self.device))  # [8]
        
        object_features = torch.stack(object_features)  # [batch_size, 8, 8]
        action_mask = torch.stack(action_mask)          # [batch_size, 8]
        
        # 3. å±•å¹³ç‰©ä½“ç‰¹å¾
        object_features_flat = object_features.view(batch_size, -1)  # [batch_size, total_objects_per_env * 8]
        
        # 4. ç»„åˆæœ€ç»ˆè§‚æµ‹
        # obs = concat(obj_feats.flatten(), action_mask, global_feats)
        final_obs = torch.cat([
            object_features_flat,  # [batch_size, total_objects_per_env * 8] - ç‰©ä½“ç‰¹å¾
            action_mask,          # [batch_size, total_objects_per_env]  - åŠ¨ä½œæ©ç 
            global_features       # [batch_size, 3]  - å…¨å±€ç‰¹å¾
        ], dim=1)  # [batch_size, total_objects_per_env * 9 + 3]
        
        # è¿”å›æ‰å¹³åŒ–çš„è§‚æµ‹å‘é‡ï¼Œç¬¦åˆbaselineè®­ç»ƒè¦æ±‚
        return final_obs

    
    def _set_gripper_target(self, target_width: float, env_idx: int = None) -> torch.Tensor:
        """
        è®¾ç½®å¤¹çˆªç›®æ ‡å®½åº¦ï¼Œå‚è€ƒpandaé£æ ¼çš„å¤¹çˆªæ§åˆ¶
        
        Args:
            target_width: å¤¹çˆªç›®æ ‡å®½åº¦ (0.0=é—­åˆ, 0.04=å®Œå…¨æ‰“å¼€)
            env_idx: ç¯å¢ƒç´¢å¼•ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ç¯å¢ƒ
            
        Returns:
            action: 7ç»´åŠ¨ä½œå‘é‡ï¼Œåªè®¾ç½®å¤¹çˆªéƒ¨åˆ†
        """
        # é™åˆ¶å¤¹çˆªå®½åº¦åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆå‚è€ƒpanda.pyçš„é…ç½®ï¼‰
        target_width = max(0.0, min(0.04, target_width))
        
        # æ„å»º7ç»´åŠ¨ä½œå‘é‡ [dx, dy, dz, drx, dry, drz, gripper]
        if env_idx is not None:
            # å•ä¸ªç¯å¢ƒ
            action = torch.zeros(7, device=self.device, dtype=torch.float32)
            action[6] = target_width
        else:
            # æ‰€æœ‰ç¯å¢ƒ
            action = torch.zeros(self.num_envs, 7, device=self.device, dtype=torch.float32)
            action[:, 6] = target_width
        
        return action

    def _is_grasping_object(self, target_obj, env_idx: int = 0, min_force: float = 0.5) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æˆåŠŸæŠ“å–ç‰©ä½“ï¼Œå‚è€ƒpanda.pyçš„is_graspingæ–¹æ³•
        åœ¨ç†æƒ³åŒ–æ¨¡å¼ä¸­ä¸»è¦ç”¨äºè°ƒè¯•å’ŒéªŒè¯
        
        Args:
            target_obj: ç›®æ ‡ç‰©ä½“
            env_idx: ç¯å¢ƒç´¢å¼•
            min_force: æœ€å°æ¥è§¦åŠ›é˜ˆå€¼
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæŠ“å–
        """
        try:
            # è·å–æœºæ¢°è‡‚çš„fingeré“¾æ¥ï¼ˆå‡è®¾ä¸pandaç»“æ„ç›¸åŒï¼‰
            finger_links = []
            for link in self.agent.robot.get_links():
                if 'finger' in link.name:
                    finger_links.append(link)
            
            if len(finger_links) < 2:
                # å¦‚æœæ‰¾ä¸åˆ°fingeré“¾æ¥ï¼Œé€€å›åˆ°ç®€å•çš„è·ç¦»æ£€æŸ¥
                tcp_pos = self.agent.tcp.pose.p
                if tcp_pos.dim() > 1:
                    tcp_pos = tcp_pos[env_idx]
                
                obj_pos = target_obj.pose.p
                if obj_pos.dim() > 1:
                    obj_pos = obj_pos[env_idx]
                
                distance = torch.linalg.norm(tcp_pos - obj_pos).item()
                return distance < 0.1  # 10cmä»¥å†…è®¤ä¸ºæŠ“å–æˆåŠŸ
            
            # ä½¿ç”¨æ¥è§¦åŠ›æ£€æŸ¥ï¼ˆå‚è€ƒpanda.pyå®ç°ï¼‰
            total_force = 0.0
            for finger_link in finger_links:
                contact_forces = self.scene.get_pairwise_contact_forces(finger_link, target_obj)
                if contact_forces is not None:
                    force_magnitude = torch.linalg.norm(contact_forces, dim=-1)
                    if force_magnitude.dim() > 0:
                        total_force += force_magnitude[env_idx].item()
                    else:
                        total_force += force_magnitude.item()
            
            return total_force >= min_force
            
        except Exception as e:
            # å‡ºé”™æ—¶ä½¿ç”¨è·ç¦»æ£€æŸ¥ä½œä¸ºå…œåº•
            tcp_pos = self.agent.tcp.pose.p
            if tcp_pos.dim() > 1:
                tcp_pos = tcp_pos[env_idx]
            
            obj_pos = target_obj.pose.p
            if obj_pos.dim() > 1:
                obj_pos = obj_pos[env_idx]
            
            distance = torch.linalg.norm(tcp_pos - obj_pos).item()
            return distance < 0.1

    def step(self, action):
        """
        è¦†ç›–stepæ–¹æ³•ä»¥æ”¯æŒç¦»æ•£åŠ¨ä½œé€‰æ‹©
        
        Args:
            action: å¦‚æœuse_discrete_action=Trueï¼Œåˆ™ä¸ºç‰©ä½“ç´¢å¼•ï¼›å¦åˆ™ä¸ºè¿ç»­åŠ¨ä½œ
        """
        if self.use_discrete_action:
            return self._discrete_step(action)
        else:
            # è°ƒç”¨çˆ¶ç±»çš„è¿ç»­åŠ¨ä½œstep
            return super().step(action)
    
    def _discrete_step(self, action):
        """
        å¤„ç†ç¦»æ•£åŠ¨ä½œçš„stepæ–¹æ³• - ä½¿ç”¨whileå¾ªç¯ç®¡ç†å¹¶è¡ŒFSM
        
        Args:
            action: è¦æŠ“å–çš„ç‰©ä½“ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º(num_envs,)æˆ–æ ‡é‡
        """
        # ç¡®ä¿actionæ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if isinstance(action, (int, np.integer)):
            # å•ä¸ªåŠ¨ä½œï¼Œå¤åˆ¶åˆ°æ‰€æœ‰ç¯å¢ƒ
            action = np.full(self.num_envs, action)
        elif isinstance(action, torch.Tensor):
            action = action.cpu().numpy()
        elif isinstance(action, np.ndarray):
            if action.shape == ():  # æ ‡é‡æ•°ç»„
                action = np.full(self.num_envs, action.item())
        
        # ç¡®ä¿actionæ˜¯æ­£ç¡®é•¿åº¦çš„æ•°ç»„
        if len(action) != self.num_envs:
            print(f"è­¦å‘Šï¼šåŠ¨ä½œé•¿åº¦{len(action)}ä¸ç¯å¢ƒæ•°é‡{self.num_envs}ä¸åŒ¹é…")
            action = np.full(self.num_envs, action[0] if len(action) > 0 else 0)
        
        # 1. ä¸ºç©ºé—²ç¯å¢ƒåˆ†é…æ–°ä»»åŠ¡
        for i in range(self.num_envs):
            if not self.env_busy[i]:
                pick = int(action[i])
                if pick >= 0 and pick < len(self.remaining_indices[i]):
                    # è·å–å®é™…çš„ç‰©ä½“ç´¢å¼•
                    target_idx = self.remaining_indices[i][pick]
                    self.env_target[i] = target_idx
                    self.remaining_indices[i].pop(pick)
                    self.env_stage[i] = 0
                    self.env_busy[i] = True
                    self.stage_tick[i] = 0
                    self.step_count[i] += 1
                    #print(f"ç¯å¢ƒ{i}: å¼€å§‹æ–°ä»»åŠ¡ - æŠ“å–ç‰©ä½“ç´¢å¼•{target_idx} (é€‰æ‹©{pick})")
        
        # 2. whileå¾ªç¯æ‰§è¡ŒFSMï¼Œç›´åˆ°æ‰€æœ‰ç¯å¢ƒå®Œæˆä»»åŠ¡
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåŸºäºé˜¶æ®µè°ƒæ•´è¶…æ—¶é™åˆ¶ï¼Œé¿å…å¤æ‚åŠ¨ä½œè¢«è¿‡æ—©æˆªæ–­
        max_fsm_steps = 1500  # é€‚åº¦é™ä½ä½†ä»è¶³å¤Ÿå®Œæˆå®Œæ•´æµç¨‹
        fsm_step_count = 0
        consecutive_stuck_steps = 0  # è¿ç»­å¡ä½æ­¥æ•°è®¡æ•°å™¨
        last_busy_count = torch.sum(self.env_busy).item()
        
        while torch.any(self.env_busy) and fsm_step_count < max_fsm_steps:
            # ä¸ºæ‰€æœ‰ç¯å¢ƒè®¡ç®—å½“å‰FSMçŠ¶æ€å¯¹åº”çš„ä½çº§è¿ç»­åŠ¨ä½œ
            cmd = torch.zeros(self.num_envs, 7, device=self.device, dtype=torch.float32)
            active_envs = 0
            
            for i in range(self.num_envs):
                if self.env_busy[i]:
                    cmd[i] = self._pick_object_step(i)
                    active_envs += 1
            
            # æ‰¹å¤„ç†æ‰§è¡Œä¸€æ­¥ç‰©ç†ä»¿çœŸ - æ‰€æœ‰ç¯å¢ƒåŒæ­¥å‰è¿›
            super().step(cmd)
            fsm_step_count += 1
            
            # ğŸ”§ æ£€æµ‹æ˜¯å¦æœ‰è¿›å±•ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
            current_busy_count = torch.sum(self.env_busy).item()
            if current_busy_count == last_busy_count:
                consecutive_stuck_steps += 1
            else:
                consecutive_stuck_steps = 0
                last_busy_count = current_busy_count
            
            # å¦‚æœè¿ç»­å¾ˆå¤šæ­¥æ²¡æœ‰è¿›å±•ï¼Œæå‰ç»“æŸé¿å…å¡æ­»
            if consecutive_stuck_steps > 500:
                print(f"âš ï¸ æ£€æµ‹åˆ°FSMå¯èƒ½å¡æ­»ï¼ˆè¿ç»­{consecutive_stuck_steps}æ­¥æ— è¿›å±•ï¼‰ï¼Œå¼ºåˆ¶ç»“æŸ")
                break
            
            # æ¯300æ­¥è¾“å‡ºä¸€æ¬¡è¿›åº¦ä¿¡æ¯ï¼ˆå‡å°‘è¾“å‡ºé¢‘ç‡ä½†æä¾›è¶³å¤Ÿåé¦ˆï¼‰
            if fsm_step_count % 300 == 0:
                print(f"FSMæ­¥éª¤ {fsm_step_count}: ä»æœ‰ {current_busy_count} ä¸ªç¯å¢ƒåœ¨æ‰§è¡Œä»»åŠ¡")
        
        # æ£€æŸ¥æ˜¯å¦å› ä¸ºè¶…æ—¶è€Œé€€å‡ºå¾ªç¯
        if fsm_step_count >= max_fsm_steps or consecutive_stuck_steps > 500:
            reason = "è¶…æ—¶" if fsm_step_count >= max_fsm_steps else "æ— è¿›å±•"
            print(f"âš ï¸ FSMæ‰§è¡Œå› {reason}è€Œç»ˆæ­¢ (æ­¥æ•°: {fsm_step_count})ï¼Œå¼ºåˆ¶ç»“æŸæ‰€æœ‰ä»»åŠ¡")
            # ğŸ”§ ä¼˜é›…åœ°å¼ºåˆ¶ç»“æŸæ‰€æœ‰ä»»åŠ¡å¹¶æ¸…ç†çŠ¶æ€
            self._force_terminate_all_tasks()
        
        completed_envs = torch.sum(~self.env_busy).item()
        # æä¾›æ›´è¯¦ç»†çš„å®Œæˆä¿¡æ¯
        if completed_envs == self.num_envs:
            pass  # æˆåŠŸå®Œæˆæ—¶ä¸è¾“å‡ºï¼Œå‡å°‘å¹²æ‰°
        else:
            active_envs = self.num_envs - completed_envs
            print(f"âš ï¸ FSMæ‰§è¡Œç»“æŸ - æ€»æ­¥æ•°: {fsm_step_count}, å®Œæˆ{completed_envs}ä¸ªï¼Œå‰©ä½™{active_envs}ä¸ª")
        
        # 3. æ¸…ç†æ‰€æœ‰ç¯å¢ƒçŠ¶æ€ï¼Œç¡®ä¿ä¸‹ä¸€è½®èƒ½æ­£å¸¸å¼€å§‹
        self._cleanup_completed_tasks()
        
        # 4. è®¡ç®—æœ€ç»ˆå¥–åŠ±å’Œè§‚æµ‹ - åŸºäºå®Œæ•´åŠ¨ä½œçš„æœ€ç»ˆç»“æœ
        info = self.get_info()
        obs = self.get_obs(info)
        reward = self.get_reward(obs=obs, action=action, info=info)
        
        # 5. æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        terminated = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        truncated = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        
        # ä½¿ç”¨ info ä¸­çš„ success å’Œ fail çŠ¶æ€
        if "success" in info and "fail" in info:
            terminated = torch.logical_or(info["success"], info["fail"])
        elif "success" in info:
            terminated = info["success"].clone()
        elif "fail" in info:
            terminated = info["fail"].clone()
        
        return obs, reward, terminated, truncated, info
    
    def _pick_object_step(self, env_idx: int) -> torch.Tensor:
        """
        ç†æƒ³åŒ–æŠ“å–çŠ¶æ€æœº - æ¯æ¬¡è°ƒç”¨åªæ‰§è¡Œå½“å‰çŠ¶æ€çš„ä¸€å°æ­¥
        ä½¿ç”¨ç¥è°•é€»è¾‘ï¼šé®æŒ¡/æ”¯æ’‘æ£€æŸ¥é€šè¿‡åˆ™100%æˆåŠŸï¼Œå¦åˆ™100%å¤±è´¥
        
        Args:
            env_idx: ç¯å¢ƒç´¢å¼•
            
        Returns:
            action: è¯¥ç¯å¢ƒçš„è¿ç»­åŠ¨ä½œå‘é‡ [dx, dy, dz, drx, dry, drz, gripper]
        """
        stage = self.env_stage[env_idx].item()
        target_idx = self.env_target[env_idx].item()
        tick = self.stage_tick[env_idx].item()
        
        # åˆå§‹åŒ–åŠ¨ä½œå‘é‡
        action = torch.zeros(7, device=self.device, dtype=torch.float32)
        
        try:
            # éªŒè¯ç›®æ ‡ç´¢å¼•æœ‰æ•ˆæ€§
            if target_idx < 0 or env_idx >= len(self.selectable_objects) or target_idx >= len(self.selectable_objects[env_idx]):
                print(f"ç¯å¢ƒ{env_idx}: æ— æ•ˆç›®æ ‡ç´¢å¼• target_idx={target_idx}")
                self.env_busy[env_idx] = False
                return action
            
            # è·å–ç›®æ ‡ç‰©ä½“
            target_obj = self.selectable_objects[env_idx][target_idx]
            
            # è·å–å½“å‰TCPä½ç½®
            tcp_pos = self.agent.tcp.pose.p
            if tcp_pos.dim() > 1:
                tcp_pos = tcp_pos[env_idx] if env_idx < tcp_pos.shape[0] else tcp_pos[0]

            # ğŸ”§ é‡è¦ä¿®å¤ï¼šæ¯æ¬¡å¼€å§‹æ–°çš„æŠ“å–ä»»åŠ¡æ—¶é‡æ–°è®°å½•TCPåˆå§‹ä½ç½®
            if hasattr(self, 'tcp_recorded'):
                # å¦‚æœæ˜¯æ–°çš„æŠ“å–ä»»åŠ¡ï¼ˆstage=0, tick=0ï¼‰ï¼Œæˆ–è€…ä»æœªè®°å½•è¿‡ï¼Œåˆ™é‡æ–°è®°å½•
                should_record = (stage == 0 and tick == 0) or not self.tcp_recorded[env_idx]
                if should_record:
                    self.initial_tcp_positions[env_idx] = tcp_pos.clone().detach()
                    self.tcp_recorded[env_idx] = True
                    tcp_pos_str = ', '.join([f'{x:.4f}' for x in self.initial_tcp_positions[env_idx].cpu().numpy()[:3]])
                    print(f"ç¯å¢ƒ{env_idx}: ğŸ“ è®°å½•åˆå§‹TCPä½ç½®: [{tcp_pos_str}]")

            # æ ¹æ®æ˜¯å¦å¯ç”¨ç†æƒ³åŒ–ç¥è°•é€‰æ‹©ä¸åŒçš„FSMé€»è¾‘
            if self.use_ideal_oracle:
                return self._ideal_oracle_fsm(env_idx, stage, tick, target_obj, target_idx, tcp_pos, action)
            else:
                return self._legacy_suction_fsm(env_idx, stage, tick, target_obj, target_idx, tcp_pos, action)
                
        except Exception as e:
            print(f"çŠ¶æ€æœºæ‰§è¡Œé”™è¯¯ env={env_idx}, stage={stage}: {e}")
            self.env_busy[env_idx] = False
            return action

    def _ideal_oracle_fsm(self, env_idx: int, stage: int, tick: int, target_obj, target_idx: int, tcp_pos, action: torch.Tensor) -> torch.Tensor:
        """
        ç†æƒ³åŒ–ç¥è°•FSM: é¢„æµ‹æ€§çš„å®Œç¾æŠ“å–æµç¨‹
        Stage 0: é€»è¾‘æ£€æŸ¥ï¼ˆé®æŒ¡+æ”¯æ’‘ï¼‰
        Stage 1: ç§»åŠ¨åˆ°é¢„æŠ“å–ä½å§¿
        Stage 2: ä¸‹æ¢åˆ°æŠ“å–ä½å§¿å¹¶é—­åˆå¤¹çˆª
        Stage 3: æå‡ç‰©ä½“ï¼ˆç†æƒ³åŒ–è·Ÿéšï¼‰
        Stage 4: ç¬ç§»ç‰©ä½“åˆ°è¿œå¤„
        Stage 5: å›åˆ°åˆå§‹ä½ç½®å¹¶å¼ å¼€å¤¹çˆª
        """
        if stage == 0:
            # Stage 0: ç¥è°•é€»è¾‘æ£€æŸ¥
            if tick == 0:
                print(f"ç¯å¢ƒ{env_idx}: ğŸ”® ç¥è°•æ£€æŸ¥ - ç›®æ ‡ç‰©ä½“: {target_obj.name}")
                
                # ğŸ”§ é¦–å…ˆæ£€æŸ¥ç‰©ä½“ä½ç½®æ˜¯å¦å¼‚å¸¸ï¼ˆæ˜¯å¦å·²è¢«ç¬ç§»ï¼‰
                obj_pos = target_obj.pose.p
                if obj_pos.dim() > 1:
                    obj_pos = obj_pos[0]
                pos_magnitude = (obj_pos[0]**2 + obj_pos[1]**2 + obj_pos[2]**2)**0.5
                
                if pos_magnitude > 3.0:
                    obj_pos_str = ', '.join([f'{x:.3f}' for x in obj_pos.cpu().numpy()])
                    print(f"ç¯å¢ƒ{env_idx}: âš ï¸ ç¥è°•åˆ¤å®šï¼šç‰©ä½“å·²è¢«ç¬ç§» ([{obj_pos_str}]) -> è·³è¿‡æŠ“å–")
                    self.env_busy[env_idx] = False
                    return action
                
                # é®æŒ¡æ£€æŸ¥
                is_occluded = self._is_path_occluded_by_geometry(target_obj)
                if is_occluded:
                    print(f"ç¯å¢ƒ{env_idx}: âŒ ç¥è°•åˆ¤å®šï¼šè·¯å¾„è¢«é®æŒ¡ -> æŠ“å–å¿…å¤±è´¥")
                    self.env_busy[env_idx] = False
                    return action
                
                # æ”¯æ’‘æ£€æŸ¥
                is_supporting = self._is_supporting_others(target_obj)
                if is_supporting:
                    print(f"ç¯å¢ƒ{env_idx}: âŒ ç¥è°•åˆ¤å®šï¼šç‰©ä½“æ­£æ”¯æ’‘å…¶ä»–ç‰©ä½“ -> æŠ“å–å¿…å¤±è´¥")
                    self.env_busy[env_idx] = False
                    return action
                
                print(f"ç¯å¢ƒ{env_idx}: âœ… ç¥è°•åˆ¤å®šï¼šé€»è¾‘æ¡ä»¶é€šè¿‡ -> æŠ“å–å¿…æˆåŠŸ")
                
                # è®¡ç®—ç†æƒ³æŠ“å–ä½å§¿
                grasp_pose, pre_grasp_pose = self._get_ideal_world_grasp_pose(target_obj)
                
                # ä¿å­˜æŠ“å–ç›¸å…³ä¿¡æ¯
                self.stage_positions[env_idx] = torch.tensor(pre_grasp_pose.p, device=self.device)
                # å°†æŠ“å–ä½å§¿ä¿å­˜åˆ°é¢å¤–å­—æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰
                if not hasattr(self, 'grasp_poses'):
                    self.grasp_poses = [None] * self.num_envs
                self.grasp_poses[env_idx] = grasp_pose
                
                print(f"ç¯å¢ƒ{env_idx}: é¢„æŠ“å–ä½ç½®: {pre_grasp_pose.p}")
            
            # ç›´æ¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼ˆé€»è¾‘æ£€æŸ¥åªéœ€1æ­¥ï¼‰
            self.env_stage[env_idx] = 1
            self.stage_tick[env_idx] = 0
        
        elif stage == 1:
            # Stage 1: ç§»åŠ¨åˆ°é¢„æŠ“å–ä½å§¿
            if tick == 0:
                print(f"ç¯å¢ƒ{env_idx}: Stage 1 - ç§»åŠ¨åˆ°é¢„æŠ“å–ä½ç½®")
            
            target_pos = self.stage_positions[env_idx]
            action[:3], reached = self._get_move_action(tcp_pos, target_pos, max_steps=100)
            
            if reached or tick >= 100:
                print(f"ç¯å¢ƒ{env_idx}: Stage 1å®Œæˆ - åˆ°è¾¾é¢„æŠ“å–ä½ç½®")
                # è®¾ç½®çœŸæ­£çš„æŠ“å–ä½ç½®
                if hasattr(self, 'grasp_poses') and self.grasp_poses[env_idx] is not None:
                    grasp_pos = self.grasp_poses[env_idx].p
                    self.stage_positions[env_idx] = torch.tensor(grasp_pos, device=self.device)
                
                self.env_stage[env_idx] = 2
                self.stage_tick[env_idx] = 0
            else:
                self.stage_tick[env_idx] += 1
            
            # ä¿æŒå¤¹çˆªå¼ å¼€
            action[6] = 0.04
        
        elif stage == 2:
            # Stage 2: ä¸‹æ¢åˆ°æŠ“å–ä½å§¿å¹¶é—­åˆå¤¹çˆª
            if tick == 0:
                print(f"ç¯å¢ƒ{env_idx}: Stage 2 - ä¸‹æ¢åˆ°æŠ“å–ä½ç½®å¹¶é—­åˆå¤¹çˆª")
            
            target_pos = self.stage_positions[env_idx]
            action[:3], reached = self._get_move_action(tcp_pos, target_pos, max_steps=60)
            
            # é€æ¸é—­åˆå¤¹çˆª
            if tick >= 20:  # å‰20æ­¥ç”¨äºç§»åŠ¨ï¼Œåé¢å¼€å§‹é—­åˆå¤¹çˆª
                action[6] = max(0.0, 0.04 - 0.04 * (tick - 20) / 20)  # é€æ¸ä»0.04é—­åˆåˆ°0.0
            else:
                action[6] = 0.04
            
            if reached or tick >= 60:
                # å»ºç«‹ç†æƒ³åŒ–è¿æ¥
                self.oracle_attached_object[env_idx] = target_obj
                print(f"ç¯å¢ƒ{env_idx}: Stage 2å®Œæˆ - ç†æƒ³åŒ–æŠ“å–è¿æ¥å»ºç«‹")
                
                # è®¾ç½®æå‡ç›®æ ‡ä½ç½®
                current_pos = self.stage_positions[env_idx].clone()
                current_pos[2] += 0.20  # ä¸Šå‡20cm
                self.stage_positions[env_idx] = current_pos
                
                self.env_stage[env_idx] = 3
                self.stage_tick[env_idx] = 0
            else:
                self.stage_tick[env_idx] += 1
        
        elif stage == 3:
            # Stage 3: æå‡ç‰©ä½“ï¼ˆç†æƒ³åŒ–è·Ÿéšï¼‰
            if tick == 0:
                print(f"ç¯å¢ƒ{env_idx}: Stage 3 - æå‡ç‰©ä½“")
            
            target_pos = self.stage_positions[env_idx]
            action[:3], reached = self._get_move_action(tcp_pos, target_pos, max_steps=80)
            
            # ä¿æŒå¤¹çˆªé—­åˆ
            action[6] = 0.0
            
            # ç†æƒ³åŒ–è·Ÿéšï¼šè®©ç‰©ä½“è·ŸéšTCPç§»åŠ¨
            if self.oracle_attached_object[env_idx] is not None:
                try:
                    # è·å–TCPå½“å‰ä½ç½®å¹¶è®¾ç½®ç‰©ä½“ä½ç½®
                    current_tcp_pos = tcp_pos.clone()
                    # ç‰©ä½“ä½ç½®ç¨ä½äºTCPï¼ˆæ¨¡æ‹Ÿè¢«æŠ“å–çš„æ•ˆæœï¼‰
                    object_target_pos = current_tcp_pos.clone()
                    object_target_pos[2] -= 0.05  # ç‰©ä½“åœ¨TCPä¸‹æ–¹5cm
                    
                    # ç›´æ¥è®¾ç½®ç‰©ä½“ä½ç½®ï¼ˆç†æƒ³åŒ–è·Ÿéšï¼‰
                    new_pose = sapien.Pose(p=object_target_pos.cpu().numpy())
                    self.oracle_attached_object[env_idx].set_pose(new_pose)
                except Exception as e:
                    print(f"ç¯å¢ƒ{env_idx}: ç†æƒ³åŒ–è·Ÿéšå¤±è´¥: {e}")
            
            if reached or tick >= 80:
                # æå‡å®Œæˆåï¼Œç­‰å¾…ä¸€å°ä¼šè®©ç‰©ä½“ç¨³å®š
                if tick >= 80 + 15:  # æå‡å®Œæˆåå†ç­‰å¾…15æ­¥ï¼ˆçº¦0.25ç§’ï¼‰
                    print(f"ç¯å¢ƒ{env_idx}: Stage 3å®Œæˆ - ç‰©ä½“æå‡å®Œæ¯•ï¼Œç­‰å¾…ç»“æŸ")
                    self.env_stage[env_idx] = 4
                    self.stage_tick[env_idx] = 0
                elif tick == 80:  # åˆšåˆ°è¾¾æ—¶çš„æç¤º
                    print(f"ç¯å¢ƒ{env_idx}: ç‰©ä½“æå‡åˆ°ä½ï¼Œç­‰å¾…ç¨³å®šä¸­...")
                    self.stage_tick[env_idx] += 1
                else:
                    # ç»§ç»­ç­‰å¾…ï¼Œä¿æŒç‰©ä½“è·Ÿéš
                    self.stage_tick[env_idx] += 1
            else:
                self.stage_tick[env_idx] += 1
        
        elif stage == 4:
            # Stage 4: ç¬ç§»ç‰©ä½“åˆ°è¿œå¤„ï¼ˆä¸å¼ å¼€å¤¹çˆªï¼‰
            if tick == 0:
                print(f"ç¯å¢ƒ{env_idx}: Stage 4 - ç¬ç§»ç‰©ä½“åˆ°è¿œå¤„")
                
                if self.oracle_attached_object[env_idx] is not None:
                    try:
                        # ç¬ç§»åˆ°è¿œå¤„ï¼ˆä¾‹å¦‚ (10, 10, 10)ï¼‰
                        far_pose = sapien.Pose(p=[10.0, 10.0, 10.0])
                        self.oracle_attached_object[env_idx].set_pose(far_pose)
                        print(f"ç¯å¢ƒ{env_idx}: âœ… ç‰©ä½“å·²ç¬ç§»åˆ°è¿œå¤„")
                        
                        # æ–­å¼€ç†æƒ³åŒ–è¿æ¥
                        self.oracle_attached_object[env_idx] = None
                    except Exception as e:
                        print(f"ç¯å¢ƒ{env_idx}: ç¬ç§»ç‰©ä½“å¤±è´¥: {e}")
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€è®°å½•çš„çœŸå®åˆå§‹TCPä½ç½®
                if hasattr(self, 'initial_tcp_positions') and self.initial_tcp_positions[env_idx] is not None:
                    initial_pos = self.initial_tcp_positions[env_idx]
                    initial_pos_str = ', '.join([f'{x:.4f}' for x in initial_pos.cpu().numpy()])
                    print(f"ç¯å¢ƒ{env_idx}: ä½¿ç”¨è®°å½•çš„åˆå§‹TCPä½ç½®: [{initial_pos_str}]")
                else:
                    # å…œåº•ï¼šå¦‚æœæ²¡æœ‰è®°å½•åˆ°åˆå§‹ä½ç½®ï¼Œä½¿ç”¨å®‰å…¨é»˜è®¤ä½ç½®
                    initial_pos = torch.tensor([0.0, 0.0, 0.4], device=self.device)
                    print(f"ç¯å¢ƒ{env_idx}: âš ï¸ æœªæ‰¾åˆ°è®°å½•çš„åˆå§‹ä½ç½®ï¼Œä½¿ç”¨é»˜è®¤ä½ç½®")
                
                self.stage_positions[env_idx] = initial_pos
            
            # ç›´æ¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼ˆç¬ç§»åªéœ€1æ­¥ï¼‰
            self.env_stage[env_idx] = 5
            self.stage_tick[env_idx] = 0
            
            # ä¿æŒå¤¹çˆªé—­åˆ
            action[6] = 0.0
        
        elif stage == 5:
            # Stage 5: å›åˆ°åˆå§‹ä½ç½®å¹¶å¼ å¼€å¤¹çˆª
            if tick == 0:
                print(f"ç¯å¢ƒ{env_idx}: Stage 5 - å›åˆ°åˆå§‹ä½ç½®")
            
            target_pos = self.stage_positions[env_idx]
            action[:3], reached = self._get_move_action(tcp_pos, target_pos, max_steps=100)
            
            # åˆ°è¾¾åå¼ å¼€å¤¹çˆª
            if reached or tick >= 80:
                action[6] = 0.04  # å¼ å¼€å¤¹çˆª
            else:
                action[6] = 0.0   # ä¿æŒé—­åˆ
            
            # ğŸ”§ ä¿®å¤ï¼šæ›´å®½æ¾çš„å®Œæˆæ¡ä»¶ï¼Œé¿å…å¡æ­»
            if (reached and tick >= 20) or tick >= 120:  # åˆ°è¾¾åç­‰å¾…20æ­¥ï¼Œæˆ–æœ€å¤š120æ­¥è¶…æ—¶
                # å®Œæˆæ•´ä¸ªç†æƒ³åŒ–æµç¨‹
                self.env_busy[env_idx] = False
                self.grasped_objects[env_idx].append(target_idx)
                self.stage_tick[env_idx] = 0
                self.env_stage[env_idx] = 0  # é‡ç½®çŠ¶æ€
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¸…é™¤TCPè®°å½•çŠ¶æ€ï¼Œä¸ºä¸‹ä¸€æ¬¡æŠ“å–åšå‡†å¤‡
                if hasattr(self, 'tcp_recorded'):
                    self.tcp_recorded[env_idx] = False
                    self.initial_tcp_positions[env_idx] = None
                
                print(f"ç¯å¢ƒ{env_idx}: âœ… ç†æƒ³åŒ–æŠ“å–æµç¨‹å®Œæˆ - {target_obj.name}ï¼Œå·²é‡ç½®çŠ¶æ€")
            else:
                self.stage_tick[env_idx] += 1
        
        else:
            # æœªçŸ¥çŠ¶æ€ï¼Œç»“æŸæµç¨‹
            self.env_busy[env_idx] = False
        
        # å§¿æ€æ§åˆ¶ï¼šä¿æŒå‚ç›´å‘ä¸‹
        action[3:6] = 0.0
        
        return action

    def _legacy_suction_fsm(self, env_idx: int, stage: int, tick: int, target_obj, target_idx: int, tcp_pos, action: torch.Tensor) -> torch.Tensor:
        """
        ä¼ ç»Ÿå¸ç›˜FSMï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        """
        # è¿™é‡Œä¿ç•™åŸæœ‰çš„å¸ç›˜é€»è¾‘ä½œä¸ºfallback
        print(f"ç¯å¢ƒ{env_idx}: ä½¿ç”¨ä¼ ç»Ÿå¸ç›˜æ¨¡å¼ï¼ˆç†æƒ³åŒ–ç¥è°•æœªå¯ç”¨ï¼‰")
        
        # è·å–ç›®æ ‡ç‰©ä½“ä½ç½®
        obj_pos = target_obj.pose.p[0].cpu().numpy()
        
        if stage == 0:
            # ç§»åŠ¨åˆ°ç‰©ä½“ä¸Šæ–¹
            if tick == 0:
                target_pos = obj_pos.copy()
                target_pos[2] += 0.15  # ä¸Šæ–¹15cm
                self.stage_positions[env_idx] = torch.tensor(target_pos, device=self.device)
            
            target_pos = self.stage_positions[env_idx]
            action[:3], reached = self._get_move_action(tcp_pos, target_pos, max_steps=150)
            
            if reached or tick >= 150:
                self.env_stage[env_idx] = 1
                self.stage_tick[env_idx] = 0
            else:
                self.stage_tick[env_idx] += 1
        
        # å…¶ä»–stageçš„ä¼ ç»Ÿé€»è¾‘...ï¼ˆç®€åŒ–ç‰ˆï¼Œå¦‚éœ€å®Œæ•´ç‰ˆå¯ç»§ç»­æ·»åŠ ï¼‰
        else:
            self.env_busy[env_idx] = False
        
        action[3:6] = 0.0  # å§¿æ€æ§åˆ¶
        action[6] = 0.0   # å¤¹çˆªé—­åˆ
        
        return action
    
    def _get_move_action(self, current_pos: torch.Tensor, target_pos: torch.Tensor, 
                        max_steps: int = 100) -> Tuple[torch.Tensor, bool]:
        """
        è·å–å¹³æ»‘ç§»åŠ¨åŠ¨ä½œå’Œæ˜¯å¦åˆ°è¾¾ç›®æ ‡
        
        Args:
            current_pos: å½“å‰ä½ç½®
            target_pos: ç›®æ ‡ä½ç½®
            max_steps: æœ€å¤§æ­¥æ•°ï¼ˆç”¨äºåˆ¤æ–­è¶…æ—¶ï¼‰
            
        Returns:
            action: ä½ç½®åŠ¨ä½œ [dx, dy, dz]
            reached: æ˜¯å¦åˆ°è¾¾ç›®æ ‡
        """
        if isinstance(target_pos, np.ndarray):
            target_pos = torch.tensor(target_pos, device=self.device, dtype=torch.float32)
        
        # è®¡ç®—ä½ç½®è¯¯å·®
        pos_error = target_pos - current_pos
        current_distance = torch.linalg.norm(pos_error).item()
        
        # åˆ¤æ–­æ˜¯å¦åˆ°è¾¾ - å¹³è¡¡ç²¾åº¦å’Œå¯è¾¾æ€§çš„é˜ˆå€¼
        reached = current_distance < 0.05  # 5cmç²¾åº¦ï¼Œé¿å…è¿‡äºä¸¥æ ¼å¯¼è‡´å¡æ­»
        
        if reached:
            return torch.zeros(3, device=self.device, dtype=torch.float32), True
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æ­¥é•¿ç­–ç•¥ï¼Œå‡å°‘å¯¹å…¶ä»–ç‰©ä½“çš„æ‰°åŠ¨
        max_controller_step = 0.05  # é™ä½æœ€å¤§å¢é‡åˆ°5cmï¼Œé¿å…è¿‡å¤§æ‰°åŠ¨
        
        # # ä¼˜åŒ–æ­¥é•¿ç­–ç•¥ - æé«˜æ”¶æ•›é€Ÿåº¦
        # if current_distance > 0.15:
        #     scale_factor = 1.0  # ä½¿ç”¨100%çš„æ§åˆ¶å™¨èƒ½åŠ›
        # elif current_distance > 0.10:
        #     scale_factor = 0.95  # ç¨å¾®å‡é€Ÿ
        # elif current_distance > 0.05:
        #     scale_factor = 0.8  # ä¸­ç­‰é€Ÿåº¦
        # ğŸš€ å¹³æ»‘æ­¥é•¿ç­–ç•¥ - å‚è€ƒå®˜æ–¹æ§åˆ¶å™¨çš„æœ€ä½³å®è·µ
        if current_distance > 0.20:
            scale_factor = 0.8  # é•¿è·ç¦»æ—¶é€‚åº¦å‡é€Ÿ
        elif current_distance > 0.15:
            scale_factor = 0.6  # ä¸­ç­‰è·ç¦»è¿›ä¸€æ­¥å‡é€Ÿ
        elif current_distance > 0.08:
            scale_factor = 0.4  # æ¥è¿‘ç›®æ ‡æ—¶æ˜¾è‘—å‡é€Ÿ
        elif current_distance > 0.04:
            scale_factor = 0.2  # ç²¾ç»†æ§åˆ¶é˜¶æ®µ
        else:
            scale_factor = 0.1  # æé«˜ç²¾ç»†æ§åˆ¶é€Ÿåº¦ï¼ˆä»0.5æå‡åˆ°0.7ï¼‰0.1
        
        actual_max_step = max_controller_step * scale_factor
        
        # å½’ä¸€åŒ–ä½ç½®è¯¯å·®åˆ°å¹³æ»‘æ­¥é•¿
        pos_error_norm = torch.linalg.norm(pos_error)
        if pos_error_norm > actual_max_step:
            action = (pos_error / pos_error_norm) * actual_max_step
        else:
            action = pos_error
        
        # ğŸ”§ é¢å¤–å¹³æ»‘åŒ–ï¼šå¯¹å¿«é€Ÿå˜åŒ–è¿›è¡Œé™åˆ¶
        # ğŸ”§ é¢å¤–å¹³æ»‘åŒ–ï¼šå¯¹æ¯ä¸ªç»´åº¦å•ç‹¬é™åˆ¶
        action = torch.clamp(action, -actual_max_step, actual_max_step)
        
        return action, False
    
    def _get_failed_step_result(self):
        """è·å–å¤±è´¥æ­¥éª¤çš„ç»“æœ"""
        # æƒ©ç½šæ€§å¥–åŠ± - è½¬æ¢ä¸ºtorch.Tensor
        reward = torch.tensor([-1.0], device=self.device, dtype=torch.float32)
        
        # ä¸ç»ˆæ­¢ï¼Œè®©æ™ºèƒ½ä½“å­¦ä¹  - è½¬æ¢ä¸ºtorch.Tensor
        terminated = torch.tensor([False], device=self.device, dtype=torch.bool)
        truncated = torch.tensor([False], device=self.device, dtype=torch.bool)
        
        # è·å–å½“å‰è§‚æµ‹
        info = self.evaluate()
        info.update({
            'success': False,
            'displacement': 0.0,
            'remaining_objects': sum(len(env_remaining) for env_remaining in self.remaining_indices),
            'grasped_objects': sum(len(env_grasped) for env_grasped in self.grasped_objects),
        })
        
        obs = self._get_obs_extra(info)
        
        return obs, reward, terminated, truncated, info



    @property
    def discrete_action_space(self):
        """è·å–ç¦»æ•£åŠ¨ä½œç©ºé—´"""
        if self.use_discrete_action:
            import gymnasium as gym
            return gym.spaces.Discrete(self.MAX_N)
        else:
            return None

    def evaluate(self):
        """è¯„ä¼°ä»»åŠ¡å®Œæˆæƒ…å†µ"""
        success = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        fail = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        is_grasped = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        is_robot_static = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        is_obj_placed = torch.zeros(self.num_envs, device=self.device, dtype=bool)
        
        if self.use_discrete_action:
            # ç¦»æ•£åŠ¨ä½œæ¨¡å¼ï¼šåŸºäºæŠ“å–æˆåŠŸçš„ç‰©ä½“æ•°é‡æ¯”ä¾‹è¯„ä¼°
            for env_idx in range(self.num_envs):
                # è®¡ç®—æŠ“å–æˆåŠŸçš„ç‰©ä½“æ•°é‡
                grasped_count = len(self.grasped_objects[env_idx])
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰©ä½“è¢«æˆåŠŸæŠ“å–
                is_grasped[env_idx] = grasped_count > 0
                
                # è®¡ç®—æˆåŠŸç‡ï¼šæŠ“å–æˆåŠŸçš„ç‰©ä½“æ•°é‡æ¯”ä¾‹
                success_ratio = grasped_count / self.total_objects_per_env
                success[env_idx] = success_ratio == 1.0   # æ‰€æœ‰ç‰©ä½“éƒ½è¢«æŠ“å–è®¤ä¸ºæˆåŠŸ
                
                # å¤±è´¥æ¡ä»¶ï¼šè¾¾åˆ°æœ€å¤§æŠ“å–å°è¯•æ¬¡æ•°ä½†æœªæˆåŠŸ
                # æ³¨æ„ï¼šstep_countæ˜¯æŠ“å–å°è¯•æ¬¡æ•°ï¼Œä¸æ˜¯æ€»ä»¿çœŸæ­¥æ•°
                # MAX_EPISODE_STEPS=15 è¡¨ç¤ºæœ€å¤š15æ¬¡æŠ“å–å°è¯•
                if hasattr(self, 'step_count'):
                    fail[env_idx] = self.step_count[env_idx] >= self.MAX_EPISODE_STEPS and not success[env_idx]
        else:
            # è¿ç»­åŠ¨ä½œæ¨¡å¼ï¼šåŸºäºtarget_objectè¯„ä¼°
            if hasattr(self, 'target_object') and self.target_object is not None:
                # æ£€æŸ¥ç‰©ä½“æ˜¯å¦æ”¾ç½®åˆ°ç›®æ ‡ä½ç½®
                obj_to_goal_dist = torch.linalg.norm(
                    self.goal_site.pose.p - self.target_object.pose.p, axis=1
                )
                is_obj_placed = obj_to_goal_dist <= self.goal_thresh
                
                # æ£€æŸ¥æ˜¯å¦æŠ“å–
                is_grasped = self.agent.is_grasping(self.target_object)
                
                # æ£€æŸ¥æœºå™¨äººæ˜¯å¦é™æ­¢
                is_robot_static = self.agent.is_static(0.2)
                
                # æˆåŠŸæ¡ä»¶ï¼šç‰©ä½“æ”¾ç½®åˆ°ä½ä¸”æœºå™¨äººé™æ­¢
                success = is_obj_placed & is_robot_static
        
        return {
            "success": success,
            "fail": fail,
            "is_obj_placed": is_obj_placed,
            "is_robot_static": is_robot_static,
            "is_grasped": is_grasped,
        }

    def _calculate_other_objects_displacement(self):
        """è®¡ç®—å…¶ä»–ç‰©ä½“çš„ä½ç§»è·ç¦»"""
        total_displacement = torch.zeros(self.num_envs, device=self.device)
        
        for env_idx in range(self.num_envs):
            displacement = 0.0
            obj_count = 0
            
            for i, obj in enumerate(self.all_objects):
                if hasattr(obj, '_scene_idxs') and len(obj._scene_idxs) > 0:
                    if obj._scene_idxs[0] == env_idx:
                        # è·³è¿‡ç›®æ ‡ç‰©ä½“
                        if hasattr(self, 'target_object_indices') and env_idx < len(self.target_object_indices):
                            if i == self.target_object_indices[env_idx]:
                                continue
                        
                        # è®¡ç®—ä½ç§»
                        if hasattr(self, 'initial_object_positions') and env_idx < len(self.initial_object_positions):
                            if obj_count < len(self.initial_object_positions[env_idx]):
                                initial_pos = self.initial_object_positions[env_idx][obj_count]
                                current_pos = obj.pose.p
                                displacement += torch.linalg.norm(current_pos - initial_pos)
                                obj_count += 1
            
            total_displacement[env_idx] = displacement
        
        return total_displacement

    def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è®¡ç®—å¯†é›†å¥–åŠ±"""
        reward = torch.zeros(self.num_envs, device=self.device)
        
        # æ ¹æ®åŠ¨ä½œæ¨¡å¼é€‰æ‹©ä¸åŒçš„å¥–åŠ±è®¡ç®—ç­–ç•¥
        if self.use_discrete_action:
            # ç¦»æ•£åŠ¨ä½œæ¨¡å¼ï¼šä½¿ç”¨é€‰æ‹©å¥–åŠ±é€»è¾‘
            return self._compute_discrete_action_reward(info)
        else:
            # è¿ç»­åŠ¨ä½œæ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰çš„å¯†é›†å¥–åŠ±é€»è¾‘
            return self._compute_continuous_action_reward(info)
    
    def _compute_discrete_action_reward(self, info: Dict):
        """è®¡ç®—ç¦»æ•£åŠ¨ä½œæ¨¡å¼çš„å¥–åŠ± - åŸºäºå®Œæ•´åŠ¨ä½œçš„æœ€ç»ˆç»“æœ
        
        æ³¨æ„ï¼šåœ¨æ–°æ¶æ„ä¸‹ï¼Œæ¯ä¸ªRLæ­¥éª¤å¯¹åº”ä¸€ä¸ªå®Œæ•´çš„æŠ“å–åŠ¨ä½œï¼Œ
        å¥–åŠ±åŸºäºè¯¥åŠ¨ä½œçš„æœ€ç»ˆç»“æœè®¡ç®—ï¼Œæ›´ç®€æ´ä¸”æ˜“äºæ”¶æ•›
        """
        reward = torch.zeros(self.num_envs, device=self.device)
        
        # å¥–åŠ±ç³»æ•° - é’ˆå¯¹å®Œæ•´åŠ¨ä½œä¼˜åŒ–
        R_success = 3.0      # æˆåŠŸæŠ“å–ä¸€ä¸ªç‰©ä½“çš„å¥–åŠ±
        R_complete = 15.0    # å®Œæˆæ‰€æœ‰ç‰©ä½“çš„é¢å¤–å¥–åŠ±
        R_failure = -0.5     # æŠ“å–å¤±è´¥çš„æƒ©ç½š
        w_disp = 0.8         # ä½ç§»æƒ©ç½šæƒé‡
        
        for env_idx in range(self.num_envs):
            current_grasped = len(self.grasped_objects[env_idx])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æˆåŠŸæŠ“å–
            if not hasattr(self, '_prev_grasped_count'):
                self._prev_grasped_count = [0] * self.num_envs
            
            prev_grasped = self._prev_grasped_count[env_idx]
            
            if current_grasped > prev_grasped:
                # æˆåŠŸæŠ“å–äº†æ–°ç‰©ä½“
                new_grasps = current_grasped - prev_grasped
                reward[env_idx] += R_success * new_grasps
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰ç‰©ä½“
                if current_grasped == self.total_objects_per_env:
                    reward[env_idx] += R_complete
                    
            # elif hasattr(self, 'step_count') and self.step_count[env_idx] > prev_grasped:
            #     # æœ‰æŠ“å–å°è¯•ä½†æ²¡æœ‰æˆåŠŸï¼ˆstep_countå¢åŠ ä½†grasped_countæ²¡å˜ï¼‰
            #     reward[env_idx] += R_failure
            
            # # ç®€åŒ–çš„ä½ç§»æƒ©ç½š
            # other_displacement = self._calculate_other_objects_displacement()
            # if env_idx < len(other_displacement):
            #     # å°†ä½ç§»æƒ©ç½šé™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            #     displacement_penalty = torch.clamp(other_displacement[env_idx] * w_disp, 0, 2.0)
            #     reward[env_idx] -= displacement_penalty
        
        # æ›´æ–°è®°å½•
        for env_idx in range(self.num_envs):
            self._prev_grasped_count[env_idx] = len(self.grasped_objects[env_idx])
        
        return reward
    
    def _compute_continuous_action_reward(self, info: Dict):
        """è®¡ç®—è¿ç»­åŠ¨ä½œæ¨¡å¼çš„å¥–åŠ±"""
        reward = torch.zeros(self.num_envs, device=self.device)
        
        if not hasattr(self, 'target_object') or self.target_object is None:
            return reward
        
        # 1. æ¥è¿‘å¥–åŠ±ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        tcp_to_obj_dist = torch.linalg.norm(
            self.target_object.pose.p - self.agent.tcp.pose.p, axis=1
        )
        reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)
        reward += reaching_reward * 2.0  # æƒé‡2.0
        
        # 2. æŠ“å–å¥–åŠ±
        is_grasped = info["is_grasped"]
        reward += is_grasped * 3.0  # æƒé‡3.0
        
        # 3. æ”¾ç½®å¥–åŠ±
        obj_to_goal_dist = torch.linalg.norm(
            self.goal_site.pose.p - self.target_object.pose.p, axis=1
        )
        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)
        reward += place_reward * is_grasped * 2.0  # åªæœ‰æŠ“å–æ—¶æ‰ç»™æ”¾ç½®å¥–åŠ±
        
        # 4. å…¶ä»–ç‰©ä½“ä½ç§»æƒ©ç½šï¼ˆä¼˜å…ˆçº§ç¬¬äºŒï¼‰
        other_displacement = self._calculate_other_objects_displacement()
        displacement_penalty = torch.tanh(other_displacement)
        reward -= displacement_penalty * 1.5  # æƒé‡1.5
        
        # 5. æ—¶é—´æƒ©ç½šï¼ˆä¼˜å…ˆçº§ç¬¬ä¸‰ï¼‰
        time_penalty = 0.01  # æ¯æ­¥å°æƒ©ç½š
        reward -= time_penalty
        
        # 6. é™æ­¢å¥–åŠ±
        static_reward = 1 - torch.tanh(
            5 * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)
        )
        reward += static_reward * info["is_obj_placed"] * 1.0
        
        # 7. æˆåŠŸå¥–åŠ±
        reward[info["success"]] = 10.0
        
        return reward

    def compute_sparse_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è®¡ç®—ç¨€ç–å¥–åŠ±"""
        reward = torch.zeros(self.num_envs, device=self.device)
        
        # åªæœ‰æˆåŠŸæ—¶æ‰ç»™å¥–åŠ±
        reward[info["success"]] = 1.0
        
        # å…¶ä»–ç‰©ä½“ä½ç§»æƒ©ç½š
        other_displacement = self._calculate_other_objects_displacement()
        displacement_penalty = other_displacement * 0.1
        reward -= displacement_penalty
        
        return reward

    def compute_normalized_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
        """è®¡ç®—å½’ä¸€åŒ–å¯†é›†å¥–åŠ±"""
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œnormalized_dense_reward åº”è¯¥æ˜¯å¯¹ dense_reward çš„å½’ä¸€åŒ–
        # ä¸åº”è¯¥æ ¹æ® reward_mode é€‰æ‹©ä¸åŒçš„å¥–åŠ±å‡½æ•°
        dense_reward = self.compute_dense_reward(obs=obs, action=action, info=info)
        return dense_reward / 10.0 

 

    def _init_fsm_states(self):
        """åˆå§‹åŒ–æœ‰é™çŠ¶æ€æœºçŠ¶æ€å¼ é‡"""
        self.env_stage = torch.zeros(self.num_envs, dtype=torch.int8, device=self.device)
        self.env_target = torch.full((self.num_envs,), -1, dtype=torch.int16, device=self.device)
        self.env_busy = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        self.stage_tick = torch.zeros(self.num_envs, dtype=torch.int16, device=self.device)
        self.stage_positions = torch.zeros(self.num_envs, 3, dtype=torch.float32, device=self.device)
    
    def _force_terminate_all_tasks(self):
        """ğŸ”§ å¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰ä»»åŠ¡å¹¶æ¸…ç†çŠ¶æ€"""
        self.env_busy.fill_(False)
        self.env_target.fill_(-1)
        self.env_stage.fill_(0)
        self.stage_tick.fill_(0)
        
        # æ¸…ç†ç†æƒ³åŒ–é™„ç€çŠ¶æ€
        if hasattr(self, 'oracle_attached_object'):
            for i in range(self.num_envs):
                self.oracle_attached_object[i] = None
                
        # æ¸…ç†TCPçŠ¶æ€
        if hasattr(self, 'tcp_recorded'):
            for i in range(self.num_envs):
                self.tcp_recorded[i] = False
                if hasattr(self, 'initial_tcp_positions'):
                    self.initial_tcp_positions[i] = None
        
        print("ğŸ”§ å·²å¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰FSMä»»åŠ¡å¹¶æ¸…ç†çŠ¶æ€")
    
    def _cleanup_completed_tasks(self):
        """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡çŠ¶æ€"""
        for env_idx in range(self.num_envs):
            if not self.env_busy[env_idx] and self.env_target[env_idx] != -1:
                # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡çŠ¶æ€
                old_target = self.env_target[env_idx].item()
                self.env_target[env_idx] = -1
                self.env_stage[env_idx] = 0
                self.stage_tick[env_idx] = 0
                #print(f"ç¯å¢ƒ{env_idx}: ä»»åŠ¡å®Œæˆï¼Œé‡ç½®ç›®æ ‡ {old_target}")
    
    def _init_anygrasp(self):
        """åˆå§‹åŒ–AnyGraspæ¨¡å‹ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åŠ è½½ï¼‰"""
        if not ANYGRASP_AVAILABLE:
            print("AnyGraspä¸å¯ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return
            
        if self.anygrasp_model is not None:
            return  # å·²ç»åˆå§‹åŒ–è¿‡äº†
        
        try:
            print("æ­£åœ¨åˆå§‹åŒ–AnyGraspæ¨¡å‹...")
            # åˆ›å»ºé…ç½®å¯¹è±¡
            import argparse
            cfgs = argparse.Namespace()
            cfgs.checkpoint_path = self.ANYGRASP_CHECKPOINT
            cfgs.max_gripper_width = self.ANYGRASP_MAX_GRIPPER_WIDTH
            cfgs.gripper_height = self.ANYGRASP_GRIPPER_HEIGHT
            cfgs.top_down_grasp = self.ANYGRASP_TOP_DOWN_GRASP
            cfgs.debug = False  # å…³é—­è°ƒè¯•å¯è§†åŒ–
            
            # åˆå§‹åŒ–AnyGrasp
            self.anygrasp_model = AnyGrasp(cfgs)
            self.anygrasp_model.load_net()
            print("âœ… AnyGraspæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ AnyGraspæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.anygrasp_enabled = False
            self.anygrasp_model = None
    
    def _get_camera_observations(self, camera_name: str = "base_camera") -> Dict:
        """
        è·å–ç›¸æœºè§‚æµ‹æ•°æ®ï¼ŒåŒ…æ‹¬RGBã€æ·±åº¦å’Œåˆ†å‰²
        
        Args:
            camera_name: ç›¸æœºåç§°
            
        Returns:
            åŒ…å«sensor_dataå’Œsensor_paramçš„å­—å…¸
        """
        # ä½¿ç”¨æ ‡å‡†çš„ManiSkillæ–¹å¼è·å–sensoræ•°æ®
        # è¿™ä¼šè‡ªåŠ¨å¤„ç†éšè—å¯¹è±¡ã€æ›´æ–°æ¸²æŸ“ç­‰
        for obj in self._hidden_objects:
            obj.hide_visual()
        self.scene.update_render(update_sensors=True, update_human_render_cameras=False)
        self.capture_sensor_data()
        
        # è·å–sensoræ•°æ®å’Œå‚æ•°
        sensor_data = {}
        sensor_params = {}
        
        if camera_name in self._sensors:
            camera = self._sensors[camera_name]
            sensor_data[camera_name] = camera.get_obs(
                rgb=True,
                depth=True,
                segmentation=True,
                position=True
            )
            sensor_params[camera_name] = camera.get_params()
        
        return {
            'sensor_data': sensor_data,
            'sensor_param': sensor_params
        }
    
    def _extract_target_pointcloud(self, target_obj: Actor, env_idx: int = 0, camera_name: str = "base_camera") -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        ä½¿ç”¨å®ä¾‹åˆ†å‰²æå–ç›®æ ‡ç‰©ä½“çš„ç‚¹äº‘ - ä½¿ç”¨æ·±åº¦å›¾åæŠ•å½±æ–¹æ³•
        
        Args:
            target_obj: ç›®æ ‡ç‰©ä½“Actorå¯¹è±¡
            env_idx: ç¯å¢ƒç´¢å¼•
            camera_name: ä½¿ç”¨çš„ç›¸æœºåç§°
            
        Returns:
            (points, colors): ç›¸æœºåæ ‡ç³»ä¸‹çš„ç‚¹äº‘å’Œé¢œè‰²ï¼Œå¤±è´¥è¿”å›(None, None)
        """
        try:
            # è·å–ç›¸æœºè§‚æµ‹æ•°æ®å’Œå‚æ•°
            camera_obs = self._get_camera_observations(camera_name)
            if camera_obs is None:
                return None, None
            
            sensor_data = camera_obs['sensor_data'][camera_name]
            sensor_params = camera_obs['sensor_param'][camera_name]
            
            # æå–å„é€šé“æ•°æ®
            rgb = sensor_data["rgb"]  # [B, H, W, 3]
            depth = sensor_data["depth"]  # [B, H, W, 1]
            segmentation = sensor_data["segmentation"]  # [B, H, W, 1]
            
            print(f"ç¯å¢ƒ{env_idx}: sensor_dataåŒ…å«çš„é€šé“: {list(sensor_data.keys())}")
            
            # è·å–å½“å‰ç¯å¢ƒçš„æ•°æ®
            if env_idx >= rgb.shape[0]:
                print(f"ç¯å¢ƒç´¢å¼•{env_idx}è¶…å‡ºèŒƒå›´")
                return None, None
            
            rgb_env = rgb[env_idx]  # [H, W, 3]
            depth_env = depth[env_idx]  # [H, W, 1]
            seg_env = segmentation[env_idx]  # [H, W, 1]
            
            # è·å–ç›®æ ‡ç‰©ä½“çš„å®ä¾‹ID
            target_seg_ids = target_obj.per_scene_id  # torch.int32 tensor
            if target_seg_ids.numel() == 0:
                print(f"ç›®æ ‡ç‰©ä½“æ²¡æœ‰å®ä¾‹ID")
                return None, None
            
            # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š
            if target_seg_ids.device != seg_env.device:
                target_seg_ids = target_seg_ids.to(seg_env.device)
            
            # æå–Actorå±‚çš„åˆ†å‰²ï¼ˆç¬¬0é€šé“ï¼‰
            actor_seg = seg_env[..., 0]  # [H, W]
            
            # åˆ›å»ºç›®æ ‡ç‰©ä½“çš„åŸºç¡€mask
            if target_seg_ids.numel() == 1:
                base_mask = (actor_seg == target_seg_ids.item())
            else:
                # å¤šä¸ªIDçš„æƒ…å†µ
                base_mask = torch.isin(actor_seg, target_seg_ids)
            
            # æ£€æŸ¥åŸºç¡€maskæ˜¯å¦æœ‰æ•ˆ
            base_pixels = base_mask.sum().item()
            if base_pixels == 0:
                print(f"ç¯å¢ƒ{env_idx}: ç›®æ ‡ç‰©ä½“åœ¨å½“å‰è§†è§’ä¸‹ä¸å¯è§")
                return None, None
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´å¤šä¸Šä¸‹æ–‡çš„ç‚¹äº‘æå–ç­–ç•¥ï¼Œç±»ä¼¼å®˜æ–¹demo
            
            # è·å–æ‰˜ç›˜çš„segmentation IDï¼ˆç”¨äºæ’é™¤ï¼‰
            tray_seg_ids = set()
            if hasattr(self, 'trays') and self.trays:
                for tray in self.trays:
                    if hasattr(tray, 'per_scene_id') and tray.per_scene_id.numel() > 0:
                        tray_seg_ids.add(tray.per_scene_id.item())
            
            # ğŸ”§ ç­–ç•¥æ”¹è¿›ï¼šæä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ç»™AnyGraspï¼Œä½†æ™ºèƒ½è¿‡æ»¤
            import torch.nn.functional as F
            
            # 1. åŸºç¡€ç›®æ ‡ç‰©ä½“mask
            target_mask = base_mask
            
            # 2. æ‰©å±•åŒºåŸŸåŒ…å«å‘¨å›´ç‰©ä½“ï¼ˆä¸åŒ…æ‹¬æ‰˜ç›˜ï¼‰
            # ä½¿ç”¨æ›´å¤§çš„è†¨èƒ€æ ¸æ¥è·å–å‘¨å›´ä¸Šä¸‹æ–‡ï¼Œä½†ä¼šæ™ºèƒ½è¿‡æ»¤
            kernel = torch.ones((1,1,7,7), device=base_mask.device)  # ä½¿ç”¨7x7æ ¸è·å–æ›´å¤šä¸Šä¸‹æ–‡
            expanded_mask = F.conv2d(base_mask.float().unsqueeze(0).unsqueeze(0), kernel, padding=3).squeeze()
            expanded_mask = (expanded_mask > 0)
            
            # 3. åˆ›å»ºæ‰˜ç›˜æ’é™¤mask  
            tray_exclusion_mask = torch.zeros_like(actor_seg, dtype=torch.bool)
            if tray_seg_ids:
                for tray_id in tray_seg_ids:
                    tray_exclusion_mask |= (actor_seg == tray_id)
            
            # 4. æ·±åº¦æœ‰æ•ˆæ€§æ£€æŸ¥ - ğŸ”§ ä¿®å¤ï¼šé€‚åº”æ–°çš„æ·±åº¦ç¼©æ”¾
            depth_valid = (depth_env[..., 0] > 0) & (depth_env[..., 0] / 1000.0 < 1.0)
            
            # 5. åˆ›å»ºä¸Šä¸‹æ–‡maskï¼šåŒ…å«ç›®æ ‡ç‰©ä½“å‘¨å›´çš„å…¶ä»–ç‰©ä½“ï¼Œä½†æ’é™¤æ‰˜ç›˜
            # è¿™æ ·AnyGraspå¯ä»¥çœ‹åˆ°ç›®æ ‡ç‰©ä½“åŠå…¶å‘¨å›´ç¯å¢ƒï¼Œè·å¾—æ›´å¥½çš„æŠ“å–è¯„åˆ†
            context_mask = expanded_mask & depth_valid & (actor_seg > 0) & (~tray_exclusion_mask)
            
            # 6. æœ€ç»ˆmaskï¼šç›®æ ‡ç‰©ä½“ + å‘¨å›´ä¸Šä¸‹æ–‡ï¼Œä½†æ’é™¤æ‰˜ç›˜
            mask = context_mask
            valid_pixels = mask.sum().item()
            
            print(f"ç¯å¢ƒ{env_idx}: åŸå§‹ç›®æ ‡åƒç´ : {base_pixels}, å¢å¼ºååƒç´ : {valid_pixels} (+{valid_pixels-base_pixels})")
            
            # ä½¿ç”¨æ·±åº¦å›¾åæŠ•å½±æ–¹æ³•
            # ä»sensor_paramsè·å–ç›¸æœºå†…å‚
            if 'intrinsic_cv' in sensor_params:
                # ä½¿ç”¨OpenCVæ ¼å¼çš„å†…å‚çŸ©é˜µ
                intrinsic = sensor_params['intrinsic_cv'][env_idx].cpu().numpy()  # [3, 3]
                fx, fy = intrinsic[0, 0], intrinsic[1, 1]
                cx, cy = intrinsic[0, 2], intrinsic[1, 2]
            else:
                # å¤‡é€‰æ–¹æ¡ˆï¼šä»å…¶ä»–å‚æ•°è®¡ç®—
                print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°intrinsic_cvï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ³•")
                # æ£€æŸ¥å¯ç”¨çš„å‚æ•°
                print(f"å¯ç”¨çš„sensorå‚æ•°: {list(sensor_params.keys())}")
                # ä½¿ç”¨é»˜è®¤å€¼æˆ–ä»å…¶ä»–å‚æ•°æ¨å¯¼
                H, W = depth_env.shape[:2]
                fx = fy = W / 2.0  # ç®€å•ä¼°ç®—
                cx, cy = W / 2.0, H / 2.0
            
            print(f"ç¯å¢ƒ{env_idx}: ç›¸æœºå†…å‚ fx={fx:.2f}, fy={fy:.2f}, cx={cx:.2f}, cy={cy:.2f}")
            
            # è·å–maskå†…çš„åƒç´ åæ ‡å’Œæ·±åº¦å€¼
            v, u = torch.where(mask)  # æ³¨æ„ï¼štorch.whereè¿”å›(row, col)å³(y, x)
            z = depth_env[v, u, 0]  # æ·±åº¦å€¼
            
            # ğŸ”§ ä¿®å¤æ·±åº¦æ•°æ®æ ¼å¼å’Œç¼©æ”¾ - é€‚åº”é«˜åˆ†è¾¨ç‡ç›¸æœº
            if z.dtype == torch.int16:
                # ManiSkillçš„æ·±åº¦æ•°æ®éœ€è¦ç‰¹æ®Šçš„ç¼©æ”¾å› å­
                # ğŸ”§ ä¿®å¤ï¼šé«˜åˆ†è¾¨ç‡ç›¸æœºéœ€è¦è°ƒæ•´ç¼©æ”¾å› å­
                # 640x480ç›¸æœºçš„æ·±åº¦ç¼©æ”¾å› å­éœ€è¦é‡æ–°æ ¡å‡†
                scale_factor = 1000.0  # æ›´æ¥è¿‘å®˜æ–¹demoçš„scale=1000.0
                z = z.float() / scale_factor  
                print(f"ç¯å¢ƒ{env_idx}: æ·±åº¦æ•°æ®ä»int16è½¬æ¢ä¸ºç±³ï¼ˆç¼©æ”¾å› å­{scale_factor}ï¼‰")
            elif z.dtype in [torch.float32, torch.float64]:
                # å·²ç»æ˜¯æµ®ç‚¹æ•°ï¼Œå‡è®¾å•ä½æ˜¯ç±³
                z = z.float()
            
            # è¿‡æ»¤æ— æ•ˆæ·±åº¦
            valid_depth = z > 0
            u_valid = u[valid_depth]
            v_valid = v[valid_depth]
            z_valid = z[valid_depth]
            
            if len(z_valid) == 0:
                print(f"ç¯å¢ƒ{env_idx}: æ²¡æœ‰æœ‰æ•ˆçš„æ·±åº¦å€¼")
                return None, None
            
            print(f"ç¯å¢ƒ{env_idx}: æœ‰æ•ˆæ·±åº¦ç‚¹æ•°: {len(z_valid)}, æ·±åº¦èŒƒå›´: [{z_valid.min():.3f}, {z_valid.max():.3f}]ç±³")
            
            # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»
            # ç›¸æœºåæ ‡ç³»ï¼šXå³ï¼ŒYä¸‹ï¼ŒZå‰ï¼ˆOpenGLé£æ ¼ï¼‰
            x = (u_valid.float() - cx) / fx * z_valid
            y = (v_valid.float() - cy) / fy * z_valid
            
            # ç»„åˆæˆç‚¹äº‘ [N, 3]
            points_cam = torch.stack([x, y, z_valid], dim=-1).cpu().numpy().astype(np.float32)
            
            # è·å–å¯¹åº”çš„é¢œè‰²
            colors = rgb_env[v_valid, u_valid, :3].cpu().numpy().astype(np.float32)
            
            # ç¡®ä¿é¢œè‰²åœ¨[0,1]èŒƒå›´å†…
            if colors.max() > 1.0:
                colors = colors / 255.0
            
            print(f"ç¯å¢ƒ{env_idx}: æˆåŠŸæå–{len(points_cam)}ä¸ª3Dç‚¹")
            print(f"ç‚¹äº‘èŒƒå›´: X[{points_cam[:, 0].min():.3f}, {points_cam[:, 0].max():.3f}], Y[{points_cam[:, 1].min():.3f}, {points_cam[:, 1].max():.3f}], Z[{points_cam[:, 2].min():.3f}, {points_cam[:, 2].max():.3f}]")
            
            return points_cam, colors
                
        except Exception as e:
            print(f"ç¯å¢ƒ{env_idx}: æå–ç›®æ ‡ç‚¹äº‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def _detect_grasps_for_target(self, target_obj: Actor, env_idx: int = 0, 
                                  top_k: int = 20, visualize: bool = False, 
                                  visualize_in_env: bool = False) -> Optional[List[Dict]]:
        """
        ä¸ºç›®æ ‡ç‰©ä½“æ£€æµ‹æŠ“å–ç‚¹
        
        Args:
            target_obj: ç›®æ ‡ç‰©ä½“
            env_idx: ç¯å¢ƒç´¢å¼•
            top_k: è¿”å›å‰kä¸ªæœ€ä½³æŠ“å–
            visualize: æ˜¯å¦ä½¿ç”¨Open3D/matplotlibå¯è§†åŒ–æŠ“å–ç»“æœ
            visualize_in_env: æ˜¯å¦åœ¨ä»¿çœŸç¯å¢ƒæ¸²æŸ“å›¾åƒä¸­å¯è§†åŒ–æŠ“å–
            
        Returns:
            æŠ“å–å€™é€‰åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            - pose: æŠ“å–ä½å§¿ (4x4 transformation matrix in world frame)
            - score: æŠ“å–è´¨é‡åˆ†æ•°
            - width: å¤¹çˆªå®½åº¦
            å¤±è´¥è¿”å›None
        """
        if not self.anygrasp_enabled or self.anygrasp_model is None:
            print("AnyGraspæœªå¯ç”¨æˆ–æœªåˆå§‹åŒ–")
            return None
        
        try:
            # 1. æå–ç›®æ ‡ç‰©ä½“ç‚¹äº‘
            points, colors = self._extract_target_pointcloud(target_obj, env_idx)
            if points is None or len(points) == 0:
                print(f"ç¯å¢ƒ{env_idx}: æ— æ³•æå–ç›®æ ‡ç‰©ä½“ç‚¹äº‘")
                return None
            
            # è·å–ç›¸æœºå‚æ•°ï¼ˆç”¨äºåç»­åæ ‡å˜æ¢ï¼‰
            camera_obs = self._get_camera_observations("base_camera")
            sensor_params = camera_obs['sensor_param']["base_camera"]
            
            # 2. è®¾ç½®å·¥ä½œç©ºé—´é™åˆ¶ï¼ˆç›¸æœºåæ ‡ç³»ï¼‰- ğŸ”§ ä¿®å¤ï¼šæ›´å®½æ¾çš„å·¥ä½œç©ºé—´ï¼Œç±»ä¼¼å®˜æ–¹demo
            if len(points) > 0:
                # è·å–ç‚¹äº‘çš„æ•´ä½“èŒƒå›´
                point_min = points.min(axis=0)
                point_max = points.max(axis=0)
                obj_center = points.mean(axis=0)
                
                # ğŸ”§ ä½¿ç”¨æ›´å®½æ¾çš„å·¥ä½œç©ºé—´è®¾ç½®ï¼Œå‚è€ƒå®˜æ–¹demo
                # å®˜æ–¹demo: X[-0.19, 0.12], Y[0.02, 0.15], Z[0.0, 1.0]
                # åœ¨ç›¸æœºåæ ‡ç³»ä¸‹ï¼Œç»™äºˆè¶³å¤Ÿçš„ç©ºé—´è®©AnyGraspæ£€æµ‹
                
                # åŸºäºç‚¹äº‘èŒƒå›´ï¼Œä½†ç»™äºˆå……è¶³çš„è¾¹è·
                x_range = point_max[0] - point_min[0]
                y_range = point_max[1] - point_min[1] 
                z_range = point_max[2] - point_min[2]
                
                # ä½¿ç”¨ç‚¹äº‘èŒƒå›´çš„1.5å€ä½œä¸ºå·¥ä½œç©ºé—´ï¼Œä½†æœ‰æœ€å°å€¼ä¿è¯
                x_margin = max(x_range * 0.75, 0.12)  # è‡³å°‘12cmè¾¹è·
                y_margin = max(y_range * 0.75, 0.12)  # è‡³å°‘12cmè¾¹è·
                z_margin = max(z_range * 1.0, 0.15)   # è‡³å°‘15cmæ·±åº¦è¾¹è·
                
                xmin = obj_center[0] - x_margin
                xmax = obj_center[0] + x_margin
                ymin = obj_center[1] - y_margin
                ymax = obj_center[1] + y_margin
                zmin = max(0.01, point_min[2] - z_margin/2)
                zmax = point_max[2] + z_margin
                
                # ğŸ”§ ç¡®ä¿ä¸ä¼šæ£€æµ‹åˆ°æ‰˜ç›˜åº•éƒ¨ï¼Œä½†å…è®¸è¶³å¤Ÿçš„ç©ºé—´
                # æ‰˜ç›˜åœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„å¤§è‡´æ·±åº¦æ˜¯0.33+ï¼Œæˆ‘ä»¬è®¾ç½®ä¸Šé™ä¸º0.4
                zmax = min(zmax, 0.4)
                
                print(f"ç¯å¢ƒ{env_idx}: ç‚¹äº‘èŒƒå›´: X[{points[:, 0].min():.3f}, {points[:, 0].max():.3f}], Y[{points[:, 1].min():.3f}, {points[:, 1].max():.3f}], Z[{points[:, 2].min():.3f}, {points[:, 2].max():.3f}]")
                print(f"ç¯å¢ƒ{env_idx}: ç›®æ ‡ä¸­å¿ƒ: [{obj_center[0]:.3f}, {obj_center[1]:.3f}, {obj_center[2]:.3f}]")
                print(f"ç¯å¢ƒ{env_idx}: å·¥ä½œç©ºé—´è¾¹è·: XÂ±{x_margin:.3f}, YÂ±{y_margin:.3f}, Z+{z_margin:.3f}")
            else:
                # ä½¿ç”¨ç±»ä¼¼å®˜æ–¹demoçš„é»˜è®¤èŒƒå›´ï¼ˆç›¸æœºåæ ‡ç³»ï¼‰
                xmin, xmax = -0.2, 0.2
                ymin, ymax = -0.2, 0.2
                zmin, zmax = 0.01, 0.4
            
            lims = [xmin, xmax, ymin, ymax, zmin, zmax]
            print(f"ç¯å¢ƒ{env_idx}: å·¥ä½œç©ºé—´é™åˆ¶: X[{xmin:.3f}, {xmax:.3f}], Y[{ymin:.3f}, {ymax:.3f}], Z[{zmin:.3f}, {zmax:.3f}]")
            
            # 3. è°ƒç”¨AnyGraspæ£€æµ‹æŠ“å–
            print(f"ç¯å¢ƒ{env_idx}: å¼€å§‹æ£€æµ‹æŠ“å–ç‚¹ï¼Œè¾“å…¥ç‚¹äº‘å¤§å°: {points.shape}")
            print(f"ç¯å¢ƒ{env_idx}: ç‚¹äº‘æ•°æ®ç±»å‹: {points.dtype}, é¢œè‰²æ•°æ®ç±»å‹: {colors.dtype}")
            print(f"ç¯å¢ƒ{env_idx}: ç‚¹äº‘èŒƒå›´æ£€æŸ¥: X[{points[:, 0].min():.3f}, {points[:, 0].max():.3f}], Y[{points[:, 1].min():.3f}, {points[:, 1].max():.3f}], Z[{points[:, 2].min():.3f}, {points[:, 2].max():.3f}]")
            print(f"ç¯å¢ƒ{env_idx}: é¢œè‰²èŒƒå›´æ£€æŸ¥: [{colors.min():.3f}, {colors.max():.3f}]")
            
            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
            points = points.astype(np.float32)
            colors = colors.astype(np.float32)
            
            # ğŸ”§ ä¿®å¤ï¼šä¼˜åŒ–ç‚¹äº‘å¯†åº¦æ£€æŸ¥å’ŒAnyGraspå‚æ•°
            min_points_threshold = 50  # é™ä½é˜ˆå€¼ï¼Œæ›´ç°å®
            if len(points) < min_points_threshold:
                print(f"ç¯å¢ƒ{env_idx}: ç‚¹äº‘ç¨€ç–({len(points)}ä¸ªç‚¹ï¼Œéœ€è¦è‡³å°‘{min_points_threshold}ä¸ª)ï¼Œä½†ç»§ç»­å°è¯•æ£€æµ‹")
            
            # ğŸ”§ ä¿®å¤ï¼šä¼˜åŒ–AnyGraspå‚æ•°ï¼Œæé«˜æŠ“å–è´¨é‡
            gg, cloud = self.anygrasp_model.get_grasp(
                points, 
                colors, 
                lims=lims, 
                apply_object_mask=False,  # æˆ‘ä»¬å·²ç»æä¾›äº†ç²¾ç¡®çš„ç›®æ ‡ç‚¹äº‘
                dense_grasp=False,        # ğŸ”§ ä¿®å¤ï¼šå…³é—­å¯†é›†æ£€æµ‹ï¼Œæé«˜æŠ“å–è´¨é‡
                collision_detection=True  # ğŸ”§ ä¿®å¤ï¼šå¯ç”¨ç¢°æ’æ£€æµ‹ï¼Œè¿‡æ»¤ä¸åˆç†çš„æŠ“å–
            )
            
            if len(gg) == 0:
                print(f"ç¯å¢ƒ{env_idx}: æœªæ£€æµ‹åˆ°æœ‰æ•ˆæŠ“å–")
                return None
            
            # 4. NMSå’Œæ’åº
            gg = gg.nms().sort_by_score()
            
            # 5. é€‰æ‹©Top-KæŠ“å–
            gg_top = gg[:min(top_k, len(gg))]
            
            # 6. è½¬æ¢æŠ“å–è¡¨ç¤º
            grasps = []
            for i in range(len(gg_top)):
                grasp = gg_top[i]
                
                # âœ… ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•æ„å»º4x4å˜æ¢çŸ©é˜µ
                # åŸºäºè°ƒè¯•ç»“æœï¼šgrasp.grasp_arrayæ˜¯17ç»´å‘é‡ï¼Œåº”è¯¥ä½¿ç”¨grasp.translationå’Œgrasp.rotation_matrix
                try:
                    if hasattr(grasp, 'translation') and hasattr(grasp, 'rotation_matrix'):
                        translation = grasp.translation  # [3] - æŠ“å–ä¸­å¿ƒä½ç½®
                        rotation = grasp.rotation_matrix  # [3, 3] - æ—‹è½¬çŸ©é˜µ
                        
                        # æ„å»º4x4å˜æ¢çŸ©é˜µï¼ˆç›¸æœºåæ ‡ç³»ï¼‰
                        grasp_pose_cam = np.eye(4, dtype=np.float64)
                        grasp_pose_cam[:3, :3] = rotation
                        grasp_pose_cam[:3, 3] = translation
                        
                        # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ä¿¡æ¯
                        if i == 0:  # åªä¸ºç¬¬ä¸€ä¸ªæŠ“å–æ‰“å°
                            print(f"ç¯å¢ƒ{env_idx}: âœ… æˆåŠŸæ„å»ºæŠ“å–çŸ©é˜µ")
                            print(f"  translation: {translation}")
                            print(f"  rotation shape: {rotation.shape}")
                            print(f"  æ„å»ºçš„4x4çŸ©é˜µå½¢çŠ¶: {grasp_pose_cam.shape}")
                    else:
                        print(f"ç¯å¢ƒ{env_idx}: âŒ æŠ“å–å¯¹è±¡ç¼ºå°‘translationæˆ–rotation_matrixå±æ€§")
                        continue
                        
                except Exception as e:
                    print(f"ç¯å¢ƒ{env_idx}: âŒ æ„å»ºæŠ“å–çŸ©é˜µå¤±è´¥: {e}")
                    continue
                
                # è·å–ç›¸æœºåˆ°ä¸–ç•Œçš„å˜æ¢
                if 'cam2world_gl' in sensor_params:
                    # ä½¿ç”¨cam2worldå˜æ¢çŸ©é˜µ
                    cam2world = sensor_params['cam2world_gl'][env_idx].cpu().numpy().astype(np.float64)
                    T_world_cam = cam2world
                else:
                    # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ç›¸æœºé…ç½®ä¸­çš„å›ºå®šä½å§¿
                    camera_config_pose = self._default_sensor_configs[0].pose
                    cam_pos = np.array(camera_config_pose.p, dtype=np.float64)
                    cam_quat = np.array(camera_config_pose.q, dtype=np.float64)
                    
                    # æ„å»ºç›¸æœºåˆ°ä¸–ç•Œçš„å˜æ¢çŸ©é˜µ
                    T_world_cam = np.eye(4, dtype=np.float64)
                    from scipy.spatial.transform import Rotation
                    R = Rotation.from_quat(cam_quat)  # [x,y,z,w]æ ¼å¼
                    T_world_cam[:3, :3] = R.as_matrix()
                    T_world_cam[:3, 3] = cam_pos
                
                # å°†æŠ“å–ä½å§¿è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
                try:
                    grasp_pose_world = T_world_cam @ grasp_pose_cam
                    # åªä¸ºç¬¬ä¸€ä¸ªæŠ“å–æ‰“å°æˆåŠŸä¿¡æ¯
                    if i == 0:
                        print(f"ç¯å¢ƒ{env_idx}: âœ… åæ ‡å˜æ¢æˆåŠŸï¼ŒçŸ©é˜µå½¢çŠ¶: {T_world_cam.shape} @ {grasp_pose_cam.shape} -> {grasp_pose_world.shape}")
                except Exception as matmul_error:
                    print(f"ç¯å¢ƒ{env_idx}: âŒ åæ ‡å˜æ¢å¤±è´¥: {matmul_error}")
                    # æš‚æ—¶ä½¿ç”¨ç›¸æœºåæ ‡ç³»ç»“æœ
                    grasp_pose_world = grasp_pose_cam
                    if i == 0:
                        print(f"ç¯å¢ƒ{env_idx}: ä½¿ç”¨ç›¸æœºåæ ‡ç³»ç»“æœ")
                
                grasps.append({
                    'pose': grasp_pose_world,
                    'score': float(grasp.score),
                    'width': float(grasp.width),
                    'translation': grasp_pose_world[:3, 3],  # æŠ“å–ä¸­å¿ƒä½ç½®
                    'rotation': grasp_pose_world[:3, :3],    # æŠ“å–å§¿æ€
                })
            
            # ğŸ”§ ä¿®å¤ï¼šåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­è¿›ä¸€æ­¥è¿‡æ»¤æ‰˜ç›˜åŒºåŸŸçš„æŠ“å–
            filtered_grasps = []
            tray_center = np.array([-0.2, 0.0, 0.006])  # æ‰˜ç›˜ä¸­å¿ƒä½ç½®ï¼ˆä¸–ç•Œåæ ‡ï¼‰
            tray_size = np.array([0.6, 0.6, 0.15])       # æ‰˜ç›˜å°ºå¯¸
            tray_safety_margin = 0.05  # 5cmå®‰å…¨è¾¹è·
            
            for grasp in grasps:
                grasp_pos = grasp['translation']
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ‰˜ç›˜å†…éƒ¨ï¼ˆè€ƒè™‘å®‰å…¨è¾¹è·ï¼‰
                relative_pos = np.abs(grasp_pos - tray_center)
                is_in_tray = (
                    relative_pos[0] <= (tray_size[0]/2 + tray_safety_margin) and
                    relative_pos[1] <= (tray_size[1]/2 + tray_safety_margin) and
                    grasp_pos[2] <= (tray_center[2] + tray_size[2] + tray_safety_margin)
                )
                
                if not is_in_tray:
                    filtered_grasps.append(grasp)
                else:
                    print(f"ç¯å¢ƒ{env_idx}: è¿‡æ»¤æ‰æ‰˜ç›˜å†…çš„æŠ“å–ç‚¹: [{grasp_pos[0]:.3f}, {grasp_pos[1]:.3f}, {grasp_pos[2]:.3f}], åˆ†æ•°={grasp['score']:.3f}")
            
            # ä½¿ç”¨è¿‡æ»¤åçš„æŠ“å–åˆ—è¡¨
            if not filtered_grasps:
                print(f"ç¯å¢ƒ{env_idx}: æ‰€æœ‰æŠ“å–éƒ½åœ¨æ‰˜ç›˜åŒºåŸŸå†…ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                filtered_grasps = grasps  # å¦‚æœå…¨éƒ¨è¢«è¿‡æ»¤ï¼Œåˆ™ä½¿ç”¨åŸå§‹ç»“æœ
            else:
                grasps = filtered_grasps
                print(f"ç¯å¢ƒ{env_idx}: æ‰˜ç›˜è¿‡æ»¤åå‰©ä½™{len(grasps)}ä¸ªæŠ“å–å€™é€‰")
            
            print(f"ç¯å¢ƒ{env_idx}: æœ€ç»ˆæ£€æµ‹åˆ°{len(grasps)}ä¸ªæŠ“å–å€™é€‰")
            if grasps:
                print(f"æœ€ä½³æŠ“å–åˆ†æ•°: {grasps[0]['score']:.3f}")
                print(f"æœ€ä½³æŠ“å–ä½ç½®: [{grasps[0]['translation'][0]:.3f}, {grasps[0]['translation'][1]:.3f}, {grasps[0]['translation'][2]:.3f}]")
            
            # ğŸ¨ å¯è§†åŒ–åŠŸèƒ½
            if visualize:
                # æ™ºèƒ½é€‰æ‹©å¯è§†åŒ–æ–¹å¼
                try:
                    # æ£€æµ‹æ˜¯å¦æœ‰å›¾å½¢ç•Œé¢ç¯å¢ƒ
                    if os.environ.get('DISPLAY') is None and os.environ.get('WAYLAND_DISPLAY') is None:
                        print("ğŸ” æ£€æµ‹åˆ°æ— å›¾å½¢ç•Œé¢ç¯å¢ƒï¼Œä½¿ç”¨matplotlibå¯è§†åŒ–")
                        self._visualize_grasps_matplotlib(points, colors, gg_top, f"grasp_env_{env_idx}_{target_obj.name}.png")
                    else:
                        print("ğŸ” å°è¯•Open3D 3Då¯è§†åŒ–...")
                        self._visualize_grasps(points, colors, gg_top, f"ç¯å¢ƒ{env_idx}æŠ“å–æ£€æµ‹ç»“æœ")
                except Exception as viz_error:
                    print(f"Open3Då¯è§†åŒ–å¤±è´¥: {viz_error}")
                    print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°matplotlibå¯è§†åŒ–...")
                    self._visualize_grasps_matplotlib(points, colors, gg_top, f"grasp_env_{env_idx}_{target_obj.name}.png")
            
            # ğŸ¬ åœ¨ä»¿çœŸç¯å¢ƒä¸­å¯è§†åŒ–æŠ“å–
            if visualize_in_env:
                try:
                    self._visualize_grasps_in_simulation(grasps, target_obj, env_idx)
                except Exception as env_viz_error:
                    print(f"ç¯å¢ƒå¯è§†åŒ–å¤±è´¥: {env_viz_error}")
            
            return grasps
            
        except Exception as e:
            print(f"ç¯å¢ƒ{env_idx}: æŠ“å–æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _visualize_grasps(self, points: np.ndarray, colors: np.ndarray, 
                          grasps, title: str = "æŠ“å–æ£€æµ‹ç»“æœ"):
        """
        ä½¿ç”¨Open3Då¯è§†åŒ–æŠ“å–ç»“æœ
        
        Args:
            points: ç‚¹äº‘åæ ‡ [N, 3]
            colors: ç‚¹äº‘é¢œè‰² [N, 3] 
            grasps: GraspGroupå¯¹è±¡æˆ–æŠ“å–åˆ—è¡¨
            title: å¯è§†åŒ–çª—å£æ ‡é¢˜
        """
        try:
            import open3d as o3d
            print(f"ğŸ¨ å¼€å§‹å¯è§†åŒ–: {title}")
            
            # 1. åˆ›å»ºç‚¹äº‘å¯¹è±¡
            pcd = o3d.geometry.PointCloud()
            pcd.points = o3d.utility.Vector3dVector(points.astype(np.float64))
            pcd.colors = o3d.utility.Vector3dVector(colors.astype(np.float64))
            
            print(f"ç‚¹äº‘åŒ…å« {len(points)} ä¸ªç‚¹")
            
            # 2. è·å–æŠ“å–å‡ ä½•å¯¹è±¡
            if hasattr(grasps, 'to_open3d_geometry_list'):
                # GraspGroupå¯¹è±¡
                grippers = grasps.to_open3d_geometry_list()
                print(f"ç”Ÿæˆäº† {len(grippers)} ä¸ªæŠ“å–å™¨å‡ ä½•å¯¹è±¡")
            else:
                print("âŒ æ— æ³•ä»æŠ“å–å¯¹è±¡ç”Ÿæˆå‡ ä½•å¯¹è±¡")
                return
            
            # 3. åæ ‡å˜æ¢ï¼ˆä¸demo.pyä¿æŒä¸€è‡´ï¼‰
            # ç¿»è½¬Zè½´ä»¥é€‚åº”å¯è§†åŒ–åæ ‡ç³»
            trans_mat = np.array([
                [1, 0, 0, 0],
                [0, 1, 0, 0], 
                [0, 0, -1, 0],
                [0, 0, 0, 1]
            ], dtype=np.float64)
            
            pcd.transform(trans_mat)
            for gripper in grippers:
                gripper.transform(trans_mat)
            
            # 4. å¯è§†åŒ–é€‰é¡¹
            print("ğŸ¨ æ˜¾ç¤ºå¯è§†åŒ–çª—å£...")
            print("æ“ä½œæç¤º:")
            print("  - é¼ æ ‡å·¦é”®æ‹–æ‹½: æ—‹è½¬è§†è§’")
            print("  - é¼ æ ‡å³é”®æ‹–æ‹½: å¹³ç§»è§†è§’") 
            print("  - æ»šè½®: ç¼©æ”¾")
            print("  - æŒ‰Qæˆ–å…³é—­çª—å£: é€€å‡º")
            
            # å°è¯•æ˜¾ç¤ºæ‰€æœ‰æŠ“å–
            print(f"æ˜¾ç¤ºç‚¹äº‘ + æ‰€æœ‰{len(grippers)}ä¸ªæŠ“å–å€™é€‰")
            try:
                success = o3d.visualization.draw_geometries(
                    [pcd] + grippers,
                    window_name=f"{title} - æ‰€æœ‰æŠ“å–",
                    width=800,
                    height=600
                )
                
                # æ˜¾ç¤ºæœ€ä½³æŠ“å–ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if len(grippers) > 0:
                    print("æ˜¾ç¤ºç‚¹äº‘ + æœ€ä½³æŠ“å–")
                    o3d.visualization.draw_geometries(
                        [pcd, grippers[0]],
                        window_name=f"{title} - æœ€ä½³æŠ“å–",
                        width=800,
                        height=600
                    )
                
                print("âœ… å¯è§†åŒ–å®Œæˆ")
                
            except Exception as display_error:
                print(f"Open3Dçª—å£æ˜¾ç¤ºå¤±è´¥: {display_error}")
                # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºå¯è§†åŒ–å¤±è´¥ä¸åº”è¯¥å½±å“ä¸»è¦åŠŸèƒ½
                print("âš ï¸ Open3Då¯è§†åŒ–å¤±è´¥ï¼Œä½†æ£€æµ‹åŠŸèƒ½æ­£å¸¸")
            
        except ImportError:
            print("âŒ éœ€è¦å®‰è£…Open3Dæ‰èƒ½ä½¿ç”¨å¯è§†åŒ–åŠŸèƒ½: pip install open3d")
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _visualize_grasps_matplotlib(self, points: np.ndarray, colors: np.ndarray, 
                                   grasps, filename: str = "grasp_result.png"):
        """
        ä½¿ç”¨matplotlibç”Ÿæˆ2DæŠ“å–å¯è§†åŒ–ï¼ˆæœåŠ¡å™¨å‹å¥½ï¼‰
        
        Args:
            points: ç‚¹äº‘åæ ‡ [N, 3]
            colors: ç‚¹äº‘é¢œè‰² [N, 3] 
            grasps: GraspGroupå¯¹è±¡æˆ–æŠ“å–åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        try:
            import matplotlib.pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D
            import matplotlib
            matplotlib.use('Agg')  # æ— ç•Œé¢åç«¯
            
            print(f"ğŸ¨ ä½¿ç”¨matplotlibç”Ÿæˆå¯è§†åŒ–: {filename}")
            
            # æå–æŠ“å–ä¿¡æ¯
            grasp_positions = []
            grasp_scores = []
            for i in range(len(grasps)):
                grasp = grasps[i]
                if hasattr(grasp, 'translation') and hasattr(grasp, 'score'):
                    pos = grasp.translation
                    grasp_positions.append(pos)
                    grasp_scores.append(grasp.score)
            
            if not grasp_positions:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æŠ“å–æ•°æ®")
                return
            
            grasp_positions = np.array(grasp_positions)
            grasp_scores = np.array(grasp_scores)
            
            # åˆ›å»ºå¤šå­å›¾
            fig = plt.figure(figsize=(12, 8))
            
            # 3Dç‚¹äº‘+æŠ“å–ç‚¹
            ax1 = fig.add_subplot(2, 2, 1, projection='3d')
            ax1.scatter(points[:, 0], points[:, 1], points[:, 2], 
                       c=colors, s=2, alpha=0.6, label='Point Cloud')
            ax1.scatter(grasp_positions[:, 0], grasp_positions[:, 1], grasp_positions[:, 2],
                       c=grasp_scores, s=50, cmap='viridis', marker='^', alpha=0.8, label='Grasps')
            ax1.set_title('3D Point Cloud + Grasp Candidates')
            ax1.set_xlabel('X (m)')
            ax1.set_ylabel('Y (m)')
            ax1.set_zlabel('Z (m)')
            ax1.legend()
            
            # XYå¹³é¢è§†å›¾
            ax2 = fig.add_subplot(2, 2, 2)
            ax2.scatter(points[:, 0], points[:, 1], c=colors[:, 0], s=5, alpha=0.6, label='Points')
            scatter = ax2.scatter(grasp_positions[:, 0], grasp_positions[:, 1], 
                                c=grasp_scores, s=80, cmap='viridis', marker='^', alpha=0.8)
            best_idx = np.argmax(grasp_scores)
            ax2.scatter(grasp_positions[best_idx, 0], grasp_positions[best_idx, 1], 
                       s=150, facecolors='none', edgecolors='red', linewidth=2, label='Best')
            plt.colorbar(scatter, ax=ax2, label='Score')
            ax2.set_title('XY Plane View')
            ax2.set_xlabel('X (m)')
            ax2.set_ylabel('Y (m)')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # æŠ“å–åˆ†æ•°æŸ±çŠ¶å›¾
            ax3 = fig.add_subplot(2, 2, 3)
            bars = ax3.bar(range(len(grasp_scores)), grasp_scores, color='skyblue', alpha=0.7)
            bars[best_idx].set_color('red')  # é«˜äº®æœ€ä½³æŠ“å–
            ax3.axhline(y=np.mean(grasp_scores), color='orange', linestyle='--', 
                       label=f'Mean: {np.mean(grasp_scores):.4f}')
            ax3.set_title('Grasp Quality Distribution')
            ax3.set_xlabel('Grasp Index')
            ax3.set_ylabel('Score')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
            ax4 = fig.add_subplot(2, 2, 4)
            ax4.axis('off')
            
            stats_text = f"""
æŠ“å–æ£€æµ‹ç»Ÿè®¡:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ç‚¹äº‘å¤§å°: {len(points)} ä¸ªç‚¹
â€¢ æ£€æµ‹æŠ“å–: {len(grasps)} ä¸ª
â€¢ æœ€ä½³åˆ†æ•°: {np.max(grasp_scores):.4f}
â€¢ å¹³å‡åˆ†æ•°: {np.mean(grasp_scores):.4f}
â€¢ åˆ†æ•°æ ‡å‡†å·®: {np.std(grasp_scores):.4f}

æœ€ä½³æŠ“å–ä½ç½®:
â€¢ X: {grasp_positions[best_idx, 0]:.3f} m
â€¢ Y: {grasp_positions[best_idx, 1]:.3f} m  
â€¢ Z: {grasp_positions[best_idx, 2]:.3f} m

ç‚¹äº‘èŒƒå›´:
â€¢ X: [{points[:, 0].min():.3f}, {points[:, 0].max():.3f}]
â€¢ Y: [{points[:, 1].min():.3f}, {points[:, 1].max():.3f}]
â€¢ Z: [{points[:, 2].min():.3f}, {points[:, 2].max():.3f}]
"""
            
            ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=10,
                    verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print(f"âœ… matplotlibå¯è§†åŒ–å·²ä¿å­˜: {filename}")
            
        except ImportError:
            print("âŒ éœ€è¦å®‰è£…matplotlibæ‰èƒ½ä½¿ç”¨å¤‡ç”¨å¯è§†åŒ–åŠŸèƒ½")
        except Exception as e:
            print(f"âŒ matplotlibå¯è§†åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _visualize_grasps_in_simulation(self, grasps: List[Dict], target_obj: Actor, env_idx: int = 0):
        """
        åœ¨ä»¿çœŸç¯å¢ƒæ¸²æŸ“å›¾åƒä¸­å¯è§†åŒ–æŠ“å–ä½å§¿
        
        Args:
            grasps: æŠ“å–å€™é€‰åˆ—è¡¨
            target_obj: ç›®æ ‡ç‰©ä½“
            env_idx: ç¯å¢ƒç´¢å¼•
        """
        try:
            import cv2
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            print(f"ğŸ¬ åœ¨ä»¿çœŸç¯å¢ƒä¸­å¯è§†åŒ–æŠ“å–...")
            
            # 1. è·å–ç¯å¢ƒæ¸²æŸ“å›¾åƒ
            env_image = self.render_rgb_array()  # è·å–ç¯å¢ƒRGBå›¾åƒ
            if env_image is None:
                print("âŒ æ— æ³•è·å–ç¯å¢ƒæ¸²æŸ“å›¾åƒ")
                return
            
            # å¦‚æœæ˜¯å¤šç¯å¢ƒï¼Œé€‰æ‹©å¯¹åº”ç¯å¢ƒçš„å›¾åƒ
            if len(env_image.shape) == 4:  # [num_envs, H, W, C]
                if env_idx < env_image.shape[0]:
                    env_image = env_image[env_idx]
                else:
                    env_image = env_image[0]
                    print(f"âš ï¸ ç¯å¢ƒç´¢å¼•{env_idx}è¶Šç•Œï¼Œä½¿ç”¨ç¯å¢ƒ0çš„å›¾åƒ")
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„å’Œ0-255èŒƒå›´çš„uint8æ ¼å¼
            if isinstance(env_image, torch.Tensor):
                env_image = env_image.cpu().numpy()
            
            if env_image.dtype == np.float32 or env_image.dtype == np.float64:
                if env_image.max() <= 1.0:
                    env_image = (env_image * 255).astype(np.uint8)
                else:
                    env_image = env_image.astype(np.uint8)
            
            print(f"ç¯å¢ƒå›¾åƒå½¢çŠ¶: {env_image.shape}, æ•°æ®ç±»å‹: {env_image.dtype}")
            
            # 2. è·å–ç›¸æœºå‚æ•°
            camera_obs = self._get_camera_observations("base_camera")
            if camera_obs is None or "base_camera" not in camera_obs['sensor_param']:
                print("âŒ æ— æ³•è·å–ç›¸æœºå‚æ•°")
                return
            
            sensor_params = camera_obs['sensor_param']["base_camera"]
            
            # 3. åœ¨å›¾åƒä¸Šç»˜åˆ¶æŠ“å–
            annotated_image = self._draw_grasps_on_image(env_image.copy(), grasps, sensor_params, env_idx)
            
            # 4. ä¿å­˜ç»“æœ
            output_filename = f"grasp_simulation_env_{env_idx}_{target_obj.name}.png"
            
            # ä½¿ç”¨matplotlibä¿å­˜ï¼ˆæ›´å¥½çš„è´¨é‡æ§åˆ¶ï¼‰
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # åŸå§‹ç¯å¢ƒå›¾åƒ
            ax1.imshow(env_image)
            ax1.set_title('Original Environment View')
            ax1.axis('off')
            
            # å¸¦æŠ“å–æ ‡æ³¨çš„å›¾åƒ
            ax2.imshow(annotated_image)
            ax2.set_title(f'Grasp Candidates ({len(grasps)} detected)')
            ax2.axis('off')
            
            plt.tight_layout()
            plt.savefig(output_filename, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ç¯å¢ƒå¯è§†åŒ–å·²ä¿å­˜: {output_filename}")
            
            # 5. ä¿å­˜æŠ“å–ä¿¡æ¯
            info_filename = output_filename.replace('.png', '_info.txt')
            with open(info_filename, 'w', encoding='utf-8') as f:
                f.write(f"ç¯å¢ƒæŠ“å–å¯è§†åŒ–æŠ¥å‘Š\n")
                f.write(f"========================\n")
                f.write(f"ç›®æ ‡ç‰©ä½“: {target_obj.name}\n")
                f.write(f"ç¯å¢ƒç´¢å¼•: {env_idx}\n")
                f.write(f"å›¾åƒå°ºå¯¸: {env_image.shape}\n")
                f.write(f"æ£€æµ‹åˆ°æŠ“å–æ•°é‡: {len(grasps)}\n\n")
                
                for i, grasp in enumerate(grasps):
                    f.write(f"æŠ“å– {i+1}:\n")
                    f.write(f"  åˆ†æ•°: {grasp['score']:.4f}\n")
                    f.write(f"  ä½ç½®: [{grasp['translation'][0]:.3f}, {grasp['translation'][1]:.3f}, {grasp['translation'][2]:.3f}]\n")
                    f.write(f"  å®½åº¦: {grasp['width']:.3f}m\n\n")
            
            print(f"ğŸ“‹ æŠ“å–ä¿¡æ¯å·²ä¿å­˜: {info_filename}")
            
        except ImportError as e:
            print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
            print("éœ€è¦å®‰è£…: pip install opencv-python")
        except Exception as e:
            print(f"âŒ ç¯å¢ƒå¯è§†åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _draw_grasps_on_image(self, image: np.ndarray, grasps: List[Dict], 
                             sensor_params: Dict, env_idx: int = 0) -> np.ndarray:
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶æŠ“å–ä½å§¿
        
        Args:
            image: ç¯å¢ƒå›¾åƒ [H, W, 3]
            grasps: æŠ“å–å€™é€‰åˆ—è¡¨
            sensor_params: ç›¸æœºå‚æ•°
            env_idx: ç¯å¢ƒç´¢å¼•
            
        Returns:
            å¸¦æŠ“å–æ ‡æ³¨çš„å›¾åƒ
        """
        try:
            import cv2
            
            # è·å–ç›¸æœºå†…å‚
            if 'intrinsic_cv' in sensor_params:
                intrinsic = sensor_params['intrinsic_cv'][env_idx].cpu().numpy()
                fx, fy = intrinsic[0, 0], intrinsic[1, 1]
                cx, cy = intrinsic[0, 2], intrinsic[1, 2]
            else:
                # ä½¿ç”¨é»˜è®¤å†…å‚
                H, W = image.shape[:2]
                fx = fy = W / 2.0
                cx, cy = W / 2.0, H / 2.0
            
            # è·å–ç›¸æœºä½å§¿ï¼ˆä¸–ç•Œåˆ°ç›¸æœºçš„å˜æ¢ï¼‰
            if 'cam2world_gl' in sensor_params:
                cam2world = sensor_params['cam2world_gl'][env_idx].cpu().numpy()
                world2cam = np.linalg.inv(cam2world)
            else:
                # ä½¿ç”¨å•ä½çŸ©é˜µï¼ˆå‡è®¾ç›¸æœºåœ¨ä¸–ç•ŒåŸç‚¹ï¼‰
                world2cam = np.eye(4)
            
            annotated_image = image.copy()
            colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]  # BGRæ ¼å¼
            
            for i, grasp in enumerate(grasps):
                try:
                    # è·å–æŠ“å–ä½ç½®ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
                    world_pos = np.array([grasp['translation'][0], grasp['translation'][1], grasp['translation'][2], 1.0])
                    
                    # è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»
                    cam_pos = world2cam @ world_pos
                    x_cam, y_cam, z_cam = cam_pos[:3]
                    
                    # æŠ•å½±åˆ°å›¾åƒå¹³é¢
                    if z_cam > 0:  # ç¡®ä¿åœ¨ç›¸æœºå‰æ–¹
                        u = int(fx * x_cam / z_cam + cx)
                        v = int(fy * y_cam / z_cam + cy)
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…
                        if 0 <= u < image.shape[1] and 0 <= v < image.shape[0]:
                            color = colors[i % len(colors)]
                            
                            # ç»˜åˆ¶æŠ“å–ç‚¹
                            radius = max(5, int(20 * grasp['score'] / 0.1))  # æ ¹æ®åˆ†æ•°è°ƒæ•´å¤§å°
                            cv2.circle(annotated_image, (u, v), radius, color, -1)
                            
                            # ç»˜åˆ¶æŠ“å–è½®å»“ï¼ˆç®€åŒ–çš„å¤¹çˆªè¡¨ç¤ºï¼‰
                            gripper_size = int(grasp['width'] * 1000)  # è½¬æ¢ä¸ºåƒç´ 
                            gripper_size = max(10, min(50, gripper_size))  # é™åˆ¶å¤§å°
                            
                            # ç»˜åˆ¶å¤¹çˆªè½®å»“
                            cv2.rectangle(annotated_image, 
                                        (u - gripper_size//2, v - gripper_size//2),
                                        (u + gripper_size//2, v + gripper_size//2),
                                        color, 2)
                            
                            # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
                            label = f"G{i+1}: {grasp['score']:.3f}"
                            cv2.putText(annotated_image, label, (u + 10, v - 10),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                            
                            print(f"  æŠ“å–{i+1}: ä¸–ç•Œ({world_pos[:3]}) -> ç›¸æœº({x_cam:.2f},{y_cam:.2f},{z_cam:.2f}) -> å›¾åƒ({u},{v})")
                    else:
                        print(f"  æŠ“å–{i+1}: åœ¨ç›¸æœºåæ–¹ï¼Œè·³è¿‡ (z_cam={z_cam:.2f})")
                        
                except Exception as draw_error:
                    print(f"ç»˜åˆ¶æŠ“å–{i+1}å¤±è´¥: {draw_error}")
                    continue
            
            # æ·»åŠ å›¾ä¾‹
            legend_y = 30
            for i, grasp in enumerate(grasps[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                color = colors[i % len(colors)]
                cv2.rectangle(annotated_image, (10, legend_y + i*25), (30, legend_y + i*25 + 15), color, -1)
                text = f"Grasp {i+1}: Score {grasp['score']:.3f}"
                cv2.putText(annotated_image, text, (35, legend_y + i*25 + 12),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            return annotated_image
            
        except Exception as e:
            print(f"âŒ å›¾åƒæ ‡æ³¨å¤±è´¥: {e}")
            return image