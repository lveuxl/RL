#!/usr/bin/env python3
"""
æµ‹è¯•ä¼˜åŒ–åçš„ç¯å¢ƒå’Œè®­ç»ƒ
éªŒè¯æ”¶æ•›æ€§å’Œè®­ç»ƒé€Ÿåº¦
"""

import os
import time
import numpy as np
import torch
import gymnasium as gym
from datetime import datetime

# æ³¨å†Œç¯å¢ƒ
from env_clutter_optimized import EnvClutterOptimizedEnv
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
from mani_skill.utils.wrappers.record import RecordEpisode
import mani_skill.envs


def test_environment_speed():
    """æµ‹è¯•ç¯å¢ƒæ‰§è¡Œé€Ÿåº¦"""
    print("="*60)
    print("âš¡ ç¯å¢ƒé€Ÿåº¦æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºä¸åŒæ•°é‡çš„å¹¶è¡Œç¯å¢ƒæµ‹è¯•é€Ÿåº¦
    env_counts = [1, 4, 16, 64, 128]
    
    for num_envs in env_counts:
        print(f"\næµ‹è¯• {num_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ...")
        
        # åˆ›å»ºç¯å¢ƒ
        env = gym.make(
            "EnvClutterOptimized-v1",
            num_envs=num_envs,
            obs_mode="state",
            control_mode="pd_ee_delta_pose",
            reward_mode="dense",
            sim_backend="gpu",
            render_mode=None,
        )
        
        env = ManiSkillVectorEnv(env, 1, ignore_terminations=False)
        
        # æ‰§è¡Œé€Ÿåº¦æµ‹è¯•
        obs, _ = env.reset()
        
        start_time = time.time()
        total_steps = 100
        
        for _ in range(total_steps):
            # éšæœºåŠ¨ä½œ
            actions = np.random.randint(0, 9, size=num_envs)
            obs, reward, done, truncated, info = env.step(actions)
            
            if done.any():
                obs, _ = env.reset()
        
        elapsed_time = time.time() - start_time
        steps_per_second = total_steps / elapsed_time
        
        print(f"  âœ… {num_envs}ä¸ªç¯å¢ƒ: {steps_per_second:.1f} steps/ç§’")
        print(f"     æ¯æ­¥å¹³å‡æ—¶é—´: {elapsed_time/total_steps*1000:.1f}ms")
        
        env.close()
    
    print("\nğŸ’¡ å»ºè®®ï¼šä½¿ç”¨128-256ä¸ªå¹¶è¡Œç¯å¢ƒä»¥è·å¾—æœ€ä½³è®­ç»ƒé€Ÿåº¦")


def test_reward_structure():
    """æµ‹è¯•å¥–åŠ±ç»“æ„çš„åˆç†æ€§"""
    print("\n" + "="*60)
    print("ğŸ’° å¥–åŠ±ç»“æ„æµ‹è¯•")
    print("="*60)
    
    env = gym.make(
        "EnvClutterOptimized-v1",
        num_envs=1,
        obs_mode="state",
        control_mode="pd_ee_delta_pose",
        reward_mode="dense",
        sim_backend="gpu",
        render_mode=None,
    )
    
    env = ManiSkillVectorEnv(env, 1, ignore_terminations=False)
    
    # æµ‹è¯•ä¸åŒç­–ç•¥çš„å¥–åŠ±
    strategies = {
        "è‡ªä¸Šè€Œä¸‹": [6, 7, 8, 3, 4, 5, 0, 1, 2],  # ç†æƒ³ç­–ç•¥
        "è‡ªä¸‹è€Œä¸Š": [0, 1, 2, 3, 4, 5, 6, 7, 8],  # é”™è¯¯ç­–ç•¥
        "éšæœº": np.random.permutation(9).tolist(),  # éšæœºç­–ç•¥
    }
    
    for strategy_name, action_sequence in strategies.items():
        print(f"\næµ‹è¯•ç­–ç•¥: {strategy_name}")
        print(f"  åŠ¨ä½œåºåˆ—: {action_sequence}")
        
        obs, _ = env.reset()
        total_reward = 0
        rewards = []
        
        for i, action in enumerate(action_sequence):
            obs, reward, done, truncated, info = env.step([action])
            reward_value = reward.item() if hasattr(reward, 'item') else float(reward)
            rewards.append(reward_value)
            total_reward += reward_value
            
            if done or truncated:
                break
        
        print(f"  æ€»å¥–åŠ±: {total_reward:.2f}")
        print(f"  å„æ­¥å¥–åŠ±: {[f'{r:.1f}' for r in rewards]}")
        
        # åˆ†æå¥–åŠ±æ¨¡å¼
        if strategy_name == "è‡ªä¸Šè€Œä¸‹":
            assert total_reward > 0, "è‡ªä¸Šè€Œä¸‹ç­–ç•¥åº”è·å¾—æ­£å¥–åŠ±"
            print("  âœ… è‡ªä¸Šè€Œä¸‹ç­–ç•¥è·å¾—æœ€é«˜å¥–åŠ±")
    
    env.close()


def test_convergence_guarantee():
    """æµ‹è¯•è®­ç»ƒæ”¶æ•›æ€§ä¿è¯"""
    print("\n" + "="*60)
    print("ğŸ“ˆ æ”¶æ•›æ€§æµ‹è¯•")
    print("="*60)
    
    from stable_baselines3 import PPO
    from mani_skill.vector.wrappers.sb3 import ManiSkillSB3VectorEnv
    
    # åˆ›å»ºå°æ‰¹é‡ç¯å¢ƒå¿«é€Ÿæµ‹è¯•
    env = gym.make(
        "EnvClutterOptimized-v1",
        num_envs=16,
        obs_mode="state",
        control_mode="pd_ee_delta_pose",
        reward_mode="dense",
        sim_backend="gpu",
        render_mode=None,
    )
    
    vec_env = ManiSkillSB3VectorEnv(env)
    
    # åˆ›å»ºPPOæ¨¡å‹ï¼ˆå¿«é€Ÿæ”¶æ•›å‚æ•°ï¼‰
    model = PPO(
        "MlpPolicy",
        vec_env,
        learning_rate=1e-3,  # è¾ƒé«˜å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
        n_steps=64,
        batch_size=256,
        n_epochs=4,
        gamma=0.95,
        gae_lambda=0.9,
        clip_range=0.2,
        ent_coef=0.02,  # è¾ƒé«˜ç†µç³»æ•°ä¿æŒæ¢ç´¢
        policy_kwargs={
            "net_arch": [64, 64],  # å°ç½‘ç»œå¿«é€Ÿè®­ç»ƒ
            "activation_fn": torch.nn.Tanh,
        },
        verbose=0,
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    
    print("å¼€å§‹å¿«é€Ÿæ”¶æ•›æµ‹è¯•ï¼ˆ1000æ­¥ï¼‰...")
    
    # è®°å½•è®­ç»ƒå‰åçš„æ€§èƒ½
    initial_rewards = []
    for _ in range(5):
        obs = vec_env.reset()
        done = False
        episode_reward = 0
        while not done:
            action = vec_env.action_space.sample()  # éšæœºåŠ¨ä½œ
            obs, reward, done, info = vec_env.step(action)
            episode_reward += reward
            if done.any():
                break
        initial_rewards.append(episode_reward.mean())
    
    initial_mean = np.mean(initial_rewards)
    print(f"  åˆå§‹éšæœºç­–ç•¥å¥–åŠ±: {initial_mean:.2f}")
    
    # å¿«é€Ÿè®­ç»ƒ
    start_time = time.time()
    model.learn(total_timesteps=1000, progress_bar=False)
    train_time = time.time() - start_time
    
    # æµ‹è¯•è®­ç»ƒåçš„æ€§èƒ½
    trained_rewards = []
    for _ in range(5):
        obs = vec_env.reset()
        done = False
        episode_reward = 0
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = vec_env.step(action)
            episode_reward += reward
            if done.any():
                break
        trained_rewards.append(episode_reward.mean())
    
    trained_mean = np.mean(trained_rewards)
    improvement = trained_mean - initial_mean
    
    print(f"  è®­ç»ƒåç­–ç•¥å¥–åŠ±: {trained_mean:.2f}")
    print(f"  æ€§èƒ½æå‡: {improvement:.2f} ({improvement/abs(initial_mean)*100:.1f}%)")
    print(f"  è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")
    print(f"  è®­ç»ƒé€Ÿåº¦: {1000/train_time:.0f} steps/ç§’")
    
    if improvement > 0:
        print("  âœ… æ¨¡å‹è¡¨ç°å‡ºå­¦ä¹ èƒ½åŠ›ï¼Œæ”¶æ•›æ€§å¾—åˆ°éªŒè¯")
    else:
        print("  âš ï¸ éœ€è¦æ›´å¤šè®­ç»ƒæ­¥æ•°æˆ–å‚æ•°è°ƒæ•´")
    
    vec_env.close()


def test_with_video():
    """æµ‹è¯•å¹¶å½•åˆ¶è§†é¢‘"""
    print("\n" + "="*60)
    print("ğŸ¥ è§†é¢‘å½•åˆ¶æµ‹è¯•")
    print("="*60)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    video_dir = f"test_videos/optimized_{timestamp}"
    os.makedirs(video_dir, exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(
        "EnvClutterOptimized-v1",
        num_envs=1,
        obs_mode="state",
        control_mode="pd_ee_delta_pose",
        reward_mode="dense",
        sim_backend="gpu",
        render_mode="rgb_array",
    )
    
    # æ·»åŠ è§†é¢‘å½•åˆ¶
    env = RecordEpisode(
        env,
        output_dir=video_dir,
        save_video=True,
        trajectory_name="optimized_test",
        max_steps_per_video=100,
        video_fps=30,
    )
    
    env = ManiSkillVectorEnv(env, 1, ignore_terminations=False)
    
    print(f"å½•åˆ¶è‡ªä¸Šè€Œä¸‹æŠ“å–ç­–ç•¥...")
    
    # æ‰§è¡Œè‡ªä¸Šè€Œä¸‹ç­–ç•¥
    obs, _ = env.reset()
    action_sequence = [6, 7, 8, 3, 4, 5, 0, 1, 2]  # è‡ªä¸Šè€Œä¸‹é¡ºåº
    
    for i, action in enumerate(action_sequence):
        print(f"  æ‰§è¡ŒåŠ¨ä½œ {i+1}/9: æŠ“å–ç‰©ä½“ {action}")
        obs, reward, done, truncated, info = env.step([action])
        
        if done or truncated:
            print("  Episodeç»“æŸ")
            break
    
    env.close()
    print(f"\nâœ… è§†é¢‘å·²ä¿å­˜åˆ°: {video_dir}")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸš€ " + "="*58 + " ğŸš€")
    print("      ä¼˜åŒ–ç¯å¢ƒå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("ğŸš€ " + "="*58 + " ğŸš€\n")
    
    # è¿è¡Œæµ‹è¯•
    test_environment_speed()
    test_reward_structure()
    test_convergence_guarantee()
    test_with_video()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("="*60)
    print("\nå»ºè®®:")
    print("1. ä½¿ç”¨ train_optimized.py å¼€å§‹è®­ç»ƒ")
    print("2. ç›‘æ§ tensorboard --logdir logs/optimized_training")
    print("3. é¢„æœŸ 2-3 å°æ—¶å†…çœ‹åˆ°æ˜æ˜¾æ”¶æ•›")
    print("4. æˆåŠŸç‡åº”è¾¾åˆ° 90% ä»¥ä¸Š")


if __name__ == "__main__":
    main()