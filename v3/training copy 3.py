"""
ä½¿ç”¨stable-baselines3è®­ç»ƒEnvClutterç¯å¢ƒ
ç›®æ ‡ï¼šå­¦ä¹ æœ€ä¼˜çš„æŠ“å–é¡ºåº
"""

import os
import argparse
import numpy as np
import gymnasium as gym
import torch
import time

# å°è¯•å¯¼å…¥MaskablePPOï¼Œå¦‚æœæ²¡æœ‰å®‰è£…sb3-contribåˆ™ä½¿ç”¨æ™®é€šPPO
try:
    from sb3_contrib import MaskablePPO
    from sb3_contrib.common.wrappers import ActionMasker
    MASKABLE_AVAILABLE = True
    print("âœ… ä½¿ç”¨MaskablePPOè¿›è¡ŒåŠ¨ä½œæ©ç è®­ç»ƒ")
except ImportError:
    from stable_baselines3 import PPO
    MASKABLE_AVAILABLE = False
    print("âš ï¸ æœªæ£€æµ‹åˆ°sb3-contribï¼Œå°†ä½¿ç”¨æ™®é€šPPOï¼ˆå»ºè®®å®‰è£…: pip install sb3-contribï¼‰")

from stable_baselines3.common.callbacks import (
    EvalCallback, CheckpointCallback, BaseCallback, CallbackList
)
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import configure_logger
from stable_baselines3.common.logger import TensorBoardOutputFormat
import json

# å¯¼å…¥æ”¯æŒæŠ“å–é¡ºåºå­¦ä¹ çš„ç¯å¢ƒç‰ˆæœ¬
from env_clutter import EnvClutterEnv  # æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒuse_ideal_oracle
print("âœ… ä½¿ç”¨env_clutterç¯å¢ƒï¼ˆæŠ“å–é¡ºåºå­¦ä¹ ï¼‰")
HAS_IDEAL_ORACLE = True

from mani_skill.vector.wrappers.sb3 import ManiSkillSB3VectorEnv
from mani_skill.utils.wrappers.record import RecordEpisode
import mani_skill.envs

from wrappers.mask_wrapper import ExtractMaskWrapper, SB3CompatWrapper, ActionConversionWrapper
from wrappers.maskable_wrapper import create_maskable_env, MaskableVectorEnvWrapper


class TrainingMonitorCallback(BaseCallback):
    """
    è®­ç»ƒç›‘æ§å›è°ƒ - å®æ—¶ç›‘æ§è®­ç»ƒæŒ‡æ ‡å’Œæ¢¯åº¦ä¿¡æ¯
    """
    def __init__(self, check_freq: int = 1000, verbose: int = 0):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_successes = []
        self.best_mean_reward = -np.inf
        
    def _init_callback(self) -> None:
        # Create logs dir if needed
        if self.logger.get_dir() is not None:
            self.log_dir = self.logger.get_dir()
        else:
            self.log_dir = None
    
    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                # ä»episodeä¿¡æ¯ç¼“å†²åŒºè·å–ç»Ÿè®¡æ•°æ®
                rewards = [ep['r'] for ep in self.model.ep_info_buffer]
                lengths = [ep['l'] for ep in self.model.ep_info_buffer]
                
                if rewards:
                    mean_reward = np.mean(rewards)
                    mean_length = np.mean(lengths)
                    
                    # è®°å½•åˆ°TensorBoard
                    self.logger.record("train/mean_episode_reward", mean_reward)
                    self.logger.record("train/mean_episode_length", mean_length)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ€§èƒ½
                    if mean_reward > self.best_mean_reward:
                        self.best_mean_reward = mean_reward
                        self.logger.record("train/best_mean_reward", self.best_mean_reward)
            
            # è®°å½•å­¦ä¹ ç‡
            if hasattr(self.model, 'learning_rate'):
                current_lr = self.model.learning_rate
                if callable(current_lr):
                    current_lr = current_lr(1.0)  # Get current learning rate
                self.logger.record("train/learning_rate", current_lr)
            
            # è®°å½•æ¢¯åº¦ç»Ÿè®¡
            if hasattr(self.model.policy, 'parameters'):
                total_norm = 0.0
                param_count = 0
                for param in self.model.policy.parameters():
                    if param.grad is not None:
                        param_norm = param.grad.data.norm(2)
                        total_norm += param_norm.item() ** 2
                        param_count += 1
                
                if param_count > 0:
                    total_norm = total_norm ** (1. / 2)
                    self.logger.record("train/gradient_norm", total_norm)
            
        return True


class GraspSequenceAnalysisCallback(BaseCallback):
    """
    æŠ“å–åºåˆ—åˆ†æå›è°ƒ - åˆ†æå­¦ä¹ åˆ°çš„æŠ“å–ç­–ç•¥
    """
    def __init__(self, eval_env, check_freq: int = 50000, n_eval_episodes: int = 5, verbose: int = 0):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.check_freq = check_freq
        self.n_eval_episodes = n_eval_episodes
        
    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            self._analyze_grasp_sequences()
        return True
    
    def _analyze_grasp_sequences(self):
        """åˆ†ææŠ“å–åºåˆ—æ¨¡å¼"""
        try:
            sequences = []
            success_count = 0
            
            for episode in range(self.n_eval_episodes):
                obs = self.eval_env.reset()
                sequence = []
                episode_success = False
                
                for step in range(20):  # æœ€å¤š20æ­¥
                    action, _ = self.model.predict(obs, deterministic=True)
                    sequence.append(int(action[0]) if isinstance(action, np.ndarray) else int(action))
                    
                    obs, reward, done, info = self.eval_env.step(action)
                    
                    if done[0] if isinstance(done, np.ndarray) else done:
                        if isinstance(info, list) and len(info) > 0:
                            episode_success = info[0].get('success', False)
                        elif isinstance(info, dict):
                            episode_success = info.get('success', False)
                        break
                
                sequences.append(sequence[:9])  # åªè®°å½•å‰9ä¸ªåŠ¨ä½œ
                if episode_success:
                    success_count += 1
            
            # åˆ†æåºåˆ—ä¸€è‡´æ€§
            if sequences:
                unique_sequences = len(set(tuple(seq) for seq in sequences))
                sequence_consistency = 1.0 - (unique_sequences - 1) / max(len(sequences) - 1, 1)
                
                # è®°å½•åˆ†æç»“æœ
                self.logger.record("eval/sequence_consistency", sequence_consistency)
                self.logger.record("eval/unique_sequences", unique_sequences)
                self.logger.record("eval/success_rate", success_count / self.n_eval_episodes)
                
                # è®°å½•æœ€å¸¸è§çš„åºåˆ—
                from collections import Counter
                sequence_counter = Counter(tuple(seq) for seq in sequences)
                most_common_seq = sequence_counter.most_common(1)[0][0] if sequence_counter else []
                self.logger.record("eval/most_common_sequence_length", len(most_common_seq))
                
                if self.verbose >= 1:
                    print(f"\nğŸ¯ æŠ“å–åºåˆ—åˆ†æ (æ­¥æ•°: {self.n_calls}):")
                    print(f"  åºåˆ—ä¸€è‡´æ€§: {sequence_consistency:.2%}")
                    print(f"  ç‹¬ç‰¹åºåˆ—æ•°: {unique_sequences}")
                    print(f"  è¯„ä¼°æˆåŠŸç‡: {success_count/self.n_eval_episodes:.2%}")
                    if most_common_seq:
                        print(f"  æœ€å¸¸è§åºåˆ—: {list(most_common_seq)}")
                        
        except Exception as e:
            if self.verbose >= 1:
                print(f"æŠ“å–åºåˆ—åˆ†æå¤±è´¥: {e}")


def create_learning_rate_schedule(initial_lr: float = 1e-4, final_lr_ratio: float = 0.1):
    """
    åˆ›å»ºå­¦ä¹ ç‡è¡°å‡è°ƒåº¦å™¨
    """
    def lr_schedule(progress_remaining: float) -> float:
        """
        è¿›åº¦ä»1.0ï¼ˆå¼€å§‹ï¼‰åˆ°0.0ï¼ˆç»“æŸï¼‰
        """
        return initial_lr * (final_lr_ratio + (1.0 - final_lr_ratio) * progress_remaining)
    
    return lr_schedule


def create_env(env_id="EnvClutter-v1", num_envs=128, record_video=False, video_dir="./videos", **env_kwargs):
    """
    åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    """
    
    # æ„å»ºç¯å¢ƒå‚æ•°ï¼Œæ ¹æ®ç‰ˆæœ¬å†³å®šæ˜¯å¦åŒ…å«use_ideal_oracle
    env_params = {
        "num_envs": num_envs,
        "obs_mode": "state",
        "control_mode": "pd_ee_delta_pose", 
        "reward_mode": "dense",
        "sim_backend": "gpu",
        "render_mode": "rgb_array" if record_video else None,
        "use_discrete_action": True,  # å¯ç”¨ç¦»æ•£åŠ¨ä½œ
        **env_kwargs
    }
    
    # åªåœ¨æ”¯æŒçš„ç‰ˆæœ¬ä¸­æ·»åŠ use_ideal_oracleå‚æ•°
    if HAS_IDEAL_ORACLE:
        env_params["use_ideal_oracle"] = True  # ä½¿ç”¨ç†æƒ³åŒ–ç¥è°•æŠ“å–
    
    # åˆ›å»ºåŸå§‹ç¯å¢ƒï¼ˆç›´æ¥åˆ›å»ºå¤šç¯å¢ƒç‰ˆæœ¬ï¼‰
    env = gym.make(env_id, **env_params)
    
    # è·å–æœ€å¤§ç‰©ä½“æ•°é‡
    max_n = env.unwrapped.MAX_N if hasattr(env.unwrapped, 'MAX_N') else 9
    
    if MASKABLE_AVAILABLE:
        # ä½¿ç”¨MaskablePPOä¸“ç”¨åŒ…è£…å™¨
        print("ğŸ¯ é…ç½®MaskablePPOä¸“ç”¨ç¯å¢ƒåŒ…è£…...")
        env = SB3CompatWrapper(env)
        env = ExtractMaskWrapper(env, max_n=max_n)
        # è½¬æ¢ä¸ºSB3å‘é‡ç¯å¢ƒ
        vec_env = ManiSkillSB3VectorEnv(env)
        # æ·»åŠ MaskablePPOå‘é‡ç¯å¢ƒåŒ…è£…å™¨
        vec_env = MaskableVectorEnvWrapper(vec_env, max_n=max_n)
    else:
        # ä½¿ç”¨æ™®é€šPPOåŒ…è£…å™¨
        print("âš ï¸ é…ç½®æ™®é€šPPOç¯å¢ƒåŒ…è£…...")
        env = SB3CompatWrapper(env)
        env = ExtractMaskWrapper(env, max_n=max_n)
        env = ActionConversionWrapper(env)
        # è½¬æ¢ä¸ºSB3å‘é‡ç¯å¢ƒ
        vec_env = ManiSkillSB3VectorEnv(env)
    
    return vec_env


def create_eval_env(env_id="EnvClutter-v1", num_envs=16, record_video=False, video_dir="./videos", **env_kwargs):
    """
    åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    """
    
    # æ„å»ºç¯å¢ƒå‚æ•°ï¼Œæ ¹æ®ç‰ˆæœ¬å†³å®šæ˜¯å¦åŒ…å«use_ideal_oracle
    env_params = {
        "num_envs": num_envs,
        "obs_mode": "state",
        "control_mode": "pd_ee_delta_pose",
        "reward_mode": "dense", 
        "sim_backend": "gpu",
        "render_mode": "rgb_array" if record_video else None,
        "use_discrete_action": True,
        **env_kwargs
    }
    
    # åªåœ¨æ”¯æŒçš„ç‰ˆæœ¬ä¸­æ·»åŠ use_ideal_oracleå‚æ•°
    if HAS_IDEAL_ORACLE:
        env_params["use_ideal_oracle"] = True
    
    env = gym.make(env_id, **env_params)
    
    # åªæœ‰åœ¨è¯„ä¼°ç¯å¢ƒä¸”ç¯å¢ƒæ•°é‡è¾ƒå°‘æ—¶æ‰å½•åˆ¶è§†é¢‘
    if record_video and num_envs <= 4:
        timestamp = int(time.time())
        unique_trajectory_name = f"eval_trajectory_{timestamp}"
        
        env = RecordEpisode(
            env,
            output_dir=video_dir,
            save_video=True,
            trajectory_name=unique_trajectory_name,
            max_steps_per_video=2000,
            video_fps=30,
        )
    
    # è·å–æœ€å¤§ç‰©ä½“æ•°é‡
    max_n = env.unwrapped.MAX_N if hasattr(env.unwrapped, 'MAX_N') else 9
    
    if MASKABLE_AVAILABLE:
        # ä½¿ç”¨MaskablePPOä¸“ç”¨åŒ…è£…å™¨
        env = SB3CompatWrapper(env)
        env = ExtractMaskWrapper(env, max_n=max_n)
        vec_env = ManiSkillSB3VectorEnv(env)
        vec_env = MaskableVectorEnvWrapper(vec_env, max_n=max_n)
    else:
        # ä½¿ç”¨æ™®é€šPPOåŒ…è£…å™¨
        env = SB3CompatWrapper(env)
        env = ExtractMaskWrapper(env, max_n=max_n)
        env = ActionConversionWrapper(env)
        vec_env = ManiSkillSB3VectorEnv(env)
    
    return vec_env


def train_ppo(args):
    """
    è®­ç»ƒPPOæ™ºèƒ½ä½“å­¦ä¹ æœ€ä¼˜æŠ“å–é¡ºåº
    """
    print(f"å¼€å§‹è®­ç»ƒEnvClutterç¯å¢ƒ - ç›®æ ‡ï¼šå­¦ä¹ è‡ªä¸Šè€Œä¸‹çš„æŠ“å–é¡ºåº")
    print(f"ç¯å¢ƒç‰ˆæœ¬: {'æŠ“å–é¡ºåºå­¦ä¹ ç‰ˆæœ¬' if HAS_IDEAL_ORACLE else 'åŸºç¡€ç‰ˆæœ¬ï¼ˆåŠŸèƒ½å—é™ï¼‰'}")
    print(f"ç†æƒ³åŒ–ç¥è°•æŠ“å–: {'âœ…å¯ç”¨' if HAS_IDEAL_ORACLE else 'âŒæœªå¯ç”¨'}")
    print(f"å¹¶è¡Œç¯å¢ƒæ•°: {args.num_envs}")
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {args.total_timesteps}")
    print(f"æ¯å›åˆæŠ“å–æ¬¡æ•°: 9æ¬¡ï¼ˆå¯¹åº”9ä¸ªactionï¼‰")
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print("åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    vec_env = create_env(
        env_id="EnvClutter-v1",
        num_envs=args.num_envs,
        record_video=False,
        video_dir=os.path.join(args.log_dir, "train_videos"),
    )
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    print("åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    eval_env = create_eval_env(
        env_id="EnvClutter-v1", 
        num_envs=args.eval_envs,
        record_video=args.record_video,
        video_dir=os.path.join(args.log_dir, "eval_videos"),
    )
    
    # åˆ›å»ºPPOæ¨¡å‹ - æ”¯æŒåŠ¨ä½œæ©ç 
    print("åˆ›å»ºPPOæ¨¡å‹...")
    
    # ğŸš€ ç²¾è°ƒè¶…å‚æ•° - ä¸“é—¨ä¸ºè‡ªä¸Šè€Œä¸‹æŠ“å–é¡ºåºå­¦ä¹ ä¼˜åŒ–
    # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜é—®é¢˜
    total_batch_size = args.num_envs * 2048  # n_steps * n_envs
    optimal_batch_size = min(512, max(64, total_batch_size // 8))
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦
    lr_schedule = create_learning_rate_schedule(
        initial_lr=args.learning_rate, 
        final_lr_ratio=0.1
    )
    
    model_kwargs = {
        "gamma": 0.99,              # æ›´é«˜æŠ˜æ‰£å› å­ï¼Œå¼ºè°ƒé•¿æœŸç­–ç•¥
        "gae_lambda": 0.95,         # GAEå¹³æ»‘ä¼˜åŠ¿ä¼°è®¡
        "n_steps": 2048,            # å¢åŠ ç»éªŒæ”¶é›†ï¼Œæ›´å¥½å­¦ä¹ åºåˆ—å†³ç­–
        "batch_size": optimal_batch_size,  # åŠ¨æ€æ‰¹æ¬¡å¤§å°
        "n_epochs": 12,             # å¹³è¡¡è®­ç»ƒæ•ˆæœå’Œæ•ˆç‡
        "ent_coef": 0.008,          # ç¨ä½ç†µç³»æ•°ï¼Œå‡å°‘åæœŸçš„éšæœºæ¢ç´¢
        "learning_rate": lr_schedule,  # ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦
        "clip_range": 0.15,         # ç¨ç´§çš„è£å‰ªï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
        "clip_range_vf": 0.15,      # ä»·å€¼å‡½æ•°è£å‰ªï¼Œå¢å¼ºç¨³å®šæ€§
        "max_grad_norm": 0.5,       # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        "vf_coef": 0.25,           # ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°
        "target_kl": 0.02,         # æ—©åœKLæ•£åº¦é˜ˆå€¼
        "verbose": 1,
        "tensorboard_log": args.log_dir,
        "policy_kwargs": {
            # ğŸ¯ ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼šé€‚åˆå¤„ç†å¤æ‚æŠ“å–åºåˆ—å†³ç­–
            "net_arch": dict(
                pi=[512, 512, 512, 256],   # ç­–ç•¥ç½‘ç»œï¼šæ›´æ·±å±‚æ¬¡ç†è§£æŠ“å–ä¼˜å…ˆçº§
                vf=[512, 512, 256]         # ä»·å€¼ç½‘ç»œï¼šå‡†ç¡®ä¼°è®¡é•¿æœŸå›æŠ¥
            ),
            "activation_fn": torch.nn.ReLU,
            "squash_output": True,      # ç¡®ä¿è¾“å‡ºèŒƒå›´åˆé€‚
            "normalize_images": False,   # ä¸ä½¿ç”¨å›¾åƒï¼Œè®¾ä¸ºFalse
            "optimizer_class": torch.optim.AdamW,  # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
            "optimizer_kwargs": dict(
                weight_decay=0.01,       # L2æ­£åˆ™åŒ–
                eps=1e-8,               # æ•°å€¼ç¨³å®šæ€§
            ),
        }
    }
    
    # æ ¹æ®æ˜¯å¦æ”¯æŒæ©ç é€‰æ‹©æ¨¡å‹ç±»å‹
    if MASKABLE_AVAILABLE:
        # ä¸ºMaskablePPOè°ƒæ•´ç­–ç•¥å‚æ•°
        maskable_kwargs = model_kwargs.copy()
        maskable_kwargs["policy_kwargs"]["features_extractor_class"] = None  # ä½¿ç”¨é»˜è®¤ç‰¹å¾æå–å™¨
        
        model = MaskablePPO("MlpPolicy", vec_env, **maskable_kwargs)
        print("âœ… ä½¿ç”¨MaskablePPOï¼Œæ”¯æŒåŠ¨ä½œæ©ç ")
        print(f"   åŠ¨ä½œç©ºé—´: {vec_env.action_space}")
        print(f"   è§‚æµ‹ç©ºé—´: {vec_env.observation_space}")
    else:
        model = PPO("MlpPolicy", vec_env, **model_kwargs)
        print("âš ï¸ ä½¿ç”¨æ™®é€šPPOï¼Œä¸æ”¯æŒåŠ¨ä½œæ©ç ")
        print("   å»ºè®®å®‰è£…sb3-contribè·å¾—å®Œæ•´åŠŸèƒ½: pip install sb3-contrib")
    
    param_count = sum(p.numel() for p in model.policy.parameters())
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°é‡: {param_count:,}")
    print(f"åŠ¨æ€æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
    print(f"åˆå§‹å­¦ä¹ ç‡: {args.learning_rate:.2e}")
    
    # åˆ›å»ºå¢å¼ºçš„å›è°ƒå‡½æ•°ç³»ç»Ÿ
    callbacks = []
    
    # è®­ç»ƒç›‘æ§å›è°ƒ
    training_monitor = TrainingMonitorCallback(
        check_freq=1000,
        verbose=1
    )
    callbacks.append(training_monitor)
    
    # è¯„ä¼°å›è°ƒ
    if args.eval_freq > 0:
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=os.path.join(args.model_dir, "best_model"),
            log_path=os.path.join(args.log_dir, "eval_logs"),
            eval_freq=args.eval_freq,
            deterministic=True,
            render=False,
            n_eval_episodes=args.n_eval_episodes,
            verbose=1,
        )
        callbacks.append(eval_callback)
    
    # æŠ“å–åºåˆ—åˆ†æå›è°ƒ
    sequence_analysis = GraspSequenceAnalysisCallback(
        eval_env=eval_env,
        check_freq=args.eval_freq,
        n_eval_episodes=3,  # å¿«é€Ÿåˆ†æ
        verbose=1
    )
    callbacks.append(sequence_analysis)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    if args.save_freq > 0:
        checkpoint_callback = CheckpointCallback(
            save_freq=args.save_freq,
            save_path=args.model_dir,
            name_prefix="ppo_envclutter_topdown",
            verbose=1,
        )
        callbacks.append(checkpoint_callback)
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    print("ä¼˜åŒ–çš„å¥–åŠ±è®¾è®¡ï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚çš„ä¼˜å…ˆçº§ï¼‰ï¼š")
    if HAS_IDEAL_ORACLE:
        print("1. ã€ä¼˜å…ˆçº§1ã€‘æˆåŠŸæŠ“å–å¥–åŠ±:")
        print("   - åŸºç¡€æŠ“å–å¥–åŠ±: +5.0")
        print("   - é«˜åº¦å¥–åŠ±: +3.0 * normalized_height (é¼“åŠ±è‡ªä¸Šè€Œä¸‹)")
        print("   - å®Œæˆæ‰€æœ‰ç‰©ä½“: +20.0")
        print("2. ã€ä¼˜å…ˆçº§2ã€‘ä½ç§»æƒ©ç½š: -1.5 * displacement (å‡å°‘å…¶ä»–ç‰©ä½“ç§»åŠ¨)")
        print("3. ã€ä¼˜å…ˆçº§3ã€‘æ—¶é—´æƒ©ç½š: -0.1 (é¼“åŠ±æ•ˆç‡)")
        print("4. å¤±è´¥æƒ©ç½š: -1.0")
        print("")
        print("ç­–ç•¥å­¦ä¹ ç‰¹æ€§:")
        print("- å¢å¼ºè§‚æµ‹åŒ…å«ï¼šç‰©ä½“é«˜åº¦ã€ç›¸å¯¹é«˜åº¦ã€æŠ“å–ä¼˜å…ˆçº§")
        print("- åŠ¨ä½œæ©ç ç¡®ä¿ä¸é‡å¤æŠ“å–")
        print("- è‡ªä¸Šè€Œä¸‹ç­–ç•¥é€šè¿‡é«˜åº¦å¥–åŠ±å¼ºåŒ–")
    else:
        print("âš ï¸ ä½¿ç”¨åŸºç¡€ç¯å¢ƒï¼Œå¥–åŠ±å‡½æ•°å¯èƒ½ä¸åŒ…å«æ‰€æœ‰æŠ“å–é¡ºåºå­¦ä¹ ç‰¹æ€§")
        print("å»ºè®®ï¼šä½¿ç”¨env_clutter.pyç‰ˆæœ¬ä»¥è·å¾—å®Œæ•´åŠŸèƒ½")
    
    model.learn(
        total_timesteps=args.total_timesteps,
        callback=callbacks,
        progress_bar=True,
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(args.model_dir, "ppo_envclutter_final")
    model.save(final_model_path)
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    
    # å…³é—­ç¯å¢ƒ
    vec_env.close()
    eval_env.close()
    
    print("è®­ç»ƒå®Œæˆï¼")


def evaluate_model(args):
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    print(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {args.model_path}")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = create_eval_env(
        env_id="EnvClutter-v1",
        num_envs=1,  # è¯„ä¼°æ—¶ä½¿ç”¨å•ç¯å¢ƒ
        record_video=True,
        video_dir=os.path.join(args.log_dir, "eval_videos"),
    )
    
    # åŠ è½½æ¨¡å‹
    if MASKABLE_AVAILABLE:
        try:
            model = MaskablePPO.load(args.model_path)
            print("âœ… åŠ è½½MaskablePPOæ¨¡å‹")
        except:
            model = PPO.load(args.model_path)
            print("âš ï¸ å°è¯•åŠ è½½ä¸ºæ™®é€šPPOæ¨¡å‹")
    else:
        model = PPO.load(args.model_path)
    
    print("å¼€å§‹è¯„ä¼°...")
    
    # è¿è¡Œè¯„ä¼°
    obs = eval_env.reset()
    episode_rewards = []
    episode_lengths = []
    episode_successes = []
    action_sequences = []  # è®°å½•åŠ¨ä½œåºåˆ—
    
    current_episode_reward = 0
    current_episode_length = 0
    current_action_sequence = []
    
    for step in range(args.eval_steps):
        # é¢„æµ‹åŠ¨ä½œ
        action, _states = model.predict(obs, deterministic=True)
        current_action_sequence.append(action[0] if isinstance(action, np.ndarray) else action)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, done, info = eval_env.step(action)
        
        current_episode_reward += reward[0] if isinstance(reward, np.ndarray) else reward
        current_episode_length += 1
        
        if done[0] if isinstance(done, np.ndarray) else done:
            # Episodeç»“æŸ
            episode_rewards.append(current_episode_reward)
            episode_lengths.append(current_episode_length)
            action_sequences.append(current_action_sequence)
            
            # æ£€æŸ¥æˆåŠŸç‡
            if isinstance(info, list) and len(info) > 0:
                success = info[0].get('success', False)
            elif isinstance(info, dict):
                success = info.get('success', False)
            else:
                success = False
            
            episode_successes.append(success)
            
            print(f"Episodeå®Œæˆ: å¥–åŠ±={current_episode_reward:.2f}, é•¿åº¦={current_episode_length}, æˆåŠŸ={success}")
            print(f"åŠ¨ä½œåºåˆ—: {current_action_sequence[:9]}")  # æ˜¾ç¤ºå‰9ä¸ªåŠ¨ä½œ
            
            # é‡ç½®è®¡æ•°å™¨
            current_episode_reward = 0
            current_episode_length = 0
            current_action_sequence = []
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    if episode_rewards:
        print(f"\nè¯„ä¼°ç»“æœ (å…±{len(episode_rewards)}ä¸ªepisode):")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
        print(f"å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")
        print(f"æˆåŠŸç‡: {np.mean(episode_successes):.2%}")
        
        # åˆ†æåŠ¨ä½œåºåˆ—æ¨¡å¼
        if action_sequences:
            print("\nåŠ¨ä½œåºåˆ—åˆ†æ:")
            for i, seq in enumerate(action_sequences[:3]):  # æ˜¾ç¤ºå‰3ä¸ªepisodeçš„åºåˆ—
                print(f"Episode {i+1} åŠ¨ä½œåºåˆ—: {seq[:9]}")
    
    eval_env.close()


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒEnvClutterç¯å¢ƒå­¦ä¹ æœ€ä¼˜æŠ“å–é¡ºåº')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'eval'], 
                       help='è¿è¡Œæ¨¡å¼ï¼šè®­ç»ƒæˆ–è¯„ä¼°')
    parser.add_argument('--total_timesteps', type=int, default=2_000_000, 
                       help='æ€»è®­ç»ƒæ­¥æ•° - å……åˆ†å­¦ä¹ æŠ“å–é¡ºåº')
    parser.add_argument('--num_envs', type=int, default=32, 
                       help='å¹¶è¡Œè®­ç»ƒç¯å¢ƒæ•°é‡ - å¹³è¡¡æ•ˆç‡ä¸ç¨³å®šæ€§')
    parser.add_argument('--eval_envs', type=int, default=4, 
                       help='å¹¶è¡Œè¯„ä¼°ç¯å¢ƒæ•°é‡')
    
    # PPOè¶…å‚æ•° - å·²ä¼˜åŒ–
    parser.add_argument('--gamma', type=float, default=0.95, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='GAE lambda')
    parser.add_argument('--n_steps', type=int, default=512, help='æ¯ç¯å¢ƒçš„æ­¥æ•°')
    parser.add_argument('--batch_size', type=int, default=4096, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--n_epochs', type=int, default=10, help='PPOæ›´æ–°è½®æ•°')
    parser.add_argument('--ent_coef', type=float, default=0.01, help='ç†µç³»æ•°')
    parser.add_argument('--learning_rate', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--clip_range', type=float, default=0.2, help='PPOè£å‰ªèŒƒå›´')
    
    # ç›®å½•å’Œæ—¥å¿—
    parser.add_argument('--log_dir', type=str, default='./logs/sb3_topdown', 
                       help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--model_dir', type=str, default='./models/sb3_topdown', 
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    # è¯„ä¼°å’Œä¿å­˜ - ä¼˜åŒ–é¢‘ç‡ç¡®ä¿åŠæ—¶åé¦ˆ
    parser.add_argument('--eval_freq', type=int, default=50000, 
                       help='è¯„ä¼°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰- è¾ƒä½é¢‘ç‡å‡å°‘è®­ç»ƒä¸­æ–­')
    parser.add_argument('--n_eval_episodes', type=int, default=5, 
                       help='æ¯æ¬¡è¯„ä¼°çš„episodeæ•° - å¿«é€Ÿè¯„ä¼°')
    parser.add_argument('--save_freq', type=int, default=100000, 
                       help='æ¨¡å‹ä¿å­˜é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰')
    
    # è§†é¢‘å½•åˆ¶
    parser.add_argument('--record_video', action='store_true', 
                       help='æ˜¯å¦å½•åˆ¶è¯„ä¼°è§†é¢‘')
    
    # è¯„ä¼°æ¨¡å¼å‚æ•°
    parser.add_argument('--model_path', type=str, 
                       help='è¯„ä¼°æ¨¡å¼ä¸‹è¦åŠ è½½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--eval_steps', type=int, default=10000, 
                       help='è¯„ä¼°æ¨¡å¼ä¸‹çš„æ€»æ­¥æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç›®å½•
    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    if args.mode == 'train':
        train_ppo(args)
    elif args.mode == 'eval':
        if not args.model_path:
            print("é”™è¯¯ï¼šè¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š--model_pathå‚æ•°")
            return
        evaluate_model(args)


if __name__ == "__main__":
    main()