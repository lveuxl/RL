"""
æœ€ç»ˆè®­ç»ƒè„šæœ¬ - å®Œæ•´çš„æŒ‡æ ‡è¿½è¸ª
"""

import os
import numpy as np
import torch
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
from collections import deque
import time
import warnings
warnings.filterwarnings('ignore')

# æ³¨å†Œç¯å¢ƒ
from env_clutter_final import EnvClutterFinalEnv
from mani_skill.vector.wrappers.sb3 import ManiSkillSB3VectorEnv
import mani_skill.envs


class ComprehensiveMetricsCallback(BaseCallback):
    """ç»¼åˆæŒ‡æ ‡è¿½è¸ªå›è°ƒ - è®°å½•rewardã€lossã€successç‡"""
    
    def __init__(self, eval_env=None, verbose=0):
        super().__init__(verbose)
        self.eval_env = eval_env
        
        # è¿½è¸ªæŒ‡æ ‡
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.episode_successes = deque(maxlen=100)
        self.training_rewards = deque(maxlen=1000)
        
        # Lossè¿½è¸ª
        self.policy_losses = deque(maxlen=100)
        self.value_losses = deque(maxlen=100)
        self.entropy_losses = deque(maxlen=100)
        
        # è®­ç»ƒç»Ÿè®¡
        self.best_mean_reward = -float('inf')
        self.episodes_count = 0
        self.success_count = 0
        
    def _on_step(self) -> bool:
        # è®°å½•å³æ—¶å¥–åŠ±
        if self.locals.get("rewards") is not None:
            rewards = self.locals["rewards"]
            if isinstance(rewards, torch.Tensor):
                rewards = rewards.cpu().numpy()
            
            # è®°å½•æ‰€æœ‰å¥–åŠ±
            for r in rewards:
                self.training_rewards.append(float(r))
            
            # æ¯100æ­¥è®°å½•å¹³å‡å¥–åŠ±
            if self.num_timesteps % 100 == 0:
                if len(self.training_rewards) > 0:
                    mean_reward = np.mean(self.training_rewards)
                    std_reward = np.std(self.training_rewards)
                    
                    self.logger.record("train/instant_reward_mean", mean_reward)
                    self.logger.record("train/instant_reward_std", std_reward)
                    self.logger.record("train/instant_reward_max", np.max(self.training_rewards))
                    self.logger.record("train/instant_reward_min", np.min(self.training_rewards))
        
        # æ£€æŸ¥episodeç»“æŸ
        if self.locals.get("dones") is not None:
            dones = self.locals["dones"]
            if isinstance(dones, torch.Tensor):
                dones = dones.cpu().numpy()
            
            infos = self.locals.get("infos", [])
            
            for i, done in enumerate(dones):
                if done and i < len(infos):
                    info = infos[i]
                    
                    # è®°å½•episodeå¥–åŠ±
                    if "episode" in info:
                        ep_reward = info["episode"].get("r", 0)
                        ep_length = info["episode"].get("l", 0)
                        self.episode_rewards.append(ep_reward)
                        self.episode_lengths.append(ep_length)
                        self.episodes_count += 1
                    
                    # è®°å½•æˆåŠŸç‡
                    if "success" in info:
                        success = float(info["success"])
                        self.episode_successes.append(success)
                        if success > 0:
                            self.success_count += 1
                    elif "success_rate" in info:
                        self.episode_successes.append(float(info["success_rate"]))
        
        # æ¯500æ­¥è®°å½•è¯¦ç»†æŒ‡æ ‡
        if self.num_timesteps % 500 == 0:
            self._log_metrics()
        
        return True
    
    def _on_rollout_end(self) -> None:
        """rolloutç»“æŸæ—¶è®°å½•lossç­‰æŒ‡æ ‡"""
        
        # å°è¯•è·å–lossä¿¡æ¯
        if hasattr(self.model, "logger") and self.model.logger is not None:
            # ä»æ¨¡å‹çš„loggerä¸­è·å–loss
            try:
                if hasattr(self.model, "_last_obs"):
                    # è·å–ä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œå‰å‘ä¼ æ’­ä»¥è®¡ç®—loss
                    with torch.no_grad():
                        obs_tensor = torch.as_tensor(self.model._last_obs).to(self.model.device)
                        values = self.model.policy.predict_values(obs_tensor)
                        if values is not None:
                            value_mean = float(values.mean().item())
                            self.logger.record("train/value_function_mean", value_mean)
            except:
                pass
        
        # è®°å½•å­¦ä¹ ç‡
        if hasattr(self.model, "learning_rate"):
            if callable(self.model.learning_rate):
                current_lr = self.model.learning_rate(self.model._current_progress_remaining)
            else:
                current_lr = self.model.learning_rate
            self.logger.record("train/learning_rate", current_lr)
        
        # è®°å½•æ¢ç´¢ç‡ï¼ˆç†µç³»æ•°ï¼‰
        if hasattr(self.model, "ent_coef"):
            self.logger.record("train/entropy_coefficient", self.model.ent_coef)
        
        # è®°å½•è®­ç»ƒè¿›åº¦
        self.logger.record("train/progress", 1.0 - self.model._current_progress_remaining)
        self.logger.record("train/total_timesteps", self.num_timesteps)
        self.logger.record("train/episodes_count", self.episodes_count)
        
        self._log_metrics()
    
    def _log_metrics(self):
        """è®°å½•æ‰€æœ‰æŒ‡æ ‡åˆ°tensorboard"""
        
        # EpisodeæŒ‡æ ‡
        if len(self.episode_rewards) > 0:
            mean_reward = np.mean(self.episode_rewards)
            std_reward = np.std(self.episode_rewards)
            
            self.logger.record("episode/reward_mean", mean_reward)
            self.logger.record("episode/reward_std", std_reward)
            self.logger.record("episode/reward_max", np.max(self.episode_rewards))
            self.logger.record("episode/reward_min", np.min(self.episode_rewards))
            self.logger.record("episode/length_mean", np.mean(self.episode_lengths))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward
                self.logger.record("episode/best_mean_reward", self.best_mean_reward)
        
        # æˆåŠŸç‡
        if len(self.episode_successes) > 0:
            success_rate = np.mean(self.episode_successes) * 100
            self.logger.record("success/rate", success_rate)
            self.logger.record("success/total_count", self.success_count)
            
            # æœ€è¿‘10ä¸ªepisodeçš„æˆåŠŸç‡
            recent_successes = list(self.episode_successes)[-10:]
            if len(recent_successes) > 0:
                recent_success_rate = np.mean(recent_successes) * 100
                self.logger.record("success/recent_rate", recent_success_rate)
        
        # å¥–åŠ±è¶‹åŠ¿
        if len(self.training_rewards) > 100:
            # è®¡ç®—å¥–åŠ±è¶‹åŠ¿ï¼ˆæ–œç‡ï¼‰
            x = np.arange(len(self.training_rewards))
            y = np.array(self.training_rewards)
            z = np.polyfit(x, y, 1)
            reward_trend = z[0]  # æ–œç‡
            self.logger.record("trend/reward_slope", reward_trend)
            
            # å¥–åŠ±æ”¹è¿›ç‡
            early_rewards = list(self.training_rewards)[:100]
            recent_rewards = list(self.training_rewards)[-100:]
            improvement = (np.mean(recent_rewards) - np.mean(early_rewards)) / (abs(np.mean(early_rewards)) + 1e-8)
            self.logger.record("trend/improvement_rate", improvement * 100)


def create_env(env_id, num_envs, seed=0):
    """åˆ›å»ºç¯å¢ƒ"""
    env = gym.make(
        env_id,
        num_envs=num_envs,
        obs_mode="state",
        control_mode="pd_ee_delta_pose",
        reward_mode="dense",
        sim_backend="gpu" if torch.cuda.is_available() else "cpu",
        render_mode=None,
    )
    
    # è®¾ç½®ç§å­
    env.unwrapped.seed(seed)
    
    # åŒ…è£…ä¸ºSB3ç¯å¢ƒ
    vec_env = ManiSkillSB3VectorEnv(env)
    return vec_env


def linear_schedule(initial_value: float):
    """çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦"""
    def func(progress_remaining: float) -> float:
        return progress_remaining * initial_value
    return func


def train():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # é…ç½®
    config = {
        "env_id": "EnvClutterFinal-v1",
        "num_envs": 64,  # å¹¶è¡Œç¯å¢ƒæ•°
        "total_timesteps": 500_000,
        "seed": 42,
        
        # PPOå‚æ•° - ä¼˜åŒ–åä¿è¯æ”¶æ•›
        "learning_rate": linear_schedule(3e-4),  # çº¿æ€§è¡°å‡
        "n_steps": 128,
        "batch_size": 256,
        "n_epochs": 10,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "clip_range": linear_schedule(0.2),
        "clip_range_vf": None,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.01,
        
        # è·¯å¾„
        "log_dir": "./logs/final_training",
        "tb_log_dir": "./logs/final_training/tensorboard",
        "model_dir": "./models/final_training",
    }
    
    # åˆ›å»ºç›®å½•
    os.makedirs(config["log_dir"], exist_ok=True)
    os.makedirs(config["tb_log_dir"], exist_ok=True)
    os.makedirs(config["model_dir"], exist_ok=True)
    
    # è®¾ç½®ç§å­
    set_random_seed(config["seed"])
    
    print("="*60)
    print("ğŸš€ æœ€ç»ˆç‰ˆè®­ç»ƒ - å®Œæ•´æŒ‡æ ‡è¿½è¸ª")
    print("="*60)
    print(f"ç¯å¢ƒ: {config['env_id']}")
    print(f"å¹¶è¡Œæ•°: {config['num_envs']}")
    print(f"æ€»æ­¥æ•°: {config['total_timesteps']:,}")
    print("-"*60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print("åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    train_env = create_env(config["env_id"], config["num_envs"], config["seed"])
    print("âœ“ è®­ç»ƒç¯å¢ƒå°±ç»ª")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    print("åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    eval_env = create_env(config["env_id"], num_envs=4, seed=config["seed"]+1000)
    print("âœ“ è¯„ä¼°ç¯å¢ƒå°±ç»ª")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆå§‹åŒ–PPOæ¨¡å‹...")
    model = PPO(
        "MlpPolicy",
        train_env,
        learning_rate=config["learning_rate"],
        n_steps=config["n_steps"],
        batch_size=config["batch_size"],
        n_epochs=config["n_epochs"],
        gamma=config["gamma"],
        gae_lambda=config["gae_lambda"],
        clip_range=config["clip_range"],
        clip_range_vf=config["clip_range_vf"],
        ent_coef=config["ent_coef"],
        vf_coef=config["vf_coef"],
        max_grad_norm=config["max_grad_norm"],
        target_kl=config["target_kl"],
        tensorboard_log=config["tb_log_dir"],
        policy_kwargs={
            "net_arch": dict(pi=[128, 128, 64], vf=[128, 128, 64]),
            "activation_fn": torch.nn.Tanh,
            "normalize_images": False,
        },
        verbose=1,
        seed=config["seed"],
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    
    print(f"âœ“ æ¨¡å‹å°±ç»ª (device: {model.device})")
    
    # è®¾ç½®logger
    logger = configure(config["log_dir"], ["stdout", "tensorboard"])
    model.set_logger(logger)
    
    # åˆ›å»ºå›è°ƒ
    callbacks = []
    
    # ç»¼åˆæŒ‡æ ‡å›è°ƒ
    metrics_callback = ComprehensiveMetricsCallback(eval_env=eval_env)
    callbacks.append(metrics_callback)
    
    # è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(config["model_dir"], "best_model"),
        log_path=os.path.join(config["log_dir"], "evaluations"),
        eval_freq=5000,
        n_eval_episodes=10,
        deterministic=True,
        render=False,
        verbose=1,
    )
    callbacks.append(eval_callback)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path=config["model_dir"],
        name_prefix="rl_model",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    callbacks.append(checkpoint_callback)
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "="*60)
    print("ğŸ“Š å¼€å§‹è®­ç»ƒ - ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡:")
    print("  â€¢ Reward: åº”è¯¥ä»è´Ÿå€¼é€æ¸ä¸Šå‡åˆ°æ­£å€¼")
    print("  â€¢ Success Rate: åº”è¯¥ä»0%é€æ¸ä¸Šå‡åˆ°90%+")
    print("  â€¢ Loss: åº”è¯¥é€æ¸ä¸‹é™å¹¶ç¨³å®š")
    print("="*60)
    print("\né¢„æœŸé‡Œç¨‹ç¢‘:")
    print("  5,000æ­¥: çœ‹åˆ°å¥–åŠ±å¼€å§‹ä¸Šå‡")
    print("  20,000æ­¥: æˆåŠŸç‡è¾¾åˆ°20%+")
    print("  50,000æ­¥: å¥–åŠ±ç¨³å®šä¸ºæ­£å€¼")
    print("  100,000æ­¥: æˆåŠŸç‡è¾¾åˆ°50%+")
    print("  200,000æ­¥: å­¦ä¼šè‡ªä¸Šè€Œä¸‹ç­–ç•¥")
    print("  500,000æ­¥: æˆåŠŸç‡è¾¾åˆ°90%+")
    print("-"*60 + "\n")
    
    start_time = time.time()
    
    try:
        # è®­ç»ƒ
        model.learn(
            total_timesteps=config["total_timesteps"],
            callback=callbacks,
            log_interval=1,
            progress_bar=True,
            reset_num_timesteps=True,
            tb_log_name="PPO_final",
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(config["model_dir"], "final_model")
        model.save(final_path)
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹: {final_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒä¸­æ–­")
        interrupted_path = os.path.join(config["model_dir"], "interrupted_model")
        model.save(interrupted_path)
        print(f"ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupted_path}")
    
    finally:
        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        print(f"\n" + "="*60)
        print("ğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
        print(f"  æ€»æ—¶é—´: {elapsed/3600:.2f}å°æ—¶")
        print(f"  å®Œæˆæ­¥æ•°: {model.num_timesteps:,}")
        print(f"  è®­ç»ƒé€Ÿåº¦: {model.num_timesteps/elapsed:.0f} steps/ç§’")
        
        # æœ€ç»ˆæŒ‡æ ‡
        if len(metrics_callback.episode_rewards) > 0:
            print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½:")
            print(f"  å¹³å‡å¥–åŠ±: {np.mean(metrics_callback.episode_rewards):.2f}")
            print(f"  æœ€ä½³å¥–åŠ±: {metrics_callback.best_mean_reward:.2f}")
            
        if len(metrics_callback.episode_successes) > 0:
            final_success_rate = np.mean(list(metrics_callback.episode_successes)[-20:]) * 100
            print(f"  æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%")
        
        # æ¸…ç†
        train_env.close()
        eval_env.close()
        
        print(f"\n" + "="*60)
        print("ğŸ¯ ä¸‹ä¸€æ­¥:")
        print(f"1. æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir {config['tb_log_dir']}")
        print(f"2. æµ‹è¯•æ¨¡å‹: python test_final.py")
        print(f"3. è¯„ä¼°æœ€ä½³æ¨¡å‹: python evaluate_final.py {os.path.join(config['model_dir'], 'best_model', 'best_model.zip')}")
        print("="*60)


if __name__ == "__main__":
    train()