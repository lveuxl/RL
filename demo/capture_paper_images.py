#!/usr/bin/env python3
"""
è®ºæ–‡å±•ç¤ºåœºæ™¯é«˜æ¸…å›¾åƒæ•è·è„šæœ¬
ä¿å­˜ä¸¤ä¸ªè§†è§’çš„é«˜æ¸…å›¾åƒï¼šä¿¯è§†å›¾å’Œä¾§é¢45åº¦å¹³è§†å›¾
"""

import os
import sys
import time
import argparse
import numpy as np
import torch
import cv2
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/linux/jzh/RL_Robot/demo')

# åŠ¨æ€å¯¼å…¥ç¯å¢ƒ
import importlib.util
spec = importlib.util.spec_from_file_location(
    "env_clutter_copy", 
    "/home/linux/jzh/RL_Robot/demo/env_clutter copy.py"
)
env_clutter_copy = importlib.util.module_from_spec(spec)
spec.loader.exec_module(env_clutter_copy)
PaperStackingSceneEnv = env_clutter_copy.PaperStackingSceneEnv

from mani_skill.utils import sapien_utils
from mani_skill.sensors.camera import CameraConfig


class PaperImageCaptureEnv(PaperStackingSceneEnv):
    """
    æ‰©å±•è®ºæ–‡åœºæ™¯ç¯å¢ƒï¼Œæ”¯æŒå¤šç›¸æœºè§†è§’çš„é«˜æ¸…å›¾åƒæ•è·
    """
    
    @property
    def _default_sensor_configs(self):
        """é…ç½®å¤šä¸ªç›¸æœºè§†è§’"""
        configs = []
        
        # 1. ä¿¯è§†ç›¸æœºï¼ˆä»ä¸Šå¾€ä¸‹çœ‹æ‰˜ç›˜ï¼‰
        top_view_pose = sapien_utils.look_at(
            eye=[0.0, 0.0, 0.8],      # ç›¸æœºä½ç½®ï¼šæ‰˜ç›˜æ­£ä¸Šæ–¹80cm
            target=[-0.2, 0.0, 0.1]   # çœ‹å‘æ‰˜ç›˜ä¸­å¿ƒç¨ä¸Šæ–¹
        )
        configs.append(CameraConfig(
            "top_view_camera",
            pose=top_view_pose,
            width=4096,   # è¶…é«˜æ¸…åˆ†è¾¨ç‡ 4K
            height=2160,  # 4K UHD
            fov=np.pi/4,  # 45åº¦è§†é‡è§’
            near=0.01,
            far=100,
        ))
        
        # 2. ä¾§é¢45åº¦ç›¸æœºï¼ˆæ–œå‘å¹³è§†ç‰©ä½“ï¼‰
        side_view_pose = sapien_utils.look_at(
            eye=[0.4, 0.4, 0.3],      # ç›¸æœºä½ç½®ï¼šæ‰˜ç›˜æ–œå‰æ–¹
            target=[-0.2, 0.0, 0.1]   # çœ‹å‘æ‰˜ç›˜ä¸­å¿ƒ
        )
        configs.append(CameraConfig(
            "side_view_camera", 
            pose=side_view_pose,
            width=4096,   # è¶…é«˜æ¸…åˆ†è¾¨ç‡ 4K
            height=2160,  # 4K UHD
            fov=np.pi/3,  # 60åº¦è§†é‡è§’ï¼Œçœ‹å¾—æ›´å¹¿
            near=0.01,
            far=100,
        ))
        
        # 3. ä¿ç•™é»˜è®¤ç›¸æœºä½œä¸ºå¤‡ç”¨
        default_pose = sapien_utils.look_at(
            eye=[0.5, 0.5, 0.6], 
            target=[-0.15, 0.0, 0.15]
        )
        configs.append(CameraConfig(
            "base_camera",
            pose=default_pose,
            width=1280,
            height=960,
            fov=np.pi/3,
            near=0.01,
            far=100,
        ))
        
        return configs


def capture_high_quality_images(scene_config="balanced", output_dir="paper_images"):
    """
    æ•è·è®ºæ–‡å±•ç¤ºåœºæ™¯çš„é«˜æ¸…å›¾åƒ
    
    Args:
        scene_config: åœºæ™¯é…ç½®åç§°
        output_dir: è¾“å‡ºç›®å½•
    """
    print("=" * 50)
    print("ğŸ¨ è®ºæ–‡å±•ç¤ºåœºæ™¯é«˜æ¸…å›¾åƒæ•è·")
    print("=" * 50)
    print(f"ğŸ“· åœºæ™¯é…ç½®: {scene_config}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = int(time.time())
    full_output_dir = f"{output_dir}_{scene_config}_{timestamp}"
    Path(full_output_dir).mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“‚ å®Œæ•´è¾“å‡ºè·¯å¾„: {full_output_dir}")
    
    try:
        # 1. åˆ›å»ºç¯å¢ƒ
        print("\nğŸ—ï¸  åˆ›å»ºé«˜æ¸…å›¾åƒæ•è·ç¯å¢ƒ...")
        env = PaperImageCaptureEnv(
            render_mode="rgb_array",
            obs_mode="state", 
            control_mode="pd_ee_delta_pose",
            num_envs=1,
            scene_config=scene_config,
        )
        print("âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
        # 2. é‡ç½®ç¯å¢ƒï¼Œæ„å»ºåœºæ™¯
        print("\nğŸ¬ åˆå§‹åŒ–åœºæ™¯...")
        obs, info = env.reset()
        print("âœ… åœºæ™¯é‡ç½®å®Œæˆï¼Œå †å ç»“æ„å·²åˆ›å»º")
        
        # 3. ç­‰å¾…åœºæ™¯ç¨³å®š
        print("\nâ³ ç­‰å¾…åœºæ™¯å®Œå…¨ç¨³å®š...")
        for i in range(30):  # é¢å¤–ç¨³å®šæ­¥éª¤
            env.step(np.zeros(7))  # æ— åŠ¨ä½œï¼Œè®©åœºæ™¯ç»§ç»­ç¨³å®š
            if i % 10 == 0:
                print(f"  ç¨³å®šä¸­... {i+1}/30")
        print("âœ… åœºæ™¯ç¨³å®šå®Œæˆ")
        
        # 4. æ•è·ä¸åŒè§†è§’çš„å›¾åƒ
        print("\nğŸ“¸ å¼€å§‹æ•è·é«˜æ¸…å›¾åƒ...")
        
        # è·å–æ‰€æœ‰ç›¸æœºçš„è§‚æµ‹
        env.scene.update_render(update_sensors=True)
        env.capture_sensor_data()
        
        camera_names = ["top_view_camera", "side_view_camera", "base_camera"]
        view_descriptions = ["ä¿¯è§†å›¾", "ä¾§é¢45åº¦è§†å›¾", "é»˜è®¤è§†å›¾"]
        
        saved_images = []
        
        for i, (camera_name, description) in enumerate(zip(camera_names, view_descriptions)):
            if camera_name in env._sensors:
                print(f"  ğŸ“· æ•è·{description} ({camera_name})...")
                
                # è·å–RGBå›¾åƒ
                camera = env._sensors[camera_name]
                obs_data = camera.get_obs(rgb=True, depth=False, segmentation=False)
                rgb_image = obs_data["rgb"]  # [1, H, W, 3] tensor
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if isinstance(rgb_image, torch.Tensor):
                    rgb_array = rgb_image[0].cpu().numpy()  # å–ç¬¬ä¸€ä¸ªç¯å¢ƒ [H, W, 3]
                else:
                    rgb_array = rgb_image[0]
                
                # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                if rgb_array.dtype == np.float32 or rgb_array.dtype == np.float64:
                    if rgb_array.max() <= 1.0:
                        rgb_array = (rgb_array * 255).astype(np.uint8)
                    else:
                        rgb_array = rgb_array.astype(np.uint8)
                
                # è½¬æ¢BGRæ ¼å¼ç”¨äºOpenCVä¿å­˜
                bgr_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGR)
                
                # ä¿å­˜å›¾åƒ
                image_filename = f"{full_output_dir}/{camera_name}_{scene_config}.png"
                success = cv2.imwrite(image_filename, bgr_array)
                
                if success:
                    print(f"    âœ… {description}å·²ä¿å­˜: {image_filename}")
                    print(f"       åˆ†è¾¨ç‡: {rgb_array.shape[1]}x{rgb_array.shape[0]}")
                    saved_images.append({
                        'filename': image_filename,
                        'description': description,
                        'camera': camera_name,
                        'resolution': f"{rgb_array.shape[1]}x{rgb_array.shape[0]}"
                    })
                else:
                    print(f"    âŒ {description}ä¿å­˜å¤±è´¥")
            else:
                print(f"  âš ï¸  æœªæ‰¾åˆ°ç›¸æœº: {camera_name}")
        
        # 5. ç”Ÿæˆåœºæ™¯ä¿¡æ¯æŠ¥å‘Š
        print("\nğŸ“‹ ç”Ÿæˆåœºæ™¯ä¿¡æ¯æŠ¥å‘Š...")
        report_filename = f"{full_output_dir}/scene_report.txt"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write("è®ºæ–‡å±•ç¤ºåœºæ™¯å›¾åƒæ•è·æŠ¥å‘Š\n")
            f.write("=" * 40 + "\n")
            f.write(f"æ•è·æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åœºæ™¯é…ç½®: {scene_config}\n")
            f.write(f"ç‰©ä½“æ€»æ•°: {len(env.all_objects)}\n")
            if hasattr(env, 'target_object') and env.target_object:
                f.write(f"ç›®æ ‡ç‰©ä½“: {env.target_object.name}\n")
            f.write(f"\nå›¾åƒä¿¡æ¯:\n")
            f.write("-" * 20 + "\n")
            
            for img_info in saved_images:
                f.write(f"æ–‡ä»¶å: {os.path.basename(img_info['filename'])}\n")
                f.write(f"æè¿°: {img_info['description']}\n")
                f.write(f"ç›¸æœº: {img_info['camera']}\n")
                f.write(f"åˆ†è¾¨ç‡: {img_info['resolution']}\n")
                f.write(f"è·¯å¾„: {img_info['filename']}\n")
                f.write("\n")
            
            # æ·»åŠ ç‰©ä½“åˆ—è¡¨
            f.write("åœºæ™¯ç‰©ä½“åˆ—è¡¨:\n")
            f.write("-" * 20 + "\n")
            for i, obj in enumerate(env.all_objects):
                f.write(f"{i+1:2d}. {obj.name}\n")
        
        print(f"âœ… åœºæ™¯æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")
        
        # 6. æ¸…ç†
        env.close()
        
        # 7. æ€»ç»“
        print("\n" + "=" * 50)
        print("ğŸ‰ é«˜æ¸…å›¾åƒæ•è·å®Œæˆï¼")
        print("=" * 50)
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {full_output_dir}")
        print(f"ğŸ“· æˆåŠŸæ•è·: {len(saved_images)} å¼ å›¾åƒ")
        
        for img_info in saved_images:
            print(f"  â€¢ {img_info['description']}: {os.path.basename(img_info['filename'])}")
        
        print(f"ğŸ“‹ åœºæ™¯æŠ¥å‘Š: {os.path.basename(report_filename)}")
        print("\nğŸ’¡ å»ºè®®:")
        print("  1. æ£€æŸ¥å›¾åƒè´¨é‡å’Œè§†è§’æ˜¯å¦æ»¡è¶³è®ºæ–‡éœ€æ±‚")
        print("  2. å¯è°ƒæ•´ç›¸æœºä½ç½®é‡æ–°æ•è·")
        print("  3. å›¾åƒå·²ä¸ºé«˜æ¸…åˆ†è¾¨ç‡ï¼Œé€‚åˆè®ºæ–‡ä½¿ç”¨")
        
        return full_output_dir, saved_images
        
    except Exception as e:
        print(f"\nâŒ å›¾åƒæ•è·è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®ºæ–‡å±•ç¤ºåœºæ™¯é«˜æ¸…å›¾åƒæ•è·")
    parser.add_argument("--config", type=str, default="balanced", 
                       help="åœºæ™¯é…ç½®åç§° (é»˜è®¤: balanced)")
    parser.add_argument("--output", type=str, default="paper_images",
                       help="è¾“å‡ºç›®å½•å‰ç¼€ (é»˜è®¤: paper_images)")
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå›¾åƒæ•è·
    output_dir, saved_images = capture_high_quality_images(
        scene_config=args.config,
        output_dir=args.output
    )
    
    if output_dir and saved_images:
        print(f"\nğŸ”š ä»»åŠ¡å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_images)} å¼ é«˜æ¸…å›¾åƒ")
    else:
        print("\nğŸ”š ä»»åŠ¡å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
